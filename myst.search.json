{"version":"1","records":[{"hierarchy":{"lvl1":"Algorithmic Trading"},"type":"lvl1","url":"/markdown/algorithmic-trading","position":0},{"hierarchy":{"lvl1":"Algorithmic Trading"},"content":"","type":"content","url":"/markdown/algorithmic-trading","position":1},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl2":"Introduction"},"type":"lvl2","url":"/markdown/algorithmic-trading#introduction","position":2},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl2":"Introduction"},"content":"Algorithmic trading refers to the use of computer programs to automate the process of making trading decisions and executing orders in financial markets. Although the term is widely used, its precise meaning can vary depending on the context and the regulatory framework. Two influential definitions are the following:\n\nBank for International Settlements (BIS, 2016)\n\nMarkets Committee, Bank for International Settlements, 2016:\nTrading technology in which order and trade decisions are made electronically and autonomously.\n\nMiFID II (Directive 2014/65/EU, Article 4, Definition 39)\n\nEuropean Parliament and Council of the European Union, 2014:\nTrading in financial instruments where a computer algorithm automatically determines individual parameters of orders such as whether to initiate the order, the timing, price or quantity of the order or how to manage the order after its submission, with limited or no human intervention.\n\nNotice that we use the terms algorithmic trading and automated trading as synonyms. In some references, algorithmic trading is used more narrowly to denote algorithmic execution, a specific form of automated trading focused on the optimal execution of predefined orders.\n\nAlgorithmic trading can be viewed as a subset of systematic trading, which refers to any trading strategy defined in a rule-based, methodical manner. It is also a subset of quantitative trading, where trading decisions follow the principles of the scientific method. In this framework, we first construct a scientific model of the trading environment—for example, a stochastic process such as a random walk—to represent market dynamics. This model is then used to derive inferences about quantities of interest, such as the likely range or direction of future prices. These inferences serve as inputs to mathematical optimization procedures, which determine the optimal trading actions—such as when and at what levels to trade—under given objectives and constraints.\n\nThe MiFID II definition is intentionally broad. It encompasses both complex, fully automated trading strategies and simpler automated rules, such as dynamic stop-loss orders or RfQ auto-negotiation rules. The exception being complex orders that are directly implemented within a exchange, which are considered part of the market infrastructure.\nIn the following discussion, we will adopt the first, more restrictive definition, focusing on sophisticated algorithmic strategies where automation plays a central role in decision-making and execution.","type":"content","url":"/markdown/algorithmic-trading#introduction","position":3},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl3":"High-Frequency Trading","lvl2":"Introduction"},"type":"lvl3","url":"/markdown/algorithmic-trading#high-frequency-trading","position":4},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl3":"High-Frequency Trading","lvl2":"Introduction"},"content":"High-Frequency Trading (HFT) is a subset of algorithmic trading distinguished by extremely short holding periods, rapid order submission and cancellation, and technological infrastructures designed to minimize latency. According to the Bank for International Settlements \n\nMarkets Committee, Bank for International Settlements, 2016, HFT is a “subset of automated trading in which orders are submitted and trades executed at high speed, usually measured in microseconds, and a very tight intraday inventory position is maintained.” Such strategies seek to gain advantage from the ability to process information on market conditions and react almost instantaneously, typically resulting in a very large number of small trades, held for short periods, and generating substantial message traffic. To achieve this, HFT firms tend to place their trading servers physically close to the electronic market’s matching engines—a practice known as co-location—to minimize transmission delays or latency.\n\nAt the European regulatory level, MiFID II (Directive 2014/65/EU, Article 4(1)(40)) \n\nEuropean Parliament and Council of the European Union, 2014 defines HFT as an algorithmic trading technique that relies on infrastructure designed to minimize latency, such as co-location, proximity hosting, or high-speed direct electronic access, and in which individual trades or orders are initiated, generated, routed, or executed by systems without human intervention. It also specifies that such activity typically involves high intraday message rates consisting of orders, quotes, or cancellations.\n\nIn practice, low-latency infrastructures are at the core of HFT. Co-location refers to hosting trading servers directly within or adjacent to an exchange’s data center to reduce round-trip latency to microseconds. Examples include Equinix LD4 in Slough (London) and NY4 in Secaucus (New Jersey), which host a large portion of global financial trading infrastructure. Proximity hosting involves maintaining servers in nearby facilities linked via dedicated fiber, while high-speed network access often relies on optimized fiber or microwave connections between major trading hubs (for instance, London–Frankfurt or New York–Chicago routes). Even marginal reductions in latency—on the order of microseconds—can yield significant competitive advantages in markets where prices evolve continuously and across fragmented venues.\n\nHFT strategies themselves vary in scope and complexity. Common approaches include electronic market making, where firms continuously quote bid and ask prices and manage risk within very short horizons; statistical and cross-venue arbitrage, which exploit fleeting price discrepancies between related instruments or exchanges; latency arbitrage, which reacts faster than competitors to public information or order book events; and smart order routing, which optimizes execution quality across multiple venues, sometimes also to capture fee rebates or queue priority. These strategies generate immense volumes of order messages and depend critically on precise market data, optimized software, and sophisticated risk controls.\n\nFrom a regulatory perspective, MiFID II and MiFIR introduced a harmonized framework for algorithmic and high-frequency trading, elaborated through Regulatory Technical Standard 6 (RTS-6) \n\nEuropean Securities and Markets Authority (ESMA), 2016, which defines organizational, risk-control, and testing requirements. As with all European directives, MiFID II provisions must be transposed into national law, and implementation may differ across jurisdictions. For example, Germany’s High-Frequency Trading Act \n\nFederal Financial Supervisory Authority (BaFin), 2013 introduced an explicit authorization regime for HFT firms and added an additional criterion based on the speed of the connection, ensuring that firms not genuinely engaged in high-frequency activity would not fall under the same regulatory burden. These frameworks impose significant obligations on HFT firms, including mandatory system testing, kill switches, message-rate controls, record-keeping, and stringent resilience standards—all of which contribute to higher compliance costs and operational complexity.\n\nDespite their efficiency benefits, HFT firms have attracted public and regulatory scrutiny. Critics argue that practices such as latency arbitrage or preferential access to dark pools create an uneven playing field, eroding confidence in market fairness. Michael Lewis’s Flash Boys \n\nLewis, 2014 popularized this perception by portraying HFT as exploiting microscopic speed advantages to anticipate and “front-run” slower participants, contributing to the view that HFT is inherently predatory. Although many of these practices are fully compliant with market rules, they raise ethical and transparency questions, particularly in fragmented markets where access to speed and information is uneven.\n\nRegulators are also concerned about the systemic implications of HFT. The Flash Crash of May 6, 2010 \n\nU.S. Commodity Futures Trading Commission and U.S. Securities and Exchange Commission, 2010 illustrated how rapid, automated interactions among algorithms could amplify volatility and cause temporary dislocations. Two main risks are frequently cited \n\nEuropean Securities and Markets Authority (ESMA), 2019 \n\nBank for International Settlements (BIS), 2018: false liquidity—where HFT appears to provide deep liquidity in normal conditions but withdraws it abruptly during stress—and algorithmic herding, when many algorithms respond similarly to the same signals, creating self-reinforcing price dynamics. To mitigate these risks, regulators have implemented safeguards such as circuit breakers, message-rate limits, and system testing obligations.\n\nYet, it is important to balance these concerns against the structural role HFT plays in modern markets. By continuously arbitraging prices across venues, narrowing spreads, and enhancing price consistency, HFT contributes to market efficiency and liquidity provision under most conditions. The regulatory challenge is therefore not to constrain speed itself, but to ensure that technological advantages do not compromise fairness or stability. In this sense, HFT embodies both the promise and the tension of modern market microstructure: the pursuit of efficiency through automation, bounded by the need to maintain integrity and resilience in the face of ever-faster financial systems.","type":"content","url":"/markdown/algorithmic-trading#high-frequency-trading","position":5},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl2":"Algorithmic Trading Growth"},"type":"lvl2","url":"/markdown/algorithmic-trading#algorithmic-trading-growth","position":6},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl2":"Algorithmic Trading Growth"},"content":"The growth of algorithmic trading is closely tied to the broader electronification of financial markets, since of course algorithms can only in principle operate when markets provide electronic protocols for submitting, routing, and cancelling orders automatically. Traditionally, this requirement limited algorithmic participation to venues with fully electronic limit-order books or standardized API-based access. However, this constraint may gradually weaken with the emergence of \n\nGenerative AI models—large language models, multimodal systems, and real-time audio models—that could enable algorithmic strategies to interact with voice, chat, and email-based trading workflows. Such systems may eventually allow automation to extend into execution channels that historically relied on human-to-human communication, potentially reshaping parts of OTC, RFQ, and relationship-driven markets.\n\nAcross major markets, empirical studies consistently show a structural transition from manual and voice-based execution to electronic and then algorithmic execution. SEC market-structure analyses report that by the late 2010s much of U.S. trading activity in the capital markets is executed electronically, with automated execution constituting a large portion of activity \n\nU.S. Securities and Exchange Commission, 2020. Measurements of algorithmic and HFT share vary depending on methodology—whether based on order messages, trade count, or value traded—but most empirical work places algorithmic/HFT participation in U.S. equities in the range of 40–60% during the 2010s. Studies using more recent data at millisecond or microsecond granularity reveal that while HFT’s share of displayed liquidity peaked in the mid-2010s, algorithmic activity remains central to U.S. market functioning. A recent reconstruction of HFT participation from 2012 to 2023 shows persistent, high algorithmic involvement across the full universe of listed stocks \n\nBoehmer & others, 2024\n\nIn Europe, ESMA’s empirical reviews of MiFID markets confirm that algorithmic trading expanded rapidly after 2007, driven by fragmentation and the introduction of new trading venues. While HFT participation in European equities is generally found to be lower than in the United States —often around one-third of trading depending on the metric—, algorithmic order submission and cancellation rates are substantial, and similar structural patterns emerge: high electronic venue reliance, dense order-book activity, and widespread use of automated execution by intermediaries and buy-side institutions \n\nEuropean Securities,Markets Authority, 2014 \n\nEuropean Securities,Markets Authority, 2021. ESMA’s multi-year datasets also document significant cross-country heterogeneity, with markets such as the UK and the Netherlands exhibiting higher algorithmic intensity than smaller or less-fragmented continental venues.\n\nIn the Asia–Pacific region, algorithmic trading expanded at a later stage but has shown strong growth wherever electronic limit order-book infrastructure is well developed. Empirical studies on the Tokyo Stock Exchange report substantial algorithmic and HFT participation from the mid-2010s onward, driven by improvements in exchange matching technology \n\nHosaka, 2014. Similar patterns appear in Australia, Korea, and, increasingly, select Chinese markets. The region is more heterogeneous than Europe or the United States: in several Asian jurisdictions the persistence of voice or hybrid OTC mechanisms constrains the penetration of high-speed algorithmic strategies. Where low-latency access and co-location are available, algorithmic trading tends to scale rapidly.\n\nThe growth of algorithmic trading also differs markedly across asset classes. Equities exhibit the deepest empirical record: numerous studies show that algorithmic execution and HFT reshaped equity microstructure by increasing order-book depth, tightening spreads, and raising message traffic. Futures and derivatives markets have experienced a similarly strong expansion. CFTC-backed studies show that between 2012 and 2018 many major U.S. futures contracts saw sharp increases in HFT share, in some cases more than doubling relative to early-2010s baselines \n\nHaynes & Roberts, 2022. Derivatives markets, being fully electronic and centrally cleared, are particularly conducive to high-speed algorithmic strategies, and empirical work documents widespread cross-asset and cross-venue arbitrage activity.\n\nFX markets present a more complex case due to their predominantly OTC structure. BIS fact-finding studies nonetheless show rising algorithmic and HFT presence in electronic spot-FX platforms such as EBS and Reuters venues \n\nBank for International Settlements, 2011. Here, algorithmic trading is concentrated in highly liquid currency pairs (G10 countries) and in segments where anonymous, central-limit-order-book trading is available; elsewhere, voice trading and relationship-based bilateral execution remain dominant.\n\nIn fixed income markets the evidence points to a steady but uneven rise in automation. ICMA and New York Fed research finds that electronification, especially on all-to-all corporate bond platforms and electronic interdealer Treasury markets, has enabled algorithmic strategies to scale. However, voice and RFQ mechanisms continue to retain significant market share in many bond categories. Algorithmic market making in bond futures has grown particularly quickly, mirroring the pattern observed in equity index futures \n\nBech et al., 2016 \n\nFederal Reserve Bank of New York, 2015.\n\nFinally, cryptocurrency markets have undergone one of the fastest transitions to algorithmic execution. Empirical studies using millisecond-level data show that, from the mid-2010s onward, many large crypto exchanges experienced rapid growth in latency-sensitive strategies, cross-venue arbitrage, and automated market making. Fragmentation across exchanges, low latency APIs, and continuous trading environments have resulted in market microstructures that resemble early electronic equities: high message traffic, narrow spreads in liquid pairs, and a strong role for algorithmic intermediaries \n\nMakarov & Schoar, 2020.\n\nTaken together, empirical research shows that algorithmic trading expands most rapidly in markets that have electronic order books, provide direct or low-latency access, and standardize market data and clearing processes. Regional patterns differ: U.S. markets lead, Europe follows with strong regulatory oversight, and Asia expands heterogeneously, but the underlying trajectory is similar. Across asset classes, algorithmic activity is most advanced in equities and futures, growing in FX and fixed income as electronification proceeds, and already dominant in many crypto venues. The result is a global, multi-asset shift in which algorithmic strategies have become a central mechanism for price formation, liquidity provision, and execution quality in modern markets.","type":"content","url":"/markdown/algorithmic-trading#algorithmic-trading-growth","position":7},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl2":"Reasons to use Algorithmic Trading"},"type":"lvl2","url":"/markdown/algorithmic-trading#reasons-to-use-algorithmic-trading","position":8},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl2":"Reasons to use Algorithmic Trading"},"content":"The widespread adoption of algorithmic trading across financial markets is driven by a combination of technological, economic, and organisational factors. At its core, algorithmic trading enables market participants to operate more efficiently, consistently, and at scale than would be possible through purely manual processes.\n\nA primary motivation for using trading algorithms is speed. Computer algorithms can submit orders and react to changes in market conditions in timeframes far shorter than those accessible to human traders. In electronic markets, where prices and liquidity can change within milliseconds, this speed advantage is often essential, particularly for strategies that rely on short-lived opportunities or rapid risk adjustments.\n\nClosely related to speed is the ability of algorithms to perform large-scale information processing. Trading algorithms can simultaneously monitor and analyse multiple data sources, including prices, order books, volumes, and derived indicators across many instruments and markets. This parallel processing capability allows algorithms to detect patterns and respond to market signals that would be difficult or impossible for a human trader to observe in real time.\n\nAlgorithmic trading also provides scalability. Once developed, an algorithm can be deployed across a large number of instruments, markets, or client orders with minimal incremental cost. This enables firms to increase trading activity and market coverage without a proportional increase in human headcount. Scalability is particularly important for institutions managing large portfolios, operating multiple strategies concurrently, or providing execution and liquidity services to many clients.\n\nAnother key advantage is the promotion of systematic trading. Algorithms enforce a disciplined and repeatable decision-making process, reducing the influence of emotions such as fear, overconfidence, or hesitation. This systematic approach facilitates rigorous evaluation of strategy performance using historical data, allows meaningful comparisons between alternative strategies, and supports continuous improvement through testing and refinement. While systematic trading is not exclusive to algorithms, automation makes it far easier to apply consistently over time and at scale.\n\nTrading algorithms are also naturally aligned with a quantitative approach to trading, so much than in \n\nBank for International Settlements, 2011, they are defined as “the use of computers and advanced mathematical models to make decisions about the timing, price and quantity of a market order”. They enable the explicit formulation of objectives—such as minimising execution costs, controlling inventory risk, or maximising profit and loss—within a mathematical or statistical framework. By doing so, algorithms can optimise trading decisions in a consistent manner, subject to clearly defined constraints. This is particularly evident in execution algorithms that seek to minimise transaction costs and in market-making algorithms that aim to balance profitability against inventory and risk limits.\n\nFinally, algorithmic trading helps to reduce human errors. Manual trading is susceptible to operational mistakes, such as incorrect order sizes, prices, or instruments, often referred to as “fat-finger” errors. By automating order generation and routing, algorithms can significantly lower the incidence of such errors. At the same time, it is important to recognise that automation introduces its own set of risks, including software bugs, model errors, and system failures. These risks do not negate the benefits of algorithmic trading, but they do require robust governance, testing, and monitoring frameworks, which are addressed in later chapters.","type":"content","url":"/markdown/algorithmic-trading#reasons-to-use-algorithmic-trading","position":9},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl2":"A Brief History of Algorithmic Trading"},"type":"lvl2","url":"/markdown/algorithmic-trading#a-brief-history-of-algorithmic-trading","position":10},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl2":"A Brief History of Algorithmic Trading"},"content":"The evolution of algorithmic trading is deeply intertwined with the broader digitisation of financial markets. In the 1970s and 1980s, exchanges began computerising order workflows, while quantitative finance—particularly the Black–Scholes framework \n\nBlack & Scholes, 1973—rapidly entered trading practice. Black–Scholes provided a replicating-portfolio logic that could be encoded into dynamic rules, enabling strategies such as portfolio insurance, which sought to mimic a put option by mechanically selling futures as markets fell. As computers automated these adjustments, model-driven execution became one of the first large-scale uses of electronic trading logic. During the 1987 crash, the procyclical behaviour of these strategies—selling as prices declined—was widely seen as amplifying market stress, revealing how model-based, computerised trading could interact with volatility in destabilising ways.\n\nThrough the 1990s, electronic trading accelerated. Several U.S. exchanges migrated to fully electronic execution, and new electronic communication networks (ECNs) such as Island and Archipelago fostered a more competitive, fragmented market structure. Decimalisation in 2001 further reduced tick sizes and compressed spreads, creating conditions that favoured automation. Early statistical arbitrage strategies, including pairs trading, became prominent in this era, with pioneering work at Morgan Stanley and subsequent developments by David Shaw, James Simons, and others who would later build systematic hedge funds. At the academic frontier, Bertsimas and Lo \n\nBertsimas & Lo, 1998, followed by Almgren and Chriss \n\nAlmgren & Chriss, 2000, created a rigorous mathematical foundation for optimal trade execution, establishing the framework that still underpins institutional execution algorithms today.\n\nThe 2000s saw rapid advances in connectivity, co-location, and message-handling infrastructure. As latency budgets shrank, high-frequency trading (HFT) grew from a niche practice into a major component of equity-market activity. Regulatory changes—in particular Regulation NMS in the United States and MiFID I in Europe—encouraged venue competition and liquidity fragmentation, making automated smart order routing essential for best execution. Around the same period, Avellaneda and Stoikov (2008) \n\nAvellaneda & Stoikov, 2008 introduced a dynamic framework for limit-order-book market making, which became one of the theoretical cornerstones of modern electronic liquidity provision.\n\nBy the end of the decade, HFT accounted for a substantial share of U.S. equity volumes, though competition subsequently reduced margins. The “Flash Crash” of 6 May 2010 demonstrated the speed and interconnectedness of automated strategies: a large sell algorithm interacting with aggressive HFT flows produced a rapid, self-reinforcing price collapse across multiple asset classes before markets recovered minutes later. After 2010, firms increasingly pushed for technological edge through specialised hardware such as FPGAs and—critically—through the deployment of microwave and later millimetre-wave transmission networks. These reduced Chicago–New Jersey latency close to the physical limit achievable in the atmosphere.\n\nDuring the 2010s, automated ingestion of unstructured data emerged as another frontier. Firms began deploying real-time news analytics, and by 2012 platforms such as Dataminr enabled machine-readable detection of market-relevant events from public information flows. Twitter-based signals were incorporated into professional terminals, although early episodes such as the 2013 hacked Associated Press tweet showed the risks inherent in embedding fast-reaction systems into the market ecosystem. Meanwhile, crypto-asset markets developed their own microstructures, with HFT-style market making and arbitrage quickly dominating trading on major exchanges by the mid-2010s.\n\nThe late 2010s brought an increase in experimentation with machine learning in execution optimisation, short-horizon prediction, and market-making parameterisation. Reinforcement learning, in particular, attracted academic and industry attention, although deployment into truly latency-critical or autonomous production systems remained limited. Most firms adopted ML in peripheral or advisory capacities—signal generation, short-term prediction, and parameter tuning—rather than placing machine learning models directly “in the loop” for continuous order submission.\n\nThe 2020s introduced several structural developments. Cloud-based trading infrastructure matured, allowing firms to shift large-scale analytics, simulation, and historical data processing to scalable cloud compute while continuing to run their latency-sensitive components on dedicated hardware. Alternative datasets, spanning geospatial indicators, mobility traces, supply-chain data, and ESG-linked information, became integrated into quantitative research pipelines, further blurring the boundary between traditional quant signals and broader data science.\n\nGenerative AI technologies, including large language models, began to influence research, monitoring, and operational workflows. Firms are actively experimenting with these tools—for example in news classification, documentation analysis, surveillance, and compliance—but there is little evidence of LLMs being embedded directly into production trading engines. Their role remains largely complementary: accelerating research, aiding supervision, or supporting decision-making rather than autonomously driving order flow.\n\nEvents such as the extreme volatility during the COVID-19 market shock reinforced the centrality of algorithmic trading to market functioning: automated execution models proved adaptable under stress, yet liquidity provision by HFT market makers contracted at critical moments, widening spreads and exposing structural fragilities. In crypto-asset markets, the 2022 crisis offered a parallel demonstration of algorithmic microstructure dynamics—latency races, liquidity evaporation, and execution-layer vulnerabilities—within a largely unregulated environment.\n\nAcross these decades, algorithmic trading evolved from a narrow automation tool into a comprehensive technological discipline encompassing market microstructure, optimisation, data engineering, and increasingly advanced machine learning. While the foundational principles of execution and liquidity provision remain rooted in the early models of the 1990s and 2000s, the field continues to expand, shaped by hardware frontiers, data availability, and new analytical methods. The next stage of development will likely come from integrating these elements—systematic modelling, high-performance infrastructure, and advanced AI—while maintaining the stringent reliability and control standards required in live markets.","type":"content","url":"/markdown/algorithmic-trading#a-brief-history-of-algorithmic-trading","position":11},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl2":"Types of Trading Algorithms"},"type":"lvl2","url":"/markdown/algorithmic-trading#types-of-trading-algorithms","position":12},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl2":"Types of Trading Algorithms"},"content":"Trading algorithms can be grouped into a small number of broad categories according to their primary economic objective and their mode of interaction with financial markets. While real-world strategies often combine elements of more than one category, this classification provides a useful analytical framework for understanding the role algorithms play in modern trading systems. At a high level, trading algorithms can be divided into execution algorithms, market-making algorithms, and investment algorithms. Each category reflects a distinct function within the trading process, ranging from the efficient implementation of decisions, to the provision of liquidity, to the allocation of capital with the expectation of future returns.","type":"content","url":"/markdown/algorithmic-trading#types-of-trading-algorithms","position":13},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl3":"Execution Algorithms","lvl2":"Types of Trading Algorithms"},"type":"lvl3","url":"/markdown/algorithmic-trading#execution-algorithms","position":14},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl3":"Execution Algorithms","lvl2":"Types of Trading Algorithms"},"content":"The goal of executing algorithms is to buy or sell a financial instrument minimising the transaction costs. In this setting, the decision to trade is taken outside the algorithm, for example by a portfolio manager, risk manager, or also another trading algorithm. The algorithm’s role is to determine how to trade, taking into account market conditions, liquidity, and the structure of the trading venue.\n\nThese algorithms are particularly important when trading large volumes in markets organised around a limit order book. Liquidity at the best bid or offer is often insufficient to absorb a large order without significant price concessions. A naïve execution, such as submitting a single market order, would therefore result in substantial transaction costs, also called temporary market impact. The alternative of using limit orders does not guarantee execution and might signal other players the interest to transact. They might react modifying their orders in a way that adversely impact our cost of execution, for example by removing orders in the opposite side of the order book and placing them at less favourable prices. In both cases, the information conferred by these orders might produce a longer term effect on the other players, for instance affecting the likelihood of arrival of other investor’s orders or the price at which they are willing to trade. This is called the permanent market impact. The following figure illustrates this trade-off at the level of primitive orders.\n\n\n\nFigure 1:Illustration of the trade-off between executing an order in a limit order book (upper left) using market orders (upper right) or limit orders (lower left). Market orders guarantee immediacy of execution but incur in worse execution prices, since they consume the available liquidity. Limit orders can execute at more favourably prices, but their execution is not guaranteed.\n\nThis trade-off between market impact and execution probability is at the heart of the design of execution algorithms. Notice that execution probability translates into market or price risk if the algorithm must fill the full order, since prices might move adversely (but also, of course, favourably) if the order is not executed immediately. In general, execution algorithms seek to address this problem by intelligently deciding how to split the full order (also called the parent order) into smaller chunks (child orders) in order to reduce the market impact while keeping price risk under control. The algorithm not only decides the type and size of the child orders. It also can delay their submission to avoid leaking information to the market and expecting that the liquidity consumed will be replenished by new orders arriving to the market.\n\nFor example, a simple execution strategy is the TWAP (Time Weighted Average Price), which splits the order in chunks of equal size that are executed over a given period of time (e.g. a trading session) during certain allocated time buckets (e.g. five minutes buckets). For example, an order of 300 shares to be executed during the next hour using a TWAP algorithm could be divided in twelve chunks of 25 shares to be executed every 5 minutes. Every chunk of 25 shares is then executed in the limit order book using primitive orders, e.g. a combination of limit orders and market orders. The following figure illustrates such strategy.\n\n\n\nFigure 2:Simple sketch of Time Weighted Average Price (TWAP) execution schedule, where an equal number of shares are allocated to equal time buckets over the proposed execution period. Within each bucket, the allocated order is executed using a combination of primitive orders. A TWAP strategy is a simple way to reduce market impact in execution without compromising the execution of the full order.\n\nAs we wil discuss in chapter \n\nExecution fundamentals, there is a large variety of standard execution algorithms that can be used depending on the objectives and profile of the party submitting the order. For instance, for orders that target the average price of the orders executed in the market during the execution window, there is the VWAP (Volume Weighted Average Price) algorithm. And the family of Implementation Shortfall execution algorithms allow to adjust the market impact vs price risk trade-off to the risk tolerance of the investor.","type":"content","url":"/markdown/algorithmic-trading#execution-algorithms","position":15},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl3":"Market-Making algorithms","lvl2":"Types of Trading Algorithms"},"type":"lvl3","url":"/markdown/algorithmic-trading#market-making-algorithms","position":16},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl3":"Market-Making algorithms","lvl2":"Types of Trading Algorithms"},"content":"Market-making algorithms are concerned with the continuous provision of liquidity to financial markets. These algorithms stand ready to buy and sell a financial instrument by continuously quoting bid and ask prices. The economic compensation for providing this immediacy is the bid–ask spread, which must cover both operating costs and the risks inherent in the activity.\n\nTwo main sources of risk characterise market making. The first is inventory risk, which arises because the market maker must hold positions in order to provide liquidity. If prices move unfavourably, the value of this inventory may decline. The second is information asymmetry, whereby counterparties may trade on superior information, leaving the market maker exposed to adverse selection. These risks explain why market makers typically quote prices away from a perceived fair value rather than at the mid-price. Additionally, the market-maker needs to strike a balance between the frequency of trading and the profitability per round-trip of buying and selling, determined by the spreads quoted. Since a high-frequency and low-profit strategy can be seen to have a lower volatility of overall profits & losses for the market-maker, this trade-off is also considered a form of risk, named transactional risk.\n\nAlgorithmic market making automates the process of quote generation and risk management. A full market-making algorithm typically needs to maintain good estimations of the fair price of the instrument quoted and determine appropriate bid and ask quotes around that price, taking into account current market conditions and internal risk constraints. Quotes may be adjusted asymmetrically to manage inventory, for example by quoting more aggressively on one side of the market to reduce accumulated exposure. In many implementations, inventory risk is further mitigated through automatic hedging using correlated instruments. The following figure sketches the components of a market-making algorithm for Interest Rate Swaps:\n\n\n\nFigure 3:A market-making algorithm for standard Interest Rate Swaps (IRS) quoted in interbank markets based on a limit order book. The main components of the algorithm are 1) fair price determination (mid-price in the order book), 2) spread determination, 3) hedging portfolio exposures, in this case using liquid futures on government bonds.\n\nThese algorithms are widely used by broker-dealers, particularly for smaller transactions, allowing human traders to focus on larger or more complex trades. They are also central to the business models of electronic liquidity providers and high-frequency trading firms. Market-making algorithms operate across different market structures, including order-driven, quote-driven, and hybrid markets, adapting their logic to the specific rules and microstructure of each venue.","type":"content","url":"/markdown/algorithmic-trading#market-making-algorithms","position":17},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl3":"Investment algorithms","lvl2":"Types of Trading Algorithms"},"type":"lvl3","url":"/markdown/algorithmic-trading#investment-algorithms","position":18},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl3":"Investment algorithms","lvl2":"Types of Trading Algorithms"},"content":"Investment can be widely defined as the allocation of scarce resources  with the expectation of future benefits. In particular, investment strategies in the financial markets allocate such resources to financial instruments, after a careful analysis of future profit opportunities. Investment trading algorithms simply execute these strategies autonomously using computers. Given the generality of this objective, it is not surprising that investment algorithms encompass a large variety of different strategies. And although it is possible to categorize broadly these strategies in families that share a common pattern for the source of profits, in practice investors that implement investment algorithms spend a fair amount of time trying to find new types of strategies or unexplored asset classes or markets where existing strategies can be applied. The reason is that as soon as certain investment strategies become popular, those trades become crowded and profit opportunities fade away quickly.\n\nInvestment algorithms can operate over different time horizons, and the role of automation varies accordingly. A useful distinction can be made between intraday investment algorithms and investment algorithms operating over longer horizons.\n\nIntraday investment algorithms focus on short-term opportunities that may persist for minutes, seconds, or even fractions of a second. These strategies aim to exploit transient patterns in prices, order flow, or relative valuations, such as short-term momentum, mean reversion, or arbitrage opportunities across instruments or venues. In this context, algorithms are often essential rather than optional: the speed at which signals emerge and decay makes manual trading impractical. As a result, intraday investment algorithms are typically implemented in a fully automated manner and are closely linked to market microstructure, execution quality, and latency.\n\nBy contrast, investment algorithms operating over longer time horizons —such as days, weeks, or months— often play a more varied role. In some cases, algorithms are used primarily as decision-support tools, systematically analysing data and generating signals that inform human investment decisions. Orders may then be executed manually or via standard execution algorithms. This hybrid approach is common in traditional asset management, where explainability, governance, and oversight considerations may limit full automation.\n\nIn other cases, particularly in systematic and robo-investment strategies, algorithms take on a more comprehensive role. They may determine asset allocation, rebalance portfolios periodically, and manage risk in a largely automated fashion. Execution itself may still be delegated to specialised execution algorithms, but the investment logic —how capital is allocated across assets and over time— is embedded directly in the algorithmic framework.\n\nAs the number of strategies, instruments, or portfolios managed by an institution increases, the economic rationale for automation becomes stronger. Managing a large and diverse set of investment strategies manually is costly and error-prone, even when trading frequencies are relatively low. In such environments, investment algorithms provide scalability, consistency, and the ability to manage complexity in a controlled and systematic way.","type":"content","url":"/markdown/algorithmic-trading#investment-algorithms","position":19},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl2":"Developing Trading Algorithms"},"type":"lvl2","url":"/markdown/algorithmic-trading#developing-trading-algorithms","position":20},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl2":"Developing Trading Algorithms"},"content":"Developing a trading algorithm is a structured and iterative process that begins with a clear definition of its objectives. The purpose of the algorithm dictates the approach, data requirements, and performance metrics. For execution algorithms, the primary objective is to minimize transaction costs and market impact when buying or selling large volumes of assets. This often involves breaking down orders into smaller trades and executing them over time or across multiple venues to avoid moving the market. For market-making algorithms, the goal is to provide liquidity by continuously quoting bid and ask prices, profiting from the spread while carefully managing inventory risk and adverse selection. Meanwhile, intraday investment algorithms aim to generate returns by exploiting short-term price inefficiencies, arbitrage opportunities, or momentum effects, where speed and precision are critical.\n\nOnce the objective is defined, the next step is to establish how the algorithm’s performance will be measured. The choice of benchmarks is crucial, as it determines how success is evaluated and how different strategies are compared. Performance metrics vary depending on the type of algorithm. For example, execution algorithms are often evaluated based on their ability to achieve a favorable average price relative to the market, while investment strategies focus on risk-adjusted returns and drawdowns. Market - making algorithms typically track the profit & loss of the portfolio, as well as specific liquidity provision measures like the percentage of time during the market hours that bid and ask quotes were posted (for LOB based market-making) or the percentage of request for quotes for which a price has been provided (for RfQ based market-making).\n\nData plays a central role in algorithmic trading. The type and quality of data required depend on the strategy: high-frequency trading demands ultra-low-latency, tick-level data, while longer-term strategies may rely on daily or hourly price series. Historical data is used for both backtesting and calibrating the algorithm’s parameters. However, it is essential to recognize the limitations of historical data, as markets evolve and past performance does not guarantee future results. Overfitting—where an algorithm is excessively tailored to historical patterns—can lead to poor performance in live trading. To address this, developers use techniques such as out-of-sample testing, cross-validation, and stress testing under different market conditions.\n\nThe design phase is where the algorithm is developed. There are several approaches to strategy creation:\n\nMathematical Optimization: Techniques like Dynamic Programming or Mean - Variance optimization are used to design a strategy that achieves the selected goal.\n\nMachine Learning: Algorithms can be trained to recognize patterns in market data, adapt to changing conditions, or predict short-term price movements. In addition, techniques like Reinforcement Learning can be used for developing adaptive trading strategies.\n\nHeuristic Rules: Some strategies are based on empirical observations or trader intuition, codified into rules that guide the algorithm’s actions. While these may lack the complexity of data-driven models, they can be effective in markets where human judgment is still valuable.\n\nHybrid Approaches: Many algorithms combine elements of the above, leveraging machine learning for pattern recognition and mathematical optimization for efficient execution.\n\nAfter the design phase, the algorithm is implemented in code, typically using languages such as Python, C++, or Java, depending on performance requirements. The algorithm is then tested in a controlled environment, first through historical backtesting and later in real-time simulations or paper trading. Backtesting helps assess how the algorithm would have performed in past market conditions, but it is important to account for transaction costs, slippage, and market impact, which are often overlooked in simplified tests. Stress testing under extreme scenarios—such as flash crashes or periods of low liquidity—helps identify potential vulnerabilities before deployment.\n\nDeployment marks the beginning of a new phase: continuous monitoring and refinement. Markets are dynamic, and an algorithm that performs well in one regime may struggle in another. Real-time analytics are used to track performance and risk metrics, allowing for quick adjustments if needed. Successful algorithmic trading requires not only robust initial development but also ongoing adaptation to maintain effectiveness in changing market conditions.","type":"content","url":"/markdown/algorithmic-trading#developing-trading-algorithms","position":21},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl2":"Benchmarks for Algorithmic Strategies"},"type":"lvl2","url":"/markdown/algorithmic-trading#benchmarks-for-algorithmic-strategies","position":22},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl2":"Benchmarks for Algorithmic Strategies"},"content":"The choice of benchmarks depends on the type of algorithm and its objectives. Below are the most common benchmarks used in the industry, defined mathematically where applicable:","type":"content","url":"/markdown/algorithmic-trading#benchmarks-for-algorithmic-strategies","position":23},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl3":"Execution Strategies","lvl2":"Benchmarks for Algorithmic Strategies"},"type":"lvl3","url":"/markdown/algorithmic-trading#execution-strategies","position":24},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl3":"Execution Strategies","lvl2":"Benchmarks for Algorithmic Strategies"},"content":"For algorithms focused on executing trades with minimal market impact, the following benchmarks are typically used:\n\nClose: The mid-price of the asset at the time the strategy completes execution.\n\nMarket Order Close: The cost of executing a market order at the end of the strategy’s time horizon, using the notional value of the strategy.\n\nOpen-High-Low-Close (OHLC) Average: The average of the mid-prices at the open, high, low, and close during the execution period.\n\nTime Weighted Average Price (TWAP):\nThe arithmetic mean of the asset’s price over the execution window, calculated as:\\text{TWAP} = \\frac{1}{N} \\sum_{i=1}^{N} P_i\n\nwhere P_i is the mid-price at time i and N is the number of time intervals.\n\nVolume Weighted Average Price (VWAP):\nThe average price weighted by trading volume over the execution period, calculated as:\\text{VWAP} = \\frac{\\sum_{i=1}^{N} (P_i \\times V_i)}{\\sum_{i=1}^{N} V_i}\n\nwhere P_i is the price at time i and (V_i) is the volume traded at that time.\n\nDecision Open: The mid-price of the asset at the time the strategy is initiated.\n\nMarket Order Decision Open: The cost of executing a market order at the time the strategy is launched, using the notional value of the strategy.\n\nArrival Open: The mid-price of the asset when the first order generated by the strategy reaches the market.\n\nMarket Order Arrival Open: The cost of executing a market order at the time the first order reaches the market, using the notional value of the strategy.","type":"content","url":"/markdown/algorithmic-trading#execution-strategies","position":25},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl3":"Market-Making and investment Strategies","lvl2":"Benchmarks for Algorithmic Strategies"},"type":"lvl3","url":"/markdown/algorithmic-trading#market-making-and-investment-strategies","position":26},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl3":"Market-Making and investment Strategies","lvl2":"Benchmarks for Algorithmic Strategies"},"content":"For algorithms designed to generate returns or provide liquidity, such as market-making, momentum, mean-reversion, or arbitrage strategies, the following benchmarks are commonly used:\n\nProfit and Loss (P&L):\nThe cumulative return of the strategy over a given period, starting from an initial position or inventory.\n\nSharpe Ratio:\nMeasures the excess return per unit of risk (volatility), defined as:S_a = \\frac{E[R_a - R_b]}{\\sigma_a}\n\nwhere E[R_a - R_b] is the expected excess return of the strategy over a risk-free rate, and \\sigma_a is the standard deviation of the strategy’s returns.\n\nBeta (\\beta):\nIndicates the sensitivity of the strategy’s returns to market movements. A beta greater than 1 suggests the strategy is more volatile than the market, while a beta less than 1 indicates lower volatility.\n\nMaximum Drawdown:\nThe largest peak-to-trough decline in the strategy’s cumulative returns over a specified period. It is a measure of downside risk.\n\nSortino Ratio:\nA variation of the Sharpe ratio that focuses on downside volatility, defined as:S = \\frac{R - T}{DR}\n\nwhere R is the average return, T is the target return (often the risk-free rate), and DR is the standard deviation of negative returns (downside deviation).\n\nOmega Ratio (\\Omega):\nCompares the probability of achieving returns above a threshold to the probability of returns below that threshold, defined as:\\Omega(r) = \\frac{\\int_{r}^{\\infty} (1 - F(x)) \\, dx}{\\int_{-\\infty}^{r} F(x) \\, dx}\n\nwhere F(x) is the cumulative distribution function of returns, and r is the target return.\n\nMarket-making algorithms are also evaluated based on their ability to provide liquidity consistently and effectively. The following metrics are commonly used:\n\nPercentage of Time Quoted (LOB-based market making):\nThe proportion of the trading session during which the algorithm maintains active bid and ask quotes in the limit order book (LOB). This reflects the algorithm’s presence and commitment to providing liquidity.\n\nPercentage of Requests for Quote (RfQs) Quoted (RfQ-based market making):\nThe ratio of RfQs for which the algorithm returns a price to the client, calculated as:\\text{Quote Rate} = \\frac{\\text{Number of RfQs Quoted}}{\\text{Total Number of RfQs Received}}\n\nHit Ratio (RfQ-based market making):\nThe ratio of RfQs that result in a trade (hit) to the total number of RfQs received, defined as:\\text{Hit Ratio} = \\frac{\\text{Number of Hits}}{\\text{Total Number of RfQs Received}}\n\nHit & Miss Ratio (RfQ-based market making):\nThe ratio of RfQs that result in a trade (hit) to the total number of RfQs that are closed by any dealer (i.e., the client traded). This excludes price discovery RfQs where the client did not trade, and is calculated as:\\text{Hit \\& Miss Ratio} = \\frac{\\text{Number of Hits}}{\\text{Total Number of RfQs Closed by Any Dealer}}","type":"content","url":"/markdown/algorithmic-trading#market-making-and-investment-strategies","position":27},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl2":"Algorithmic Trading Infrastructure"},"type":"lvl2","url":"/markdown/algorithmic-trading#algorithmic-trading-infrastructure","position":28},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl2":"Algorithmic Trading Infrastructure"},"content":"The effectiveness of an algorithmic trading strategy is not solely determined by the sophistication of the algorithm itself, but also by the robustness and efficiency of the infrastructure that supports it. A well-designed infrastructure ensures low latency, high capacity, and resilience—qualities that are indispensable in modern electronic markets. The infrastructure required for algorithmic trading is composed of several interconnected components, each playing a critical role in the execution, monitoring, and optimization of trading strategies.","type":"content","url":"/markdown/algorithmic-trading#algorithmic-trading-infrastructure","position":29},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl3":"Core Requirements of Algorithmic Trading Infrastructure","lvl2":"Algorithmic Trading Infrastructure"},"type":"lvl3","url":"/markdown/algorithmic-trading#core-requirements-of-algorithmic-trading-infrastructure","position":30},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl3":"Core Requirements of Algorithmic Trading Infrastructure","lvl2":"Algorithmic Trading Infrastructure"},"content":"To support the demands of algorithmic trading, the infrastructure must meet several fundamental requirements. First and foremost is the need for high speed and low latency. Delays in transmitting data or executing orders can significantly impact performance, particularly for high-frequency trading (HFT) strategies, where microsecond-level latency can make the difference between profit and loss. The necessary latency threshold depends on the type of algorithm and the markets in which it operates. For instance, HFT strategies often require microsecond-level precision, while execution algorithms may tolerate slightly higher delays.\n\nAnother critical requirement is capacity. Algorithmic trading generates a vast number of messages due to the high frequency of orders, cancellations, and updates. The infrastructure must be capable of handling this volume without degradation in performance, ensuring that the system remains responsive even during periods of peak activity.\n\nFinally, resiliency is essential. The system must include mechanisms for monitoring performance in real-time and handling failures gracefully. This includes the ability to disconnect from the market quickly and orderly if necessary, as well as redundant systems to ensure continuity in the event of hardware or software failures. Resiliency also involves implementing pre-trade and post-trade controls to prevent errors and detect potential market abuse, ensuring that the trading process remains both efficient and compliant.","type":"content","url":"/markdown/algorithmic-trading#core-requirements-of-algorithmic-trading-infrastructure","position":31},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl3":"Key Components of Algorithmic Trading Infrastructure","lvl2":"Algorithmic Trading Infrastructure"},"type":"lvl3","url":"/markdown/algorithmic-trading#key-components-of-algorithmic-trading-infrastructure","position":32},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl3":"Key Components of Algorithmic Trading Infrastructure","lvl2":"Algorithmic Trading Infrastructure"},"content":"A robust algorithmic trading system relies on modular, interconnected components, each designed for a distinct role in the trading workflow. While real-world setups may consolidate some functions—especially in smaller or simpler systems—the architecture naturally evolves toward this specialized structure as complexity and scale increase. Below, we break down the key elements of this framework, as illustrated in the following figure.\n\n\n\nFigure 4:Architecture of an algorithmic trading system. The overall principle for the template is based on encapsulation and functional specialization of each component.","type":"content","url":"/markdown/algorithmic-trading#key-components-of-algorithmic-trading-infrastructure","position":33},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl4":"Algorithmic Trading Server","lvl3":"Key Components of Algorithmic Trading Infrastructure","lvl2":"Algorithmic Trading Infrastructure"},"type":"lvl4","url":"/markdown/algorithmic-trading#algorithmic-trading-server","position":34},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl4":"Algorithmic Trading Server","lvl3":"Key Components of Algorithmic Trading Infrastructure","lvl2":"Algorithmic Trading Infrastructure"},"content":"The algorithmic trading server, often referred to as a “strategy container,” is the core platform where trading algorithms are executed. It generates orders based on the algorithm’s logic and reacts to real-time market data. There are several approaches to building this component, each with its own advantages and trade-offs.\n\nFirms may choose to use third-party servers that include a suite of pre-built algorithms. These platforms often provide tools for developing proprietary strategies, ranging from support for traditional programming languages to complex graphical interfaces. Examples of such providers are Algo Trader, Pragma360 and QuantConnect.\n\nOn the other hand, large brokers, dealers, and hedge funds often develop their own algorithmic trading servers in-house. This approach allows for greater customization and differentiation, as proprietary algorithms can be tailored to specific market conditions and trading objectives. The choice of programming language depends on performance requirements, with high-performance code typically written in languages like Java, C#, or C++, while Python and MATLAB are often used for prototyping and less latency-sensitive applications.\n\nThe server typically employs complex event processing (CEP) logic to handle real-time data streams and make rapid trading decisions. They are also sometimes built using reactive programming principles, which allow the system to respond dynamically to changes in market conditions, further enhancing the agility and responsiveness of the trading infrastructure.","type":"content","url":"/markdown/algorithmic-trading#algorithmic-trading-server","position":35},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl4":"Market Data Server (MDS)","lvl3":"Key Components of Algorithmic Trading Infrastructure","lvl2":"Algorithmic Trading Infrastructure"},"type":"lvl4","url":"/markdown/algorithmic-trading#market-data-server-mds","position":36},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl4":"Market Data Server (MDS)","lvl3":"Key Components of Algorithmic Trading Infrastructure","lvl2":"Algorithmic Trading Infrastructure"},"content":"The Market Data Server (MDS) serves as the central hub for both historical and real-time market data, which is essential for algorithm calibration, backtesting, and live trading. Historical data is stored in a “data store,” “data lake,” or “data repository” and is used for calibrating and backtesting strategies. KDB+ remains the industry standard for handling time-series data due to its in-memory performance and ability to process vast datasets at high speed. It is widely used in finance for algorithmic trading, risk management, and market surveillance, particularly in environments where latency and accuracy are paramount.\n\nFor real-time data, firms can obtain market data either directly from trading venues or through vendors like LSEG (acronym for London Stock Exchange Group, which bought Refinitiv, previously known as Thomson Reuters)  or Bloomberg. While vendors offer the advantage of a single point of access and standardized data formats, connecting directly to exchanges can reduce latency, which is critical for high-frequency and latency-sensitive strategies. For professional applications, it is recommended to have at least two alternative data sources to ensure redundancy and reliability.","type":"content","url":"/markdown/algorithmic-trading#market-data-server-mds","position":37},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl4":"Order Management System (OMS)","lvl3":"Key Components of Algorithmic Trading Infrastructure","lvl2":"Algorithmic Trading Infrastructure"},"type":"lvl4","url":"/markdown/algorithmic-trading#order-management-system-oms","position":38},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl4":"Order Management System (OMS)","lvl3":"Key Components of Algorithmic Trading Infrastructure","lvl2":"Algorithmic Trading Infrastructure"},"content":"The Order Management System (OMS) is responsible for handling all orders sent to trading venues. It manages order entry, routing, and status monitoring, as well as trade booking and reconciliation. Key functionalities include order entry, which can be done manually or via algorithms, and order routing, which involves encoding orders according to a standardized protocol and transmitting them to trading venues.\n\nThe Financial Information eXchange (FIX) protocol remains the most widely used standard in electronic trading, providing a consistent and efficient means of communication between clients, brokers, and exchanges. OMS platforms such as Bloomberg AIM, Fidessa OMS, and FIS Valdi continue to be prominent in the industry, offering robust solutions for managing order flow across multiple asset classes and regions.","type":"content","url":"/markdown/algorithmic-trading#order-management-system-oms","position":39},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl4":"Execution Management System (EMS)","lvl3":"Key Components of Algorithmic Trading Infrastructure","lvl2":"Algorithmic Trading Infrastructure"},"type":"lvl4","url":"/markdown/algorithmic-trading#execution-management-system-ems","position":40},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl4":"Execution Management System (EMS)","lvl3":"Key Components of Algorithmic Trading Infrastructure","lvl2":"Algorithmic Trading Infrastructure"},"content":"While OMS platforms focus on order management, Execution Management Systems (EMS) are specialized for order execution and algorithmic trading. EMS platforms often overlap with OMS in functionality but are more outward-facing, offering connectivity to multiple exchanges, brokers, and trading platforms. They provide tools for pre-trade and post-trade analysis, real-time monitoring, and Direct Market Access (DMA).\n\nBloomberg EMSX remains a leading EMS platform, offering global, broker-neutral equities and futures trading, as well as integration with Bloomberg’s buy-side OMS, AIM. EMSX supports a wide range of asset classes, including equities, futures, options, and index swaps, and provides access to nearly every listed market through its extensive network of broker destinations.","type":"content","url":"/markdown/algorithmic-trading#execution-management-system-ems","position":41},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl4":"Market Making Trading Platforms","lvl3":"Key Components of Algorithmic Trading Infrastructure","lvl2":"Algorithmic Trading Infrastructure"},"type":"lvl4","url":"/markdown/algorithmic-trading#market-making-trading-platforms","position":42},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl4":"Market Making Trading Platforms","lvl3":"Key Components of Algorithmic Trading Infrastructure","lvl2":"Algorithmic Trading Infrastructure"},"content":"For firms engaged in market-making activities, specialized platforms are available that provide tools for managing Request for Quote (RfQ) and Request for Stream (RfS) workflows. These platforms typically include pricing and quoting engines, auto-quoting and auto-hedging capabilities, and integration with OMS and EMS.\n\nItiviti, now part of Broadridge, continues to offer a suite of market-making solutions, including Tbricks and Orc Trader, which are designed for listed derivatives and provide real-time risk control and customization. Numerix Oneview for Trading ioffers real-time pricing, market data management, and risk calculations for structured products and derivatives. ION Trading is a major player offering platforms for Fixed Income trading.","type":"content","url":"/markdown/algorithmic-trading#market-making-trading-platforms","position":43},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl4":"Analytics and Backtesting Server","lvl3":"Key Components of Algorithmic Trading Infrastructure","lvl2":"Algorithmic Trading Infrastructure"},"type":"lvl4","url":"/markdown/algorithmic-trading#analytics-and-backtesting-server","position":44},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl4":"Analytics and Backtesting Server","lvl3":"Key Components of Algorithmic Trading Infrastructure","lvl2":"Algorithmic Trading Infrastructure"},"content":"The analytics server is used for off-line analysis, including the calibration of algorithm parameters and performance evaluation. It relies on historical or synthetic data to simulate how strategies would have performed under different market conditions. Historical data reflects real market conditions but is limited to past scenarios, while synthetic data can be generated to simulate a wider range of market conditions, including extreme or unlikely scenarios.\n\nPlatforms like QuantConnect and Deltix provide comprehensive backtesting and analytics capabilities, allowing traders to test and refine their strategies before deployment.","type":"content","url":"/markdown/algorithmic-trading#analytics-and-backtesting-server","position":45},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl4":"Profit & Loss (P&L) and Risk Server","lvl3":"Key Components of Algorithmic Trading Infrastructure","lvl2":"Algorithmic Trading Infrastructure"},"type":"lvl4","url":"/markdown/algorithmic-trading#profit-loss-p-l-and-risk-server","position":46},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl4":"Profit & Loss (P&L) and Risk Server","lvl3":"Key Components of Algorithmic Trading Infrastructure","lvl2":"Algorithmic Trading Infrastructure"},"content":"The P&L and risk server tracks the real-time performance of trading positions, providing metrics such as profit and loss, exposure, and risk indicators. These systems are often integrated with the firm’s broader risk management infrastructure but may include specialized features for algorithmic trading.\n\nMurex MX.3 remains a leading platform in this space, offering integrated solutions for trading, risk management, and processing across a wide range of asset classes. It provides real-time risk monitoring and analysis, enabling institutions to identify and mitigate potential risks promptly.","type":"content","url":"/markdown/algorithmic-trading#profit-loss-p-l-and-risk-server","position":47},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl4":"Infrastructure for Small Firms and Private Investors","lvl3":"Key Components of Algorithmic Trading Infrastructure","lvl2":"Algorithmic Trading Infrastructure"},"type":"lvl4","url":"/markdown/algorithmic-trading#infrastructure-for-small-firms-and-private-investors","position":48},{"hierarchy":{"lvl1":"Algorithmic Trading","lvl4":"Infrastructure for Small Firms and Private Investors","lvl3":"Key Components of Algorithmic Trading Infrastructure","lvl2":"Algorithmic Trading Infrastructure"},"content":"Not all market participants require the same level of infrastructure. Small firms and private investors often rely on third-party solutions that bundle algorithmic trading servers, analytics, backtesting, and connectivity to brokers and exchanges.\n\nPlatforms like MetaTrader 5 (MT5), Interactive Brokers, and QuantConnect offer accessible and cost-effective alternatives, providing a range of tools for developing, testing, and deploying trading strategies. Crowdsourced hedge funds such as Numerai and Quantiacs continue to provide environments for developing and backtesting trading algorithms, with the best-performing strategies potentially being included in the fund’s portfolio.\n\nFor those with limited resources, open-source libraries like PyAlgoTrade, Zipline, and Backtrader provide cost-effective alternatives for backtesting and strategy development. Cloud infrastructure from providers like AWS, Google Cloud, and Microsoft Azure further enhances accessibility, allowing traders to scale their operations without significant upfront investment.","type":"content","url":"/markdown/algorithmic-trading#infrastructure-for-small-firms-and-private-investors","position":49},{"hierarchy":{"lvl1":"The Almgren - Chriss Framework"},"type":"lvl1","url":"/markdown/almgren-chriss","position":0},{"hierarchy":{"lvl1":"The Almgren - Chriss Framework"},"content":"","type":"content","url":"/markdown/almgren-chriss","position":1},{"hierarchy":{"lvl1":"The Almgren - Chriss Framework","lvl2":"Introduction"},"type":"lvl2","url":"/markdown/almgren-chriss#introduction","position":2},{"hierarchy":{"lvl1":"The Almgren - Chriss Framework","lvl2":"Introduction"},"content":"","type":"content","url":"/markdown/almgren-chriss#introduction","position":3},{"hierarchy":{"lvl1":"The Almgren - Chriss Framework","lvl2":"Almgren - Chriss model"},"type":"lvl2","url":"/markdown/almgren-chriss#almgren-chriss-model","position":4},{"hierarchy":{"lvl1":"The Almgren - Chriss Framework","lvl2":"Almgren - Chriss model"},"content":"","type":"content","url":"/markdown/almgren-chriss#almgren-chriss-model","position":5},{"hierarchy":{"lvl1":"The Almgren - Chriss Framework","lvl2":"Implementation Shortfall target"},"type":"lvl2","url":"/markdown/almgren-chriss#implementation-shortfall-target","position":6},{"hierarchy":{"lvl1":"The Almgren - Chriss Framework","lvl2":"Implementation Shortfall target"},"content":"","type":"content","url":"/markdown/almgren-chriss#implementation-shortfall-target","position":7},{"hierarchy":{"lvl1":"The Almgren - Chriss Framework","lvl2":"VWAP target"},"type":"lvl2","url":"/markdown/almgren-chriss#vwap-target","position":8},{"hierarchy":{"lvl1":"The Almgren - Chriss Framework","lvl2":"VWAP target"},"content":"","type":"content","url":"/markdown/almgren-chriss#vwap-target","position":9},{"hierarchy":{"lvl1":"The Almgren - Chriss Framework","lvl2":"Portfolio execution"},"type":"lvl2","url":"/markdown/almgren-chriss#portfolio-execution","position":10},{"hierarchy":{"lvl1":"The Almgren - Chriss Framework","lvl2":"Portfolio execution"},"content":"","type":"content","url":"/markdown/almgren-chriss#portfolio-execution","position":11},{"hierarchy":{"lvl1":"The Almgren - Chriss Framework","lvl2":"Exercises"},"type":"lvl2","url":"/markdown/almgren-chriss#exercises","position":12},{"hierarchy":{"lvl1":"The Almgren - Chriss Framework","lvl2":"Exercises"},"content":"","type":"content","url":"/markdown/almgren-chriss#exercises","position":13},{"hierarchy":{"lvl1":"Arbitrage Strategies"},"type":"lvl1","url":"/markdown/arbitrage","position":0},{"hierarchy":{"lvl1":"Arbitrage Strategies"},"content":"","type":"content","url":"/markdown/arbitrage","position":1},{"hierarchy":{"lvl1":"Arbitrage Strategies","lvl2":"Introduction"},"type":"lvl2","url":"/markdown/arbitrage#introduction","position":2},{"hierarchy":{"lvl1":"Arbitrage Strategies","lvl2":"Introduction"},"content":"","type":"content","url":"/markdown/arbitrage#introduction","position":3},{"hierarchy":{"lvl1":"Arbitrage Strategies","lvl2":"Exercises"},"type":"lvl2","url":"/markdown/arbitrage#exercises","position":4},{"hierarchy":{"lvl1":"Arbitrage Strategies","lvl2":"Exercises"},"content":"","type":"content","url":"/markdown/arbitrage#exercises","position":5},{"hierarchy":{"lvl1":"The Avellaneda and Stoikov Framework"},"type":"lvl1","url":"/markdown/avellaneda-stoikov","position":0},{"hierarchy":{"lvl1":"The Avellaneda and Stoikov Framework"},"content":"","type":"content","url":"/markdown/avellaneda-stoikov","position":1},{"hierarchy":{"lvl1":"The Avellaneda and Stoikov Framework","lvl2":"Introduction"},"type":"lvl2","url":"/markdown/avellaneda-stoikov#introduction","position":2},{"hierarchy":{"lvl1":"The Avellaneda and Stoikov Framework","lvl2":"Introduction"},"content":"","type":"content","url":"/markdown/avellaneda-stoikov#introduction","position":3},{"hierarchy":{"lvl1":"The Avellaneda and Stoikov Framework","lvl2":"The single trade model"},"type":"lvl2","url":"/markdown/avellaneda-stoikov#the-single-trade-model","position":4},{"hierarchy":{"lvl1":"The Avellaneda and Stoikov Framework","lvl2":"The single trade model"},"content":"","type":"content","url":"/markdown/avellaneda-stoikov#the-single-trade-model","position":5},{"hierarchy":{"lvl1":"The Avellaneda and Stoikov Framework","lvl2":"The Avellaneda and Stoikov models"},"type":"lvl2","url":"/markdown/avellaneda-stoikov#the-avellaneda-and-stoikov-models","position":6},{"hierarchy":{"lvl1":"The Avellaneda and Stoikov Framework","lvl2":"The Avellaneda and Stoikov models"},"content":"","type":"content","url":"/markdown/avellaneda-stoikov#the-avellaneda-and-stoikov-models","position":7},{"hierarchy":{"lvl1":"The Avellaneda and Stoikov Framework","lvl2":"Asymptotic approximations"},"type":"lvl2","url":"/markdown/avellaneda-stoikov#asymptotic-approximations","position":8},{"hierarchy":{"lvl1":"The Avellaneda and Stoikov Framework","lvl2":"Asymptotic approximations"},"content":"","type":"content","url":"/markdown/avellaneda-stoikov#asymptotic-approximations","position":9},{"hierarchy":{"lvl1":"The Avellaneda and Stoikov Framework","lvl2":"The mean reversion argument"},"type":"lvl2","url":"/markdown/avellaneda-stoikov#the-mean-reversion-argument","position":10},{"hierarchy":{"lvl1":"The Avellaneda and Stoikov Framework","lvl2":"The mean reversion argument"},"content":"","type":"content","url":"/markdown/avellaneda-stoikov#the-mean-reversion-argument","position":11},{"hierarchy":{"lvl1":"The Avellaneda and Stoikov Framework","lvl2":"Applications to LOBs and RfQs"},"type":"lvl2","url":"/markdown/avellaneda-stoikov#applications-to-lobs-and-rfqs","position":12},{"hierarchy":{"lvl1":"The Avellaneda and Stoikov Framework","lvl2":"Applications to LOBs and RfQs"},"content":"","type":"content","url":"/markdown/avellaneda-stoikov#applications-to-lobs-and-rfqs","position":13},{"hierarchy":{"lvl1":"The Avellaneda and Stoikov Framework","lvl2":"Exercises"},"type":"lvl2","url":"/markdown/avellaneda-stoikov#exercises","position":14},{"hierarchy":{"lvl1":"The Avellaneda and Stoikov Framework","lvl2":"Exercises"},"content":"","type":"content","url":"/markdown/avellaneda-stoikov#exercises","position":15},{"hierarchy":{"lvl1":"Data-driven methods"},"type":"lvl1","url":"/markdown/data-driven-methods","position":0},{"hierarchy":{"lvl1":"Data-driven methods"},"content":"","type":"content","url":"/markdown/data-driven-methods","position":1},{"hierarchy":{"lvl1":"Data-driven methods","lvl2":"Introduction"},"type":"lvl2","url":"/markdown/data-driven-methods#introduction","position":2},{"hierarchy":{"lvl1":"Data-driven methods","lvl2":"Introduction"},"content":"Not really an advanced introduction, just to set the language. Many books on this topics","type":"content","url":"/markdown/data-driven-methods#introduction","position":3},{"hierarchy":{"lvl1":"Data-driven methods","lvl2":"Data-driven methodologies"},"type":"lvl2","url":"/markdown/data-driven-methods#data-driven-methodologies","position":4},{"hierarchy":{"lvl1":"Data-driven methods","lvl2":"Data-driven methodologies"},"content":"train set validation\n\ntypes: supervised, unsupervised, reinforcement learning\n\nsupervised: classification, regression","type":"content","url":"/markdown/data-driven-methods#data-driven-methodologies","position":5},{"hierarchy":{"lvl1":"Data-driven methods","lvl2":"Supervised learning"},"type":"lvl2","url":"/markdown/data-driven-methods#supervised-learning","position":6},{"hierarchy":{"lvl1":"Data-driven methods","lvl2":"Supervised learning"},"content":"","type":"content","url":"/markdown/data-driven-methods#supervised-learning","position":7},{"hierarchy":{"lvl1":"Data-driven methods","lvl3":"Regression","lvl2":"Supervised learning"},"type":"lvl3","url":"/markdown/data-driven-methods#regression","position":8},{"hierarchy":{"lvl1":"Data-driven methods","lvl3":"Regression","lvl2":"Supervised learning"},"content":"","type":"content","url":"/markdown/data-driven-methods#regression","position":9},{"hierarchy":{"lvl1":"Data-driven methods","lvl3":"Classification","lvl2":"Supervised learning"},"type":"lvl3","url":"/markdown/data-driven-methods#classification","position":10},{"hierarchy":{"lvl1":"Data-driven methods","lvl3":"Classification","lvl2":"Supervised learning"},"content":"","type":"content","url":"/markdown/data-driven-methods#classification","position":11},{"hierarchy":{"lvl1":"Data-driven methods","lvl2":"Unsupervised learning"},"type":"lvl2","url":"/markdown/data-driven-methods#unsupervised-learning","position":12},{"hierarchy":{"lvl1":"Data-driven methods","lvl2":"Unsupervised learning"},"content":"","type":"content","url":"/markdown/data-driven-methods#unsupervised-learning","position":13},{"hierarchy":{"lvl1":"Data-driven methods","lvl2":"Reinforcement learning"},"type":"lvl2","url":"/markdown/data-driven-methods#reinforcement-learning","position":14},{"hierarchy":{"lvl1":"Data-driven methods","lvl2":"Reinforcement learning"},"content":"","type":"content","url":"/markdown/data-driven-methods#reinforcement-learning","position":15},{"hierarchy":{"lvl1":"Data-driven methods","lvl2":"Exercises"},"type":"lvl2","url":"/markdown/data-driven-methods#exercises","position":16},{"hierarchy":{"lvl1":"Data-driven methods","lvl2":"Exercises"},"content":"","type":"content","url":"/markdown/data-driven-methods#exercises","position":17},{"hierarchy":{"lvl1":"Dedication"},"type":"lvl1","url":"/markdown/dedication","position":0},{"hierarchy":{"lvl1":"Dedication"},"content":"Dedicated to my family, for always standing by me as I work on this book, even when it means sometimes sacrificing our leisure time together.\n\nTo my team at BBVA, thank you for your valuable feedback, which helps me refine the clarity and content of this work\n\nJavier Sabio González","type":"content","url":"/markdown/dedication","position":1},{"hierarchy":{"lvl1":"Enriching the Avellaneda and Stoikov Model"},"type":"lvl1","url":"/markdown/enriching-avellaneda","position":0},{"hierarchy":{"lvl1":"Enriching the Avellaneda and Stoikov Model"},"content":"","type":"content","url":"/markdown/enriching-avellaneda","position":1},{"hierarchy":{"lvl1":"Enriching the Avellaneda and Stoikov Model","lvl2":"Introduction"},"type":"lvl2","url":"/markdown/enriching-avellaneda#introduction","position":2},{"hierarchy":{"lvl1":"Enriching the Avellaneda and Stoikov Model","lvl2":"Introduction"},"content":"","type":"content","url":"/markdown/enriching-avellaneda#introduction","position":3},{"hierarchy":{"lvl1":"Enriching the Avellaneda and Stoikov Model","lvl2":"Liquidity"},"type":"lvl2","url":"/markdown/enriching-avellaneda#liquidity","position":4},{"hierarchy":{"lvl1":"Enriching the Avellaneda and Stoikov Model","lvl2":"Liquidity"},"content":"","type":"content","url":"/markdown/enriching-avellaneda#liquidity","position":5},{"hierarchy":{"lvl1":"Enriching the Avellaneda and Stoikov Model","lvl2":"Relative value models"},"type":"lvl2","url":"/markdown/enriching-avellaneda#relative-value-models","position":6},{"hierarchy":{"lvl1":"Enriching the Avellaneda and Stoikov Model","lvl2":"Relative value models"},"content":"","type":"content","url":"/markdown/enriching-avellaneda#relative-value-models","position":7},{"hierarchy":{"lvl1":"Enriching the Avellaneda and Stoikov Model","lvl2":"Flow imbalance"},"type":"lvl2","url":"/markdown/enriching-avellaneda#flow-imbalance","position":8},{"hierarchy":{"lvl1":"Enriching the Avellaneda and Stoikov Model","lvl2":"Flow imbalance"},"content":"","type":"content","url":"/markdown/enriching-avellaneda#flow-imbalance","position":9},{"hierarchy":{"lvl1":"Enriching the Avellaneda and Stoikov Model","lvl2":"Client profiling"},"type":"lvl2","url":"/markdown/enriching-avellaneda#client-profiling","position":10},{"hierarchy":{"lvl1":"Enriching the Avellaneda and Stoikov Model","lvl2":"Client profiling"},"content":"","type":"content","url":"/markdown/enriching-avellaneda#client-profiling","position":11},{"hierarchy":{"lvl1":"Enriching the Avellaneda and Stoikov Model","lvl2":"Exercises"},"type":"lvl2","url":"/markdown/enriching-avellaneda#exercises","position":12},{"hierarchy":{"lvl1":"Enriching the Avellaneda and Stoikov Model","lvl2":"Exercises"},"content":"","type":"content","url":"/markdown/enriching-avellaneda#exercises","position":13},{"hierarchy":{"lvl1":"Execution fundamentals"},"type":"lvl1","url":"/markdown/execution-fundamentals","position":0},{"hierarchy":{"lvl1":"Execution fundamentals"},"content":"","type":"content","url":"/markdown/execution-fundamentals","position":1},{"hierarchy":{"lvl1":"Execution fundamentals","lvl2":"Introduction"},"type":"lvl2","url":"/markdown/execution-fundamentals#introduction","position":2},{"hierarchy":{"lvl1":"Execution fundamentals","lvl2":"Introduction"},"content":"","type":"content","url":"/markdown/execution-fundamentals#introduction","position":3},{"hierarchy":{"lvl1":"Execution fundamentals","lvl2":"The execution business objective"},"type":"lvl2","url":"/markdown/execution-fundamentals#the-execution-business-objective","position":4},{"hierarchy":{"lvl1":"Execution fundamentals","lvl2":"The execution business objective"},"content":"","type":"content","url":"/markdown/execution-fundamentals#the-execution-business-objective","position":5},{"hierarchy":{"lvl1":"Execution fundamentals","lvl2":"Execution Risks and Tradeoffs"},"type":"lvl2","url":"/markdown/execution-fundamentals#execution-risks-and-tradeoffs","position":6},{"hierarchy":{"lvl1":"Execution fundamentals","lvl2":"Execution Risks and Tradeoffs"},"content":"","type":"content","url":"/markdown/execution-fundamentals#execution-risks-and-tradeoffs","position":7},{"hierarchy":{"lvl1":"Execution fundamentals","lvl2":"Execution Benchmarks"},"type":"lvl2","url":"/markdown/execution-fundamentals#execution-benchmarks","position":8},{"hierarchy":{"lvl1":"Execution fundamentals","lvl2":"Execution Benchmarks"},"content":"","type":"content","url":"/markdown/execution-fundamentals#execution-benchmarks","position":9},{"hierarchy":{"lvl1":"Execution fundamentals","lvl2":"Execution strategies and execution tactics"},"type":"lvl2","url":"/markdown/execution-fundamentals#execution-strategies-and-execution-tactics","position":10},{"hierarchy":{"lvl1":"Execution fundamentals","lvl2":"Execution strategies and execution tactics"},"content":"","type":"content","url":"/markdown/execution-fundamentals#execution-strategies-and-execution-tactics","position":11},{"hierarchy":{"lvl1":"Execution fundamentals","lvl2":"Transaction Cost Analysis"},"type":"lvl2","url":"/markdown/execution-fundamentals#transaction-cost-analysis","position":12},{"hierarchy":{"lvl1":"Execution fundamentals","lvl2":"Transaction Cost Analysis"},"content":"","type":"content","url":"/markdown/execution-fundamentals#transaction-cost-analysis","position":13},{"hierarchy":{"lvl1":"Execution fundamentals","lvl2":"Exercises"},"type":"lvl2","url":"/markdown/execution-fundamentals#exercises","position":14},{"hierarchy":{"lvl1":"Execution fundamentals","lvl2":"Exercises"},"content":"","type":"content","url":"/markdown/execution-fundamentals#exercises","position":15},{"hierarchy":{"lvl1":"Execution tactics"},"type":"lvl1","url":"/markdown/execution-tactics","position":0},{"hierarchy":{"lvl1":"Execution tactics"},"content":"","type":"content","url":"/markdown/execution-tactics","position":1},{"hierarchy":{"lvl1":"Execution tactics","lvl2":"Introduction"},"type":"lvl2","url":"/markdown/execution-tactics#introduction","position":2},{"hierarchy":{"lvl1":"Execution tactics","lvl2":"Introduction"},"content":"","type":"content","url":"/markdown/execution-tactics#introduction","position":3},{"hierarchy":{"lvl1":"Execution tactics","lvl2":"Vanilla strategy"},"type":"lvl2","url":"/markdown/execution-tactics#vanilla-strategy","position":4},{"hierarchy":{"lvl1":"Execution tactics","lvl2":"Vanilla strategy"},"content":"","type":"content","url":"/markdown/execution-tactics#vanilla-strategy","position":5},{"hierarchy":{"lvl1":"Execution tactics","lvl2":"Stochastic optimal control"},"type":"lvl2","url":"/markdown/execution-tactics#stochastic-optimal-control","position":6},{"hierarchy":{"lvl1":"Execution tactics","lvl2":"Stochastic optimal control"},"content":"","type":"content","url":"/markdown/execution-tactics#stochastic-optimal-control","position":7},{"hierarchy":{"lvl1":"Execution tactics","lvl2":"Reinforcement learning strategies"},"type":"lvl2","url":"/markdown/execution-tactics#reinforcement-learning-strategies","position":8},{"hierarchy":{"lvl1":"Execution tactics","lvl2":"Reinforcement learning strategies"},"content":"","type":"content","url":"/markdown/execution-tactics#reinforcement-learning-strategies","position":9},{"hierarchy":{"lvl1":"Execution tactics","lvl2":"Exercises"},"type":"lvl2","url":"/markdown/execution-tactics#exercises","position":10},{"hierarchy":{"lvl1":"Execution tactics","lvl2":"Exercises"},"content":"","type":"content","url":"/markdown/execution-tactics#exercises","position":11},{"hierarchy":{"lvl1":"Factor Investing"},"type":"lvl1","url":"/markdown/factor-investing","position":0},{"hierarchy":{"lvl1":"Factor Investing"},"content":"","type":"content","url":"/markdown/factor-investing","position":1},{"hierarchy":{"lvl1":"Factor Investing","lvl2":"Introduction"},"type":"lvl2","url":"/markdown/factor-investing#introduction","position":2},{"hierarchy":{"lvl1":"Factor Investing","lvl2":"Introduction"},"content":"","type":"content","url":"/markdown/factor-investing#introduction","position":3},{"hierarchy":{"lvl1":"Factor Investing","lvl2":"Exercises"},"type":"lvl2","url":"/markdown/factor-investing#exercises","position":4},{"hierarchy":{"lvl1":"Factor Investing","lvl2":"Exercises"},"content":"","type":"content","url":"/markdown/factor-investing#exercises","position":5},{"hierarchy":{"lvl1":"Fair value estimation"},"type":"lvl1","url":"/markdown/fair-price-estimation","position":0},{"hierarchy":{"lvl1":"Fair value estimation"},"content":"","type":"content","url":"/markdown/fair-price-estimation","position":1},{"hierarchy":{"lvl1":"Fair value estimation","lvl2":"Introduction"},"type":"lvl2","url":"/markdown/fair-price-estimation#introduction","position":2},{"hierarchy":{"lvl1":"Fair value estimation","lvl2":"Introduction"},"content":"In this chapter we introduce techniques to estimate the fair value of a financial instrument exploiting the pricing information available in financial markets.\n\nFair value is the price at which a financial instrument is economically equivalent, at a given time and under a given information set, to its future cash flows or payoffs, once transactions costs, opportunity costs and risk are appropriately accounted for\n\nIn this chapter we will focus on two conceptually different ways to estimate fair value. The first one assumes markets are efficient enough so that prices observed in the market are our best estimation of fair value, bar some idiosyncratic components coming from trading fictions like liquidity premiums, dealer spreads, and transaction costs. In this case, fair value estimation can be seen as a filtering problem, and we will study a simple albeit powerful model to carry out this task: the Kalman filter.\n\nWhen financial instruments are highly illiquid or do not trade directly in organized markets, fair value estimation must rely on economic—often referred to as fundamental—valuation models. These models infer the value of a financial instrument from the future cash flows specified by its contractual structure. Central to this approach is the concept of the time value of money, which states that a payment received in the future is not economically equivalent to the same payment received today, due to opportunity costs. As a result, future cash flows must be converted into present values through the application of a discount factor, which renders cash flows occurring at different points in time comparable.\n\nTime discounting, however, is not the only challenge faced by fundamental valuation models. Future cash flows are frequently contingent on information that is not known at the valuation date, such as future prices of financial assets or macroeconomic variables. Examples include dividends paid by a company or the value of the underlying asset referenced by a derivative contract. A simple, albeit theoretically naive, approach consists in valuing the instrument as the expected value of its future cash flows, treated as random variables. This approach, however, fails to account for heterogeneity in investors’ risk preferences. Once cash flows are stochastic, the realized return on the investment becomes uncertain, and uncertainty is not valued equally by all investors. To address this limitation, one can adopt a utility indifference pricing framework, in which risk preferences explicitly enter the valuation.\n\nIn many situations, particularly for illiquid flow instruments, this is the most refined valuation approach available. However, for the specific case of derivative instruments, stronger theoretical results can be obtained under additional assumptions. As shown by Fischer Black, Myron Scholes, and Robert C. Merton in the 1970s, it is possible to construct dynamic replication portfolios that reproduce the payoffs of a derivative using traded instruments. In such settings, the fair value of the derivative becomes independent of investors’ risk aversion, since any deviation from this price would give rise to risk-free arbitrage opportunities. This insight leads to the arbitrage-free pricing framework, which will be introduced in the final section of this chapter.\n\nFinally, a unifying pricing framework that can accommodate both the risk-aversion profile of the investor and the arbitrage-free constraints is the stochastic discount factor pricing framework \n\nCochrane, 2005, which we will briefly describe at the end of the chapter.","type":"content","url":"/markdown/fair-price-estimation#introduction","position":3},{"hierarchy":{"lvl1":"Fair value estimation","lvl2":"Filtering models for fair value estimation"},"type":"lvl2","url":"/markdown/fair-price-estimation#filtering-models-for-fair-value-estimation","position":4},{"hierarchy":{"lvl1":"Fair value estimation","lvl2":"Filtering models for fair value estimation"},"content":"As mentioned in the introduction, for financial instruments that are relatively liquid, we can aim at extracting all the pricing information from price indications and trades in the market, without having to resort to economic theories of fair value. In this setup, we consider as their fair value the one that market participants are willing to pay for.\n\nThe issue, though, is that price indications and trades cannot be considered themselves pure observations of fair value, since they might be affected by market frictions: bid ask spreads, particularities of the negotiation mechanism, liquidity fluctuations, specific needs of market participants at a given time, etc. When instruments trade in limit order books, a popular estimation of the fair value is using the mid-price, the arithmetic average of the best bid and ask. However, if bid-ask spreads are wide of liquidity is thin in the first levels, such estimation is not necessary very precise. Trades provide a lot of information, since they are real transaction and not indications of interests, the larger they are in principle the more information. Still, they are subject to the aforementioned market frictions that reduce their reliability.\n\nThese makes all these price observations noisy estimates of the fair value, so if we want to estimate a fair value out of them we need to be able to separate the signal from the noise, or in other words, filter those observations. This is precisely what, under certain model assumptions, a Kalman filter does.\n\nThe Kalman filter was introduced in the chapter on \n\nBayesian Theory. It is a Bayesian filtering algorithm that allows to perform exact inference, i.e. compute the closed-form distribution, of the latent state vector in a Linear Gaussian State Space Model (LG-SSM).\n\nRecall that a State Space Model (SSM) is a model to describe dynamic systems where we have a non or partially observable state, a vector \\mathbf{x}, whose dynamics in time is described by a so-called transition equation:\\mathbf{x}_{t+\\Delta t} = f(\\mathbf{x}_t, \\mathbf{u}_t) + \\mathbf{\\epsilon}_t\n\nwhere f(\\mathbf{x}_t, \\mathbf{u}_t) is a general function, \\mathbf{u}_t are inputs (or controls) that affect the dynamics, \\Delta t is the time-step between observations, and \\mathbf{ \\epsilon}_t is a transition noise with a given distribution. The state is observed indirectly via a proxy vector \\mathbf{y} via the observation equation:\\mathbf{y}_t = g(\\mathbf{x}_t, \\mathbf{u}_t)  + \\mathbf{\\eta}\n\nwhere g(\\mathbf{x}_t, \\mathbf{u}_t) is another general function and \\mathbf{\\eta}_t the observation noise, meaning that observations have a degree of uncertainty with respect to the latent space.\n\nA Linear Gaussian Model (LGM) is a specific case of the SSM were both the transition and observation functions are linear and the noise terms are Gaussian. In this case, we can use the Kalman filter algorithm to compute the distribution of the state vector at any time, given the observations and the transition and observation model. If some or all the parameters of these models are not known, they can be estimated using standard techniques like Maximum Likelihood Estimation (MLE) or Expectation Maximization (EM) when the former becomes computationally intractable due to the latent state vector.\n\nFor non-Linear Gaussian Models, there are extensions of the Kalman filter that can be used:\n\nExtended Kalman Filter (EKF): Extends the Kalman Filter to non-linear state space models by linearizing the dynamics and observation models around the current estimate using Taylor expansions.\n\nUnscented Kalman Filter (UKF): Avoids linearization by using deterministic sampling to approximate the state distribution.\n\nParticle filtering / sequential Monte Carlo: it uses directly Monte Carlo methods to find the posterior distribution of the fair value. See \n\nGu'eant & Pu, 2018 for more details of this approach applied to the fair-price estimation problem in corporate bonds.","type":"content","url":"/markdown/fair-price-estimation#filtering-models-for-fair-value-estimation","position":5},{"hierarchy":{"lvl1":"Fair value estimation","lvl3":"A simple pricing model","lvl2":"Filtering models for fair value estimation"},"type":"lvl3","url":"/markdown/fair-price-estimation#a-simple-pricing-model","position":6},{"hierarchy":{"lvl1":"Fair value estimation","lvl3":"A simple pricing model","lvl2":"Filtering models for fair value estimation"},"content":"Let us consider a simple setup where we aim to infer the distribution of the fair value m_t of a financial instrument that follows a random walk:m_{t+\\Delta t} = m_t + \\epsilon_t, \\epsilon_t \\sim N(0, \\sigma_\\epsilon^2 \\Delta t)\n\nWe don’t observe this fair value, only trades which we consider noisy observation of the mid since they include transaction costs and potentially other external factors like dealer inventory positions, etc:p_t = m_t + \\nu_t, \\nu_t \\sim N(0, \\sigma_\\nu^2)\n\nReaders will recognize that this is the local level model discussed extensively in the Chapter on \n\nBayesian Modelling. For the observation noise we can introduce prior business knowledge about the confidence we have on trade observations as a source of pricing information. In his Option Trading’s book \n\nSinclair, 2010, Euan Sinclair describes a simple model that quantifies the information provided by trades based on the size of the trade, v:\\sigma_\\nu (v)= \\sigma_p \\left(\\frac{v_\\text{max}}{v}-1)\\right)^+\n\nwhere \\sigma_p is a baseline observation noise and v_\\text{max} is an input to the model, the trade size we believe saturates information the information provided in the sense that our mid estimation will essentially move the price of the trade. In contrast, trades of small size, v \\ll v_\\text{max}, will have \\sigma_p \\rightarrow \\infty and will provide a negligible pricing information. An alternative simple model is:\\sigma_\\nu(v) = \\sigma_p \\frac{v_0}{v}\n\nwhere v_0 in this case is a size scale that separates the regimes where the information provided by the trade is negligible, v \\ll v_0, or relevant v \\gg v_0, but it does not saturate for a specific trading size, as in Sinclair’s model. Of course nothing prevents to use more business prior knowledge to enrich the observation model with other observable characteristics of the trade or the wider market context.","type":"content","url":"/markdown/fair-price-estimation#a-simple-pricing-model","position":7},{"hierarchy":{"lvl1":"Fair value estimation","lvl3":"Estimation of the simple pricing model","lvl2":"Filtering models for fair value estimation"},"type":"lvl3","url":"/markdown/fair-price-estimation#estimation-of-the-simple-pricing-model","position":8},{"hierarchy":{"lvl1":"Fair value estimation","lvl3":"Estimation of the simple pricing model","lvl2":"Filtering models for fair value estimation"},"content":"As discussed in \n\nBayesian Modelling, the standard way to estimate the parameters of a Kalman Filter is using the Expectation Maximization (EM) algorithm, suitable for probabilistic models with latent variables. However, the properties of the simple pricing model can be exploited to obtain closed-form estimators for its parameters using moment matching.\n\nLet us start by working with a model where observation errors have no dependency on the volume: \\sigma_\\nu (v) = \\sigma_\\nu. The key is to compute statistics of:d_t \\equiv p_{t+\\Delta t} - p_t = (m_{t+\\Delta t}- m_t) + (\\nu_{t+\\Delta t} - \\nu_t) = \\epsilon_t + (\\nu_{t+\\Delta t} - \\nu_t)\n\nwhich depend only on observed trades. First we compute the variance:Var[d_t]= Var[\\epsilon_t] + Var[(\\nu_{t+\\Delta t} - \\nu_t)]= \\sigma_\\epsilon^2 \\Delta t + 2 \\sigma_\\nu^2\n\nwhere we have used that \\epsilon_t, \\nu_{t+\\Delta t} and \\nu_t are independent random variables. This expression links the variance of the first differences in trade prices with the parameters to estimate. We need though a second expression to solve for each parameters separately. For that we compute the lag-1 auto-covariance of d_t:Cov[d_t,d_{t-\\Delta t}]=Cov[\\epsilon_t + (\\nu_{t+\\Delta t} - \\nu_t), \\epsilon_{t-\\Delta t} + (\\nu_t - \\nu_{t-\\Delta t})] = -Var[\\nu_t] = -\\sigma_\\nu^2\n\nwhere again we have used the independence between the noise terms. Our estimators read then:\\hat{\\sigma}_\\epsilon^2 \\Delta t = \\frac{1}{N-1} \\sum_{i=1}^{N-1} d_{t_i}^2 - 2 \\hat{\\sigma}_\\nu^2\\hat{\\sigma}_\\nu^2 = -\\frac{1}{N-2}\\sum_{i=2}^{N-1} d_{t_i} d_{t_{i}-\\Delta t}\n\nThe results have an interesting interpretation:\n\nStarting from the first equation: the variance of the fair value must be lower than the variance of the observed trades, given the additional noise in the observation equation. Or, in other words, since the fair value is filtered from the trades, which means that we estimate it by removing noise from the trades, it has to have a lower variance.\n\nThe second equation can be recognized as a form of the Roll estimator for effective bid-ask spreads, a typical measure of liquidity. Of course, by construction of the model, the noise that is filtered is essentially the bid-ask spread that liquidity providers request as a compensation for the liquidity provision.\n\nIn the case of volume dependent observation errors, we can still compute these statistics, which now read:Var[d_t]= \\sigma_\\epsilon^2 \\Delta t + \\sigma_\\nu^2(v_{t+\\Delta t}) + \\sigma_\\nu^2(v_t)Cov[d_t,d_{t-\\Delta t}]= -\\sigma_\\nu^2 (v_t)\n\nThe statistical estimator of the variance of the first differences can still be used, by accounting by the variability of the error with volume (heteroskedasticity):E[\\frac{1}{N-1} \\sum_{i=1}^{N-1} d_{t_i}^2]= \\frac{1}{N-1} \\sum_{i=1}^{N-1} \\left(\\sigma_\\epsilon^2 \\Delta t + \\sigma_\\nu^2(v_{t+\\Delta t}) + \\sigma_\\nu^2(v_t)\\right)=\\sigma_\\epsilon^2 \\Delta t+\\frac{1}{N-1} \\sum_{i=1}^{N-1} \\left( \\sigma_\\nu^2(v_{t+\\Delta t}) + \\sigma_\\nu^2(v_t)\\right)\n\nMoving into the 1-lag covariance, we have:E[\\frac{1}{N-2}\\sum_{i=2}^{N-1} d_{t_i} d_{t_{i}-\\Delta t}] = - \\frac{1}{N-2}\\sum_{i=2}^{N-1} \\sigma_v^2(v_{t_i})\n\nAs far as we the volume dependency has a single parameter to fit, we can still use these two equations to solve for the parameters. If we use, for instance, the simple model \\sigma_\\nu(v) = \\sigma_p \\frac{v_0}{v}, where v_0 is given and \\sigma_p is to be estimated from data (notice that we could simply estimate \\sigma_p v_0, the factorization is useful for business interpretation):E[\\frac{1}{N-1} \\sum_{i=1}^{N-1} d_{t_i}^2]=\\sigma_\\epsilon^2 \\Delta t+  \\frac{\\sigma_p^2}{N-1} \\sum_{i=1}^{N-1} \\left(  \\frac{v^2_0}{v^2_{t_i+\\Delta t}}  +  \\frac{v^2_0}{v^2_{t_i}} \\right)E[\\frac{1}{N-2}\\sum_{i=2}^{N-1} d_{t_i} d_{t_{i}-\\Delta t}] =  - \\frac{\\sigma_p^2}{N-2}\\sum_{i=2}^{N-1} \\frac{v^2_0}{v^2_{t_i}}\n\nIn this simple case, the estimation of the parameters \\sigma_p and \\sigma_\\epsilon is straightforward. More complex functions like the one from Sinclair cannot be estimated with only two moments. Further moments can be computed to provide extra equations, although at this point it might be worthy to resort to standard estimation techniques like EM if available.","type":"content","url":"/markdown/fair-price-estimation#estimation-of-the-simple-pricing-model","position":9},{"hierarchy":{"lvl1":"Fair value estimation","lvl3":"Inference on the simple pricing model","lvl2":"Filtering models for fair value estimation"},"type":"lvl3","url":"/markdown/fair-price-estimation#inference-on-the-simple-pricing-model","position":10},{"hierarchy":{"lvl1":"Fair value estimation","lvl3":"Inference on the simple pricing model","lvl2":"Filtering models for fair value estimation"},"content":"We can use the general Kalman filter equations described in \n\nBayesian Modelling to derive the distribution of our mid - price at the next time t + \\Delta t where a trade happens.\n\nThe Kalman filter algorithm operates sequentially over observation steps applying two steps, the predict step, where we compute the distribution of the fair value based purely on the random walk model, and the update step in which we incorporate the information provided by the observation of a new trade. We define m_{t+\\Delta t}^t as the distribution of the fair value at t+\\Delta t before observing the trade, and m_{t+\\Delta}^{t+\\Delta t} afterwards.\n\nLet us apply first the predict step. The distribution of m_{t+\\Delta t}^t is Gaussian with mean and variance given by:\\bar{m}_{t+\\Delta t}^t = \\bar{m}_{t}^t(\\sigma_{m,t+\\Delta t}^t)^2 = (\\sigma_{m,t}^t)^2 + \\sigma_\\epsilon^2 \\Delta t\n\nSince our model uses a drift-less random walk dynamics for the evolution of the fair value, the updated mean does not change and the variance increases proportionally to the time step \\Delta t.\n\nNow we use the update step to incorporate the information from a trade happening at t + \\Delta t:\\bar{m}_{t+\\Delta t}^{t+\\Delta t} = \\bar{m}_{t}^{t+\\Delta t} + K_t (p_{t +\\Delta t}  - \\bar{m}_{t}^{t+\\Delta t})(\\sigma_{m,t+\\Delta t}^{t+\\Delta t})^2 = \\frac{(\\sigma_{m,t+\\Delta t}^t)^2  \\sigma_\\eta^2}{(\\sigma_{m,t+\\Delta t}^t)^2  + \\sigma_\\eta^2}\n\nwhere K_t is the Kalman gain, given by:K_t = \\frac{(\\sigma_{m,t+\\Delta t}^t)^2}{(\\sigma_{m,t+\\Delta t}^t)^2  + \\sigma_\\eta^2}\n\nThe updated mean is an interpolation between the predicted mean and the trade observation, weighted by the Kalman gain\n\nIf the observation noise is much smaller than the uncertainty in the mean in the prediction step, namely \\sigma_\\eta \\ll \\sigma_{m,t+\\Delta t}^t, the Kalman gain then tends to K_t \\rightarrow 1 and \\bar{m}_{t+\\Delta t}^{t+\\Delta t} = p_{t+\\Delta t}, i.e. since our confidence on the information from the trade is much higher than our best estimation of the mean, we essentially update the mean with the trade price.\n\nOn the contrary, if \\sigma_\\eta \\gg \\sigma_{m,t+\\Delta t}^t, the Kalman gain then tends to K_t \\rightarrow 0 and \\bar{m}_{t+\\Delta t}^{t+\\Delta t} = \\bar{m}_{t}^{t+\\Delta t}. In this case, our confidence on the information provided by the trade is very low, so essentially we ignore the trade information and use the predicted mean.\n\nIn between those two limiting cases, the updated mean combines the information from the prediction using the internal dynamics and our last update, and the trade information.\n\nIf we look at the new standard deviation, we also find similar limiting behaviors:\n\nIf \\sigma_\\eta \\ll \\sigma_{m,t+\\Delta t}, then \\sigma_{m,t+\\Delta t}^{t+\\Delta t} \\rightarrow \\sigma_\\eta, since as we discussed above, we essentially use the trade information to inform our estimation of the mid.\n\nIf \\sigma_\\eta \\gg \\sigma_{m,t+\\Delta t}, then \\sigma_{m,t+\\Delta t}^{t+\\Delta t} \\rightarrow \\sigma_{m,t+\\Delta t}^t, i.e. we stick with the estimation from the predict step\n\nOne interesting consequence of the optimality of the Kalman filter is that the updated standard deviation cannot be larger than the predicted one, and for any finite \\sigma_\\eta is always smaller: the information from the trade always contributes to improve our estimation of the fair value.  This is easily seen writing:\\sigma_{m,t+\\Delta t}^{t+\\Delta t} =\\sigma_{m,t+\\Delta t}^t \\frac{1}{\\sqrt{(\\frac{\\sigma_{m,t+\\Delta t}^t}{\\sigma_\\eta})^2  + 1}}\n\nSince \\frac{\\sigma_{m,t+\\Delta t}^t}{\\sigma_\\eta} is non-negative, then the denominator is never lower than 1.\n\nIn some applications of the local level model to pricing we might also be interested in the Kalman smoothing algorithm. Recall that the difference with the Kalman filtering we have just seen is that in smoothing we estimate the latent variable using all the available data, including the future. Of course this means Kalman smoothing does not make sense for online price inference, but there are other applications of this pricing model where using the best estimation of the latent fair value is relevant:\n\nEstimation of parameters when using Expectation Maximization, as discussed in in \n\nBayesian Modelling\n\nCalibration of pricing models, for instance as we will discuss in the chapter on \n\nRfQ Modelling, when we seek to estimate the hit rate probability, or the probability that a client trades an RfQ from a dealer given the quoted price. In this case, we are interested in having the best estimation possible of the fair value to isolate the effect of the spread, which is the one that puts dealers in competition. In the same chapter we will also discuss models to evaluate the toxicity of clients’ flows, i.e. when clients seem to have more information than the dealer when trading, so the dealer seems to be later in the wrong side of the market. Estimating such models typically relies on analyzing how the fair value of the instrument moves before and after a client intends to trade.","type":"content","url":"/markdown/fair-price-estimation#inference-on-the-simple-pricing-model","position":11},{"hierarchy":{"lvl1":"Fair value estimation","lvl3":"Multiple observations of the same instrument","lvl2":"Filtering models for fair value estimation"},"type":"lvl3","url":"/markdown/fair-price-estimation#multiple-observations-of-the-same-instrument","position":12},{"hierarchy":{"lvl1":"Fair value estimation","lvl3":"Multiple observations of the same instrument","lvl2":"Filtering models for fair value estimation"},"content":"In many real pricing situations, we might have different sources that reveal information about the fair value of an instrument, for example trades in different platforms, information from composites, or pricing information derived indirectly from trading indicators like the hit&miss. We will explore later those sources in detail. If they happen asynchronously, we can just use the simple pricing model introduced in the previous section, adjusting the observation error depending on the pricing source.\n\nIf they happen synchronously, though, we need to expand the Kalman filter to cope with simultaneous observations. This requires to change the observation model to a system of equations:p_{t,i} = m_t + \\nu_{t,i}, \\vec{\\nu}_{t,i} \\sim N(0, \\sigma_{\\nu, i}^2)\n\nWe can compute the Kalman gain in this case, which is a vector matrix:K_{t,i} = \\frac{(\\sigma_{m,t+\\Delta t}^{\\,t})^2/\\sigma_{\\nu,i}^2}{1 + (\\sigma_{m,t+\\Delta t}^{\\,t})^2/\\Lambda}\n\nwhere:\\Lambda = \\sum_{i=1}^n \\frac{1}{\\sigma_{\\nu,i}^2}\n\nis the total observation precision. Notice that, as a sanity check, in the case of a single observation we recover the Kalman gain derived in the previous section. For multiple observations, the update equation then reads:\\bar{m}_{t+\\Delta t}^{t+\\Delta t} = \\bar{m}_{t}^{t+\\Delta t} + \\sum_{i=1}^n K_{t,i} (p_{t + \\Delta t ,i} - \\bar{m}_{t}^{t+\\Delta t})\n\nThe relative weight of influence of each observation depends on the fraction of the total variance that the observation variance represents, with more noisy observations having a smaller effect in the update.","type":"content","url":"/markdown/fair-price-estimation#multiple-observations-of-the-same-instrument","position":13},{"hierarchy":{"lvl1":"Fair value estimation","lvl3":"Multiple correlated instruments","lvl2":"Filtering models for fair value estimation"},"type":"lvl3","url":"/markdown/fair-price-estimation#multiple-correlated-instruments","position":14},{"hierarchy":{"lvl1":"Fair value estimation","lvl3":"Multiple correlated instruments","lvl2":"Filtering models for fair value estimation"},"content":"The Kalman filter model for pricing becomes even more relevant when we include information from other financial instruments that are historically correlated with the one whose fair value we are estimated. Typical situations are:\n\nInstruments that are more liquid, i.e. they trade more often and with smaller bid and ask spreads. This allows us to improve the estimation of the fair value until we observe a new trade from the instrument, anticipating potential relevant movements derived from common market factors.\n\nInstruments that trade in markets that are open when markets where our instrument of interest is traded are closed. This allows us to reduce the uncertainty from the overnight gap in trading data.\n\nThe simple pricing model we have analyzed so far can be easily extended to include information from a set of N instruments. Notice that in this case what we are actually doing is estimating the fair values of all the instruments in the set, not necessarily only the one of interest. The evolution of the fair values is now modelled using:\\vec{m}_{t+\\Delta t} = \\vec{m}_t + \\vec{\\epsilon}_t, \\vec{\\epsilon}_t \\sim N(\\vec{0}, \\Sigma_\\epsilon \\Delta t)\n\nwhere \\Sigma_\\epsilon is now a covariance matrix that takes into account the effect of correlations in the pricing movements. Price observations follow the model:\\vec{p}_t = \\vec{m}_t + \\vec{\\nu}_t, \\vec{\\nu}_t \\sim N(0, \\Sigma_\\nu)\n\nIn this case, since we are already modelling correlations at fair value level, a typical choice is to take \\Sigma_\\nu diagonal, i.e. \\Sigma_\\nu = \\text{diag}(\\sigma_{\\nu,1}^2, ..., \\sigma_{\\nu_N}^2), although in certain setups one might want to include some of form of bid-ask spread correlation between instruments.\n\nWith this model specification, we can directly use the filtering, smoothing and EM equations discussed in the \n\nBayesian Modelling chapter. Let us though specifically focus on the case of N=2 instruments, where we can work out in detail the Kalman filter equations to get further insights into the model’s inner workings.\n\nThe predict step in the Kalman filter is given by the following equations:\\vec{m}_{t|t-1} = \\vec{m}_{t-1|t-1}\\Sigma_{t|t-1} = \\Sigma_{t-1|t-1} + \\Sigma_\\epsilon \\Delta t\n\nwhich are relatively simple, as expected. The update step equations are more interesting, since they include the effect of observations, and critically the impact of one instrument’s trades into the fair value or the other:\\vec{m}_{t|t} = \\vec{m}_{t|t-1} + K_t ( \\vec{P}_{t+\\Delta t}-\\vec{m}_{t|t-1})\\Sigma_{t|t} = (1-K_t) \\Sigma_{t|t-1}\n\nwith the Kalman gain being:K_t =  \\Sigma_{t|t-1} ( \\Sigma_{t|t-1} + \\Sigma_\\nu)^{-1}\n\nFor the two instrument case, the Kalman gain can be expanded into:\\begin{aligned}\nK_t = \n\\frac{1}{(\\sigma_{\\nu,1}^2 + (\\sigma_{1,t}^{t-1})^2)(\\sigma_{\\nu,2}^2 + (\\sigma_{2,t}^{t-1})^2) -  (\\rho_t^{t-1}\\sigma_{1,t}^{t-1}\\sigma_{2,t}^{t-1})^2} \\nonumber \\\\\n\\begin{pmatrix} \n(\\sigma_{1,t}^{t-1})^2 \\sigma_{\\nu,2}^2 + (\\sigma_{1,t}^{t-1})^2(\\sigma_{2,t}^{t-1})^2(1- (\\rho_t^{t-1})^2) &  \\rho_t^{t-1} \\sigma_{1,t}^{t-1}\\sigma_{2,t}^{t-1}\\sigma_{\\nu,1}^2 \\\\ \n  \\rho_t^{t-1} \\sigma_{1,t}^{t-1}\\sigma_{2,t}^{t-1}\\sigma_{\\nu,2}^2 & (\\sigma_{2,t}^{t-1})^2\\sigma_{\\nu,1}^2 + (\\sigma_{1,t}^{t-1})^2 (\\sigma_{2,t}^{t-1})^2(1-(\\rho_t^{t-1})^2)\n  \\nonumber \\\\\n\\end{pmatrix}\n\\end{aligned}\n\nLet us analyze some particular cases:\n\nIf we set the correlation to zero, \\rho_t^{t-1} = 0, the Kalman gain becomes:\\begin{aligned}\n  K_t = \n  \\frac{1}{(\\sigma_{\\nu,1}^2 + (\\sigma_{1,t}^{t-1})^2)(\\sigma_{\\nu,2}^2 + (\\sigma_{2,t}^{t-1})^2)} \\nonumber \\\\\n  \\begin{pmatrix} \n  (\\sigma_{1,t}^{t-1})^2 \\sigma_{\\nu,2}^2 + (\\sigma_{1,t}^{t-1})^2(\\sigma_{2,t}^{t-1})^2 & 0 \\\\ \n  0 & (\\sigma_{2,t}^{t-1})^2\\sigma_{\\nu,1}^2 + (\\sigma_{1,t}^{t-1})^2 (\\sigma_{2,t}^{t-1})^2\n  \\end{pmatrix}  \\\\ \\nonumber \n   = \n  \\begin{pmatrix} \n  \\frac{(\\sigma_{1,t}^{t-1})^2}{\\sigma_{\\nu,1}^2 + (\\sigma_{1,t}^{t-1})^2}  & 0 \\\\ \n  0 & \\frac{(\\sigma_{2,t}^{t-1})^2}{\\sigma_{\\nu,2}^2 + (\\sigma_{2,t}^{t-1})^2}\n  \\end{pmatrix}  \\nonumber \n  \\end{aligned}\n\ni.e. we recover the equations of the Kalman gain for a single instrument for each diagonal, and the update equation factors in two independent updates.\n\nIf we set \\sigma_{\\nu, 1}^{-1} = 0, which simulates the case in which we don’t have an observation of a trade in the first financial instrument --by considering its observation error so large that it’s effect is negligible, the Kalman gain becomes:\\begin{aligned}\n  K_t = \n  \\begin{pmatrix} \n  0 &  \\frac{\\rho_t^{t-1} \\sigma_{1,t}^{t-1}\\sigma_{2,t}^{t-1}}{\\sigma_{\\nu,2}^2 + (\\sigma_{2,t}^{t-1})^2} \\\\ \n    0 & \\frac{(\\sigma_{2,t}^{t-1})^2}{\\sigma_{\\nu,2}^2 + (\\sigma_{2,t}^{t-1})^2} \n    \\nonumber \\\\\n  \\end{pmatrix}\n  \\end{aligned}\n\nThe update equations for each instruments fair value read:m_{1,t|t} = m_{1,t|t-1} + \\frac{\\rho_t^{t-1} \\sigma_{1,t}^{t-1}\\sigma_{2,t}^{t-1}}{\\sigma_{\\nu,2}^2 + (\\sigma_{2,t}^{t-1})^2} (m_{2,t|t-1} - P_{2,t + \\Delta t})m_{2,t|t} = m_{2,t|t-1} + \\frac{(\\sigma_{2,t}^{t-1})^2}{\\sigma_{\\nu,2}^2 + (\\sigma_{2,t}^{t-1})^2} (m_{2,t|t-1} - P_{2,t + \\Delta t})\n\nNotice that these equations are equivalent to the ones that we would derive if we use an observation matrix H = (0, 1) and compute the update step for arbitrary values of the parameters. Going into the results, the second equation is the same update equation we had for a single instrument for which we have observed a trade. The first equation is more interesting, since it isolates the effect that an observation of a trade in an instrument has in our fair value estimation of a correlated instrument. As expected, the influence is proportional to the estimated correlation between the instruments, \\rho_t^{t-1}. The effect of the influence depends on the relative sizes of the variances in play. It helps to rewrite the equation as:m_{1,t|t} = m_{1,t|t-1} + \\beta_{12,t}^{t-1} \\frac{1}{1+ \\frac{\\sigma_{\\nu,2}^2}{ (\\sigma_{2,t}^{t-1})^2}} (m_{2,t|t-1} - P_{2,t + \\Delta t})\n\nwhere \\beta_{12,t}^{t-1} \\equiv \\frac{\\rho_t^{t-1} \\sigma_{1,t}^{t-1}}{\\sigma_{2,t}^{t-1}} is the linear regression coefficient between m_{1,t} and m_{2,t}. It provides an upper bound on the Kalman gain between the two instruments, which happens when the observation in the second instrument has no error \\sigma_{\\nu, 2} = 0. This makes sense, since in the absence of noise in the observation of the second instrument, the update equation becomes the best linear prediction of m_{1,t} using m_{2,t}.\n\npriceLet’s evaluate this model in one of the typical scenarios for fair value discovery discussed above: two correlated instruments traded in markets with some periods of non-overlapping trading. The objective is to leverage their correlation to estimate fair values for the instrument whose market is closed. The underlying principle is that new information affecting the price of the actively traded instrument during its market hours would similarly impact the closed-market instrument, if it were tradable.\n\nFor that, we first generate synthetic fair values using a correlated Brownian motion with \\rho = 0.9, \\sigma_1 = 5e-4 and \\sigma_2 = 4e-4. Then we generate trades over 22 days but for each day, each day consisting on 60 time-steps to make the simulation efficient. We consider three situations: one in which only the first instrument is traded, on in which only the second instrument is traded, and a third one in which both are simultaneously traded. We use a diagonal observation covariance to generate the trades, i.e. we assume that there is no correlation between the spreads with respect to the fair value, so correlation is driven exclusively by fair value correlations. To generate the trades, we use standard deviations in the observation covariance of 0.032 and 0.045, respectively. Then we use Expectation Maximization (EM) over the first half of the synthetic trade data to estimate the parameters of the model, and run the Kalman filter over the second half of the data to compare the estimations of the fair value to the real simulated values. The results can be seen in the following figure:\n\n\n\nFigure 1:Estimation of the fair value of instruments when their market is closed, using information from correlated instrument that trade at those times. The results are based on a simulation in which first the fair values are generated (blue lines) and trades (blue dots) are simulated when the market is open, which. happens half of the day. Notice that a third of the day both instruments trade simultaneously. The orange line are the fair values estimated using the Kalman filter, which is trained with half of the data using EM and the run over the second half of the data. The figure focus on four days of test data.\n\nAs we see, the Kalman filter successfully exploits the correlation between instruments to update the fair value of the instruments when the market is closed. The updates are not perfect but they capture the overnight trends, improving over typical baselines like the closing price of the instrument. In general, the estimated fair values include the true fair values within one standard deviation, depicted as the shaded grey area in the figure.","type":"content","url":"/markdown/fair-price-estimation#multiple-correlated-instruments","position":15},{"hierarchy":{"lvl1":"Fair value estimation","lvl3":"Pricing sources and models","lvl2":"Filtering models for fair value estimation"},"type":"lvl3","url":"/markdown/fair-price-estimation#pricing-sources-and-models","position":16},{"hierarchy":{"lvl1":"Fair value estimation","lvl3":"Pricing sources and models","lvl2":"Filtering models for fair value estimation"},"content":"When it comes to feeding the Kalman filter with information to improve the estimations of the fair value, there is a variety of sources that is typically used. Let us discuss the most typical sources, those based on Limit Order Book (LOB) data and those based on Request for Quote (RfQ) data. As we have seen, these sources can be used both if they belong to the instrument of interest or correlated ones. However, there are some considerations to take into account when using correlated instruments, which we discuss in the end.\n\nOne point that will become a common theme in this section is the information content of the observations. Intuitively, not every source of pricing is equally informative. For example, as we discussed above, we expect that trades with larger volumes are more informative than small trades. In the same way, a cancellation of a limit order deep in a Limit Order Book might not carry meaningful new pricing information. These effects need to be incorporated case by case into the pricing model.","type":"content","url":"/markdown/fair-price-estimation#pricing-sources-and-models","position":17},{"hierarchy":{"lvl1":"Fair value estimation","lvl4":"LOB traded","lvl3":"Pricing sources and models","lvl2":"Filtering models for fair value estimation"},"type":"lvl4","url":"/markdown/fair-price-estimation#lob-traded","position":18},{"hierarchy":{"lvl1":"Fair value estimation","lvl4":"LOB traded","lvl3":"Pricing sources and models","lvl2":"Filtering models for fair value estimation"},"content":"Limit Order Books (LOBs) are complex structures that contain extensive pricing information. Nevertheless, as discussed in Chapter \n\nMarket microstructure, the primary references for price discovery are the best bid and ask quotes, the mid-price —defined as the average of these two—and the prices of executed trades. To improve robustness and reduce the impact of potential market manipulation, it is standard practice to compute volume-weighted averages of the first K levels on both the bid and ask sides, and to define a robust mid-price based on these averages.","type":"content","url":"/markdown/fair-price-estimation#lob-traded","position":19},{"hierarchy":{"lvl1":"Fair value estimation","lvl5":"Mid-price information","lvl4":"LOB traded","lvl3":"Pricing sources and models","lvl2":"Filtering models for fair value estimation"},"type":"lvl5","url":"/markdown/fair-price-estimation#mid-price-information","position":20},{"hierarchy":{"lvl1":"Fair value estimation","lvl5":"Mid-price information","lvl4":"LOB traded","lvl3":"Pricing sources and models","lvl2":"Filtering models for fair value estimation"},"content":"As mentioned above, a robust mid-price indicator in a LOB is:P_{\\text{mid},t}^{\\text{LOB}} = \\frac{1}{2} \\left(\\frac{\\sum_{i=1}^K v_{b,i} P_{b,i}}{\\sum_{i=1}^K v_{b,i}} + \\frac{\\sum_{i=1}^K v_{a,i} P_{a,i}}{\\sum_{i=1}^K v_{a,i}}\\right)\n\nwhere v_{b/a, i} and P_{b/a, i} are volumes and prices of bid and ask, respectively, and K is the number of levels that are taken into account, for instance K = 4. We have omitted the time subscript in volumes and prices but they are also time dependent. Similarly, we can compute a robust bid-ask spread in the form:S_t^{\\text{LOB}} =  \\frac{\\sum_{i=1}^K v_{a,i} P_{a,i}}{\\sum_{i=1}^K v_{a,i}} - \\frac{\\sum_{i=1}^K v_{b,i} P_{b,i}}{\\sum_{i=1}^K v_{b,i}}\n\nwhich has to be positive since any bid and ask limit orders with the same price are automatically matched in the LOB. With this information, we can compute a simple model of fair value based on LOB information:m_t^{\\text{LOB}} \\sim N(P_{\\text{mid},t}^\\text{LOB}, (S_t^{\\text{LOB}})^2 )\n\nThe choice of a Gaussian distribution is for simplicity, since it captures well our subjective view of the pricing information that the LOB contains, i.e. as we saw in chapter \n\nBayesian Modelling, since we only identify the mean and variance as the constraints, the maximum entropy distribution associated with these constraints in the Gaussian distribution --admittedly, there is an extra constraint in the form of positivity of the fair value, but for prices far from the zero boundary we can safely omit this constraint.  A more grounded criticism of this model could be that the Gaussian distribution allows the fair value to be beyond the best bid and ask prices, and those prices are tradeable: if the market consensus of fair value was beyond those prices, market participants would be willing to trade at those prices until they the fair value lies between the bid and ask spread. However, since liquidity might be thin at the first levels, and therefore those prices might not be available for bulk transactions, we consider a soft constraint as more appropriate, even when we use the robust bid-ask spread instead of the best bid and ask spread.\n\nOf course, in many situations this pricing source might not be sufficient for actual applications, since in illiquid markets the bid-ask spreads are large and therefore the pricing source has a large uncertainty. In those situations is precisely where the Kalman filter model plays a part.\n\nUsing directly m_t^{\\text{LOB}} as an observation in the Kalman filter is problematic, since the feed is available in streaming and therefore, even if we limit the updates to the Kalman filter to the moments in which the LOB gets updates (i.e. arriving of new orders, cancellations, modifications), the pricing observations might jam the Kalman filter estimation, making the model to consider that m_t^{\\text{LOB}} is a perfect source of pricing information. To see this, let us see the effect of a second observation following an initial one. Recall that the first predict plus update gave:\\bar{m}_{t+\\Delta t}^{t+\\Delta t} = \\bar{m}_{t}^{t} + K_t (p_{t+\\Delta t} - \\bar{m}_{t}^{t})(\\sigma_{m,t+\\Delta t}^{t+\\Delta t})^2 = \\frac{((\\sigma_{m,t}^t)^2 + \\sigma_\\epsilon^2 \\Delta t)  \\sigma_\\eta^2}{(\\sigma_{m,t}^t)^2 + \\sigma_\\epsilon^2 \\Delta t + \\sigma_\\eta^2}\n\nwith:K_t = \\frac{(\\sigma_{m,t}^t)^2 + \\sigma_\\epsilon^2 \\Delta t }{(\\sigma_{m,t}^t)^2 + \\sigma_\\epsilon^2 \\Delta t + \\sigma_\\eta^2}\n\nIf we approach the limit \\Delta t \\rightarrow 0 this simplifies to:\\bar{m}_{t+\\Delta t}^{t+\\Delta t} = \\bar{m}_{t}^{t} +  K_t (p_{t+\\Delta t} - \\bar{m}_{t}^{t})(\\sigma_{m,t+\\Delta t}^{t+\\Delta t})^2 = \\frac{(\\sigma_{m,t}^t)^2  \\sigma_\\eta^2}{(\\sigma_{m,t}^t)^2  + \\sigma_\\eta^2}K_t = \\frac{(\\sigma_{m,t}^t)^2  }{(\\sigma_{m,t}^t)^2 + \\sigma_\\eta^2}\n\nLet us apply a second predict plus update steps on top of this, keeping the \\Delta t \\rightarrow 0 limit:\\bar{m}_{t+2\\Delta t}^{t+2\\Delta t} = \\bar{m}_{t}^{t} + K_t (p_{t+\\Delta t} - \\bar{m}_{t}^{t}) + K_{t+\\Delta t} (p_{t+2\\Delta t} - \\bar{m}_{t+\\Delta t}^{t +\\Delta t}) \\nonumber \\\\ \n=  \\bar{m}_{t}^{t} (1- K_t (1-K_{t+\\Delta t}) - K_{t+\\Delta t}) + K_t(1-K_{t+\\Delta t}) p_{t+\\Delta t}+ K_{t+\\Delta t} p_{t+2\\Delta t}(\\sigma_{m,t+2\\Delta t}^{t+2\\Delta t})^2 = \\frac{(\\sigma_{m,t}^{t})^2   \\sigma_\\eta^2}{2 (\\sigma_{m,t}^{t})^2 + \\sigma_\\eta^2}\n\nSince:K_{t+\\Delta t} = \\frac{(\\sigma_{m,t+\\Delta t}^{t+\\Delta t})^2}{(\\sigma_{m,t+\\Delta t}^{t+\\Delta t})^2  + \\sigma_\\eta^2} = \n \\frac{(\\sigma_{m,t}^t)^2 }{2 (\\sigma_{m,t}^t)^2  + \\sigma_\\eta^2}1-K_{t+\\Delta t}  = \n \\frac{(\\sigma_{m,t}^t)^2 +\\sigma_\\eta^2}{2 (\\sigma_{m,t}^t)^2  + \\sigma_\\eta^2}K_t (1-K_{t+\\Delta t}) =  \\frac{(\\sigma_{m,t}^t)^2  }{2(\\sigma_{m,t}^t)^2 + \\sigma_\\eta^2} = K_{t+\\Delta t}\n\nthen:\\bar{m}_{t+2\\Delta t}^{t+2\\Delta t} = (1-2K_{t+\\Delta t}) \\bar{m}_{t}^{t}  + K_{t+\\Delta t}(p_{t+\\Delta t} + p_{t+2\\Delta t})  \\nonumber \\\\ =   \\frac{\\sigma_\\eta^2}{2 (\\sigma_{m,t}^t)^2  + \\sigma_\\eta^2}  \\bar{m}_{t}^{t}  + \n \\frac{(\\sigma_{m,t}^t)^2 }{2 (\\sigma_{m,t}^t)^2  + \\sigma_\\eta^2} (p_{t+\\Delta t} + p_{t+2\\Delta t})\n\nIf we continue applying n predict plus update steps in the \\Delta \\rightarrow 0 limit, by using the induction principle we arrive at the following result:\\bar{m}_{t+n\\Delta t}^{t+n\\Delta t} = \\frac{\\sigma_\\eta^2}{n (\\sigma_{m,t}^t)^2  + \\sigma_\\eta^2}  \\bar{m}_{t}^{t}  + \n \\frac{(\\sigma_{m,t}^t)^2 }{n (\\sigma_{m,t}^t)^2  + \\sigma_\\eta^2} \\sum_{i=1}^n p_{t+n \\Delta t}(\\sigma_{m,t+n\\Delta t}^{t+n\\Delta t})^2 = \\frac{(\\sigma_{m,t}^{t})^2   \\sigma_\\eta^2}{n (\\sigma_{m,t}^{t})^2 + \\sigma_\\eta^2}\n\nIf we now take the limit n\\rightarrow \\infty we converge to:\\bar{m}_{t+n\\Delta t}^{t+n\\Delta t} \\rightarrow \n \\frac{1}{n} \\sum_{i=1}^n p_{t+n \\Delta t}(\\sigma_{m,t+n\\Delta t}^{t+n\\Delta t})^2 \\rightarrow 0\n\nTherefore, in the case of LOB updates that don’t significantly provide new pricing information but happen with a high frequency, as in the case in LOBs for liquid instruments, the Kalman filter will converge to the mid-price with zero uncertainty.\n\nA simple way to fix this issue is to estimate the Kalman filter with other pricing information (trades, RfQs, see next section) and combine it as two separate fair value estimations at any time. The best linear combination of two estimators is the one that minimizes the variance:\\hat{m}_t = \\alpha m_t^{\\text{LOB}} + (1-\\alpha) m_t^{\\text{kalman}}\\text{Var}(\\hat{m}_t) =  \\alpha^2 (S_t^{\\text{LOB}})^2 + (1-\\alpha)^2 (\\sigma_{m,t}^t)^2\n\nwhich is achieved for:\\hat{\\alpha} = \\frac{(\\sigma_{m,t}^t)^2}{(S_t^{\\text{LOB}})^2 + (\\sigma_{m,t}^t)^2}\n\nNotice that we have considered them independent estimators, otherwise there would be an extra term accounting for the correlation. Therefore, the best linear estimator is:\\hat{m}_t = \\frac{(\\sigma_{m,t}^t)^2}{(S_t^{\\text{LOB}})^2 + (\\sigma_{m,t}^t)^2} m_t^{\\text{LOB}} + \\frac{(S_t^{\\text{LOB}})^2 }{(S_t^{\\text{LOB}})^2 + (\\sigma_{m,t}^t)^2} m_t^{\\text{kalman}}\n\nThis makes intuitive sense: the smaller the relative error of one estimator compared to the other, the more weight the combined estimate assigns to it. Importantly, the resulting variance is lower than that of either individual estimator.\\text{Var}(\\hat{m}_t) =  \\frac{(S_t^{\\text{LOB}})^2(\\sigma_{m,t}^t)^2}{(S_t^{\\text{LOB}})^2 + (\\sigma_{m,t}^t)^2}\n\nTo check it, simply take the ratio of the final variance with any of the variances of the independent predictors, for example:\\frac{\\text{Var}(\\hat{m}_t)}{(S_t^{\\text{LOB}})^2} =  \\frac{(\\sigma_{m,t}^t)^2}{(S_t^{\\text{LOB}})^2 + (\\sigma_{m,t}^t)^2} \\leq 1\n\nthe equality happening when S_t^{\\text{LOB}} = 0 i.e. it was already a perfect predictor and, therefore, there is no way to improve the prediction.\n\nNotice that, since the combined fair value estimator is reconstructed independently at each point in time —without relying on past estimates—it is not affected by the high-frequency sampling issue observed when using the LOB mid-price as an observation.","type":"content","url":"/markdown/fair-price-estimation#mid-price-information","position":21},{"hierarchy":{"lvl1":"Fair value estimation","lvl5":"Trades","lvl4":"LOB traded","lvl3":"Pricing sources and models","lvl2":"Filtering models for fair value estimation"},"type":"lvl5","url":"/markdown/fair-price-estimation#trades","position":22},{"hierarchy":{"lvl1":"Fair value estimation","lvl5":"Trades","lvl4":"LOB traded","lvl3":"Pricing sources and models","lvl2":"Filtering models for fair value estimation"},"content":"Trades happening in the LOB are a valuable source of pricing information, since they correspond to real transaction prices and not only interests to trade as limit orders. When a trade happens, the exchange reports publicly the time, the size and the price, but not the parties or the orders involved. The latter is particularly relevant since a relevant pricing information is the side (buy or sell) of the order that was aggressive, meaning the one that consumed the liquidity in the order book. As we discussed in \n\nMarket microstructure, this can typically a market order or a limit order at a price that is equal or better that prices available in the opposite side. Reverse engineering the side from a trade is an inference problem, and requires a model. A simple one widely used is the so-called tick-rule model which consists on comparing the price of the trade with the mid-price available in the order book just before the trade:\n\nIf the price of the trade is below the mid-price, then we assume it was an aggressive sell order, since the the trade price has to be an average weighted by size of the limit orders available. Therefore, it makes sense to assume the trade consumed liquidity in the side closer to the trade price using the mid-price as a reference.\n\nIf the price of the trade is above the mid-price, we assume it was an aggressive buy order\n\nThis model is not perfect, however. It does not account for hidden liquidity that might exist at more favorable prices than those displayed, which would alter the reference mid-price and, consequently, the trade classification logic. Moreover, it assumes a sequential processing of orders, whereas in practice, multiple orders may arrive simultaneously. In such cases, the exchange’s internal matching engine determines execution priority and order pairing through mechanisms that are not observable externally, meaning the apparent sequence of trades and quote updates in public data may not reflect the true matching process. This lack of transparency can lead to misclassification when applying the tick rule or similar models.\n\nOnce we have the relevant pricing information for the trade, namely time, side, size and or course price, it can be used to update the current fair value estimation of the financial instrument. The size information is useful to include some measure of information content in the order, as discussed in the simple pricing model section when introducing the Sinclair model: intuitively, a very small size trade should not be as relevant as a large size trade when updating our fair value estimation. Sinclair proposes to model a trade observation as a Gaussian random variable {\\mathcal N}(P_t^{\\text{trade}}, \\sigma^2(v)), where P_t^{\\text{trade}} is the observed trade price at time t, and \\sigma(v) is given by:\\sigma (v)= \\sigma_p \\left(\\frac{v_\\text{max}}{v}-1)\\right)^+\n\nwhere \\sigma_p is a constant to be estimated, and v_{\\text{max}} is an exogenous parameters that provides a typical scale for which trades are considered informative. It can be given by business prior knowledge or estimated from the statistical distribution of trade sizes. Notice that this error has the desirable properties of becoming zero at the scale v_{\\text{max}} and above, i.e. trades above this scale are considered maximally informative and the fair value is instantaneously update to this value. It also becomes infinitely large as the volume tends to zero, which makes the Kalman filter model to essentially ignore those observations.\n\nSinclair’s model is not the only way to introduce this behavior into the model. Other choices of \\sigma(v) are also valid. For example, the model:\\sigma (v)= \\sigma_0 \\frac{\\exp(-\\frac{v}{v_0})}{1+ \\exp(-\\frac{v}{v_0})}\n\nmight be more realistic in the sense that volume adjusts the degree of information but it never completely ignores small trades, for which the error tends to \\sigma_0/2. Another alternative that does not completely agree with trades or large size is the following:\\sigma (v)= \\sigma_{\\min} + (\\sigma_0 - \\sigma_{\\min}) e^{-\\frac{v}{v_0}}\n\nwhich tends to \\sigma_{\\min} as v \\rightarrow \\infty.\n\nSo far we have not used the side information inferred from the tick-rule model. There are different ways this information can be factored into the pricing. In markets that are quite unbalanced, the observation of a trade in the opposite side where the market is prevalently trading might be considered more informative. This means potentially adjusting the error function with the side information. Another alternative is directly building separate Kalman filters for buy and sell information, which are then combined in a final fair value estimation using the model discussed in the previous section for optimally combining two predictors, although in this case neglecting correlation between bid and ask estimations might not be a solid modelling choice. We leave as an exercise to the reader to derive the optimal linear predictor with correlation.","type":"content","url":"/markdown/fair-price-estimation#trades","position":23},{"hierarchy":{"lvl1":"Fair value estimation","lvl4":"RfQ traded","lvl3":"Pricing sources and models","lvl2":"Filtering models for fair value estimation"},"type":"lvl4","url":"/markdown/fair-price-estimation#rfq-traded","position":24},{"hierarchy":{"lvl1":"Fair value estimation","lvl4":"RfQ traded","lvl3":"Pricing sources and models","lvl2":"Filtering models for fair value estimation"},"content":"We have discussed the Request for Quote (RfQ) protocol in the chapter on \n\nMarket Microstructure. In terms of pricing information, the main difference with LOBs is the asymmetry in information between the different participants in the process: dealers and clients. Let us focus on the case of negotiation via Multi-Dealer-to-Client platforms, which has the richer casuistic, from the point of view of the dealer, who is typically the one actively trying to calculate the fair value of the instruments. The pricing information that the dealer receives is the following:\n\nPlatform composites: the platform does not provide individual streaming bid and ask prices from other dealers, this is an information only available to clients. However, the platform typically offers a composite price, and index calculated aggregating the individual feeds from the dealers and applying proprietary rules. A composite typically consists on a stream of bid and ask prices that roughly represent average indicative or sometimes executable streamed prices from the dealers active in the platform. For example:\n\nBloombergs’ CBBT for bonds (Composite Bloomberg Bond Trader), using executable bid and ask streams from dealers\n\nTradeweb’s TW Composite, using both indicative and executable prices\n\nIn some cases, the platforms offer pricing feeds that already incorporate multiple pricing sources. For instance, Bloomberg offers BGN (Bloomberg Generic Price) for multiple instruments, which incorporates several pricing sources in the calculation, from dealer’s indicative prices, interdealer brokers, traded data from exchanges (if available), etc. Marketaxess offers CP+ (Composite plus) for bonds, using indicative and executable streams from dealers but adding also information from trades within its platform as well as reported data from trade repositories like Trax and TRACE (see below)\n\nTraded price: when the dealer trades the RfQ (or ends tied), of course she has the information from the traded price.\n\nCover price: When a dealer wins the RFQ and executes the trade, the platform  discloses the second-best price quoted by a competitor, called cover price.\n\nNon pricing information: if the dealer quoted the second-best price, the platform informs them of this outcome, although it does not disclose the actual traded price. The dealer can, however, infer that the executed price must have been better than their own quote. Each participating dealer is also notified whether the client executed with another dealer or decided not to trade at all.\n\nApart from the pricing information gained via the trading platform, in order to improve transparency and price formation, there are private and public initiatives to share pricing information post-trade, particularly in the bond markets:\n\nTRACE (United States): Operated by FINRA since 2002, the Trade Reporting and Compliance Engine (TRACE) requires broker-dealers to report transactions in corporate, agency, and certain securitized and Treasury securities. Reports must be submitted within minutes of execution, and a portion of the data is made public, providing prices, volumes, and timestamps. TRACE has become the main benchmark for post-trade transparency in U.S. fixed income and is widely used for valuation, best-execution monitoring, and market analysis.\n\nMiFID II / APAs (European Union): Under the MiFID II and MiFIR framework, investment firms must publish details of OTC bond and derivative transactions through Approved Publication Arrangements (APAs). These entities disseminate standardized post-trade data on price, size, and time of execution, subject to deferrals for large or illiquid trades. Although the system has enhanced transparency across European markets, the coexistence of multiple APAs has led to fragmentation and the absence of a unified consolidated tape.\n\nTRAX and Axess All (Europe): TRAX, originally created by ICMA and now operated by MarketAxess, serves as a trade matching and regulatory reporting system under MiFID II, EMIR, and SFTR. Building on this infrastructure, MarketAxess launched Axess All, a private transparency service that aggregates and anonymizes post-trade bond data to produce daily composite levels for European government and corporate bonds. While not an official regulatory tape, it provides a valuable commercial reference for post-trade pricing and market trends.\n\nOther Regional Systems: Comparable frameworks exist in other jurisdictions. In the United Kingdom, the FCA maintains the MiFID-style APA regime and is developing a consolidated tape for fixed income. In Canada, the IIROC operates a corporate bond transparency service with public post-trade data similar to TRACE. In Asia, initiatives remain limited, though regulators in Japan and Singapore are exploring TRACE-like models to improve fixed-income transparency.","type":"content","url":"/markdown/fair-price-estimation#rfq-traded","position":25},{"hierarchy":{"lvl1":"Fair value estimation","lvl5":"Composites","lvl4":"RfQ traded","lvl3":"Pricing sources and models","lvl2":"Filtering models for fair value estimation"},"type":"lvl5","url":"/markdown/fair-price-estimation#composites","position":26},{"hierarchy":{"lvl1":"Fair value estimation","lvl5":"Composites","lvl4":"RfQ traded","lvl3":"Pricing sources and models","lvl2":"Filtering models for fair value estimation"},"content":"Modelling composites is similar to modelling mid-prices from order books, since they share in common the structure of information in terms of a continuous feed of bid and ask prices, only that for composites are indicative prices. Again, given that the frequent updates in the feed might not carry new pricing information, it makes sense to model it as an independent fair value source, that can be then aggregated with other estimations, like those using Kalman filters on trades and other information.\n\nThe model is therefore:m_t^{\\text{comp}} \\sim N(P_{\\text{mid},t}^\\text{comp}, (S_t^{\\text{comp}})^2 )\n\nwith:P_{\\text{mid},t}^\\text{comp} = \\frac{1}{2}(a_t^\\text{comp} +b_t^\\text{comp})S_{\\text{mid},t}^\\text{comp} = \\frac{1}{2}(a_t^\\text{comp} -b_t^\\text{comp})\n\nwhere b_t^\\text{comp}, a_t^\\text{comp} are, respectively, the bid and ask composite prices published by the platform.","type":"content","url":"/markdown/fair-price-estimation#composites","position":27},{"hierarchy":{"lvl1":"Fair value estimation","lvl5":"RfQs","lvl4":"RfQ traded","lvl3":"Pricing sources and models","lvl2":"Filtering models for fair value estimation"},"type":"lvl5","url":"/markdown/fair-price-estimation#rfqs","position":28},{"hierarchy":{"lvl1":"Fair value estimation","lvl5":"RfQs","lvl4":"RfQ traded","lvl3":"Pricing sources and models","lvl2":"Filtering models for fair value estimation"},"content":"The information from RfQs depends, as discussed above, on the final status of the dealer in the process. If the dealer wins the RfQ, the observation corresponds to a trade, and the modelization is overall similar to the one discussed in the context of trades in the LOB. There is, though, one key difference with the case of LOBs, in that the cover price is also informed to the dealer.\n\nIntuitively, the closer the cover and the trade price, the more confidence we might put on the trade price as a pricing source, since we have the agreement from a second dealer quoting a similar level. Or put it in a different way, if the dealer wins the trade at a price much further than the cover price, it might imply a clear miss-pricing that needs to be adjusted, not a reliable pricing source. One way to incorporate this into the model is to make the observation error a function of the distance to the cover:\\sigma (v, d_t^{\\text{cover}}) = \\sigma(v) g(d_t^{\\text{cover}})\n\nwhere d_t^{\\text{cover}} = |P_t^\\text{trade} - P_t^\\text{cover}| is the distance to the cover, and g(\\cdot) a modulating function with a minimum at zero.\n\nA second case is the one in which the dealer misses the RfQ, but the price was the second best quoted. The fact that among the number of dealers quoting competitively the price quoted was the second best carries some significant information that can be used to adjust the fair value. One way to do this is to fit a probabilistic model that predicts the distance to cover based on features f_i available in the negotiation, for example, a simple linear regression model on d_t^\\text{cover}:d_t^\\text{cover} = \\sum_i w_i f_i + \\epsilon\n\nwhere w_i are the regression weights and \\epsilon \\sim N(0, \\sigma_d^2). Alternatively, if we want to explicitly model that the distance to cover cannot be negative, we can use a log transformation, although then we will have to use a non-linear Kalman filter to account for this observation. Assuming the simple linear regression model, we can add the observation to the Kalman filter:P_t^{\\text{inf, cover}} \\sim N(P_t^\\text{cover} + \\text{side}  \\cdot E[d_t^\\text{cover}|\\{f_i\\}], \\sigma_d^2)\n\nwhere \\text{side} = \\pm 1 depending on the side of the RfQ (buy or sell).\n\nA third case happens when the client trades, the dealer misses the RfQ but her quote was not even the second best (cover). The information in this setup is much weaker, in the form of a bound to the traded price (the quoted price). Although, in principle, a probabilistic inference on the traded price could be performed using this information, the potential model risk coming from the plethora of model assumptions typically outweighs the information gains, so we will not dive deeper here.","type":"content","url":"/markdown/fair-price-estimation#rfqs","position":29},{"hierarchy":{"lvl1":"Fair value estimation","lvl5":"Hit & Miss","lvl4":"RfQ traded","lvl3":"Pricing sources and models","lvl2":"Filtering models for fair value estimation"},"type":"lvl5","url":"/markdown/fair-price-estimation#hit-miss","position":30},{"hierarchy":{"lvl1":"Fair value estimation","lvl5":"Hit & Miss","lvl4":"RfQ traded","lvl3":"Pricing sources and models","lvl2":"Filtering models for fair value estimation"},"content":"Another source of pricing information comes from analyzing patterns of trading information from the dealer. In particular, it is useful to analyze the hit & miss, i.e. the ratio between won RfQs over the total traded by the client with any dealer (i.e. excluding those where the client did not trade at all), over a certain window of time. Intuitively, a dealer that has an abnormally high or low hit & miss might be using an incorrect fair value estimation from the quoting. Of course, the tricky question here is to assess what are the normal levels of hit & miss, which have to take into account the influence of factors like, for example, inventory levels in the spreads quoted: if a dealer has relatively high inventory holdings, it is likely that she will skew quotes to reduce inventory risk, hence the hit & miss will be higher if there are more quotes overall in the direction of risk reduction.\n\nIf the dealer has estimated a model that estimates the probability of winning the RfQs, and the model is well calibrated, it can be evaluated for the same windows of RfQs as the empirical hit & miss (H&M). If we compute the latter at time t using the last n RfQs, it is given by:\\text{H\\&M}_t = \\frac{1}{n} \\sum_{i=1}^n 1_{\\text{win}_i}\n\nwhere \\text{win}_i is an abbreviation of the condition \\text{status}(i) = \\text{win}. For a well calibrated model, such empirical hit & miss should be close to the expected by the model:E[\\text{H\\&M}_t] = \\frac{1}{n} \\sum_{i=1}^n P(\\text{win}_i|\\mathcal{F}_{t_i})\n\nHere, \\mathcal{F}_{t_i} are the filtrations at the time t_i of request of the i-th RfQ, which include the spreads \\delta_i quoted by the dealer:\\delta_i = \\text{side} (P_{i} - m_{t_i})\n\nwith P_{i} the price quoted for the i-th RfQ and m_{t_i} the estimation of the fair value at time t_i.\n\nIf we consider each RfQ independent of each other, the variance of the hit & miss provides us with a scale of natural variability of our estimation versus the empirical hit & miss:Var[\\text{H\\&M}] = \\frac{1}{n} \\sum_{i=1}^n  P(\\text{win}_i|\\mathcal{F}_{t_i}) (1-P(\\text{win}_i|\\mathcal{F}_{t_i}))\n\nIntuitively, then, if we get a persistent deviation between \\text{H\\&M}_t and E[\\text{H\\&M}_{t}] that cannot be explained by the expected variability from the randomness of the RfQ process, this could be attributed to a potential bias in the estimation of m_t, which becomes a further pricing source for our fair value estimation model. Further modelization is required, though, to inject this information into the Kalman filter, which goes beyond the scope of this book. For us, it suffices to point out how we can potentially convert hit & miss deviations from the target into pricing information.","type":"content","url":"/markdown/fair-price-estimation#hit-miss","position":31},{"hierarchy":{"lvl1":"Fair value estimation","lvl4":"Correlated instruments","lvl3":"Pricing sources and models","lvl2":"Filtering models for fair value estimation"},"type":"lvl4","url":"/markdown/fair-price-estimation#correlated-instruments","position":32},{"hierarchy":{"lvl1":"Fair value estimation","lvl4":"Correlated instruments","lvl3":"Pricing sources and models","lvl2":"Filtering models for fair value estimation"},"content":"As we discussed above, if a set of instruments exhibit historical price correlations and we have reasons to believe those are structural correlations, i.e. they will continue existing in the present, we can do a joint estimation of the fair values using a multivariate Kalman filter. This way, information about the price of one instrument can be used to improve the estimation of the others. The pricing sources for these instruments can be any of those discussed previously.\n\nThere are some caveats though to take into account when using this source of pricing information:\n\nThe first one is that correlations are inferred parameters ans therefore themselves subjected to a certain amount of model risk. The Kalman filter model uses point estimations of the correlations for the updates, and therefore ignores the potential uncertainty associated to the estimation when updating the price. This issue can actually become quite relevant in certain situations, for example when estimating the price of illiquid instruments using more liquid ones. The liquid instrument will have a smaller estimation error than the illiquid one. If point correlation between the instruments is high, it can have a over-weighted influence on the estimation of the price of the illiquid instrument, overriding the intrinsic price dynamics of the illiquid instrument. A Bayesian treatment of correlation can overcome these issues, but then the Kalman filter estimation algorithm can no longer be used, requiring a numerical computation of posterior probabilities for the predict and update steps.\n\nThe second one arises when estimating the correlations to be used in the Kalman filter model. The typical estimation of correlations used in financial models uses synchronous pricing data, for instance end of day prices. However, rigorously speaking, the correlations used in the Kalman filter model for fair value estimation are between latent fair values, not price observations. A consistent estimation therefore requires us to compute them endogenously, for example using Maximum Likelihood Estimation or Expectation Maximization over the historical pricing data. The difficulty lies when defining estimators (e.g. MLE ones) for the covariance matrix of asynchronous data. The standard estimator is not valid and we must resort to other less standard estimators. This topic is discussed extensively in \n\nGuo et al., 2017, where they suggest using Fourier methods, among others. Again, as in the previous case, a Bayesian modelling approach for the covariance matrix provides the natural way to circumvent these issues, at the price of adding numerical complexity to the computation.","type":"content","url":"/markdown/fair-price-estimation#correlated-instruments","position":33},{"hierarchy":{"lvl1":"Fair value estimation","lvl2":"Fundamental models for fair value estimation"},"type":"lvl2","url":"/markdown/fair-price-estimation#fundamental-models-for-fair-value-estimation","position":34},{"hierarchy":{"lvl1":"Fair value estimation","lvl2":"Fundamental models for fair value estimation"},"content":"Fundamental models estimate the fair value of a financial instrument by analysing the value of their future cash-flows. Recall from our introductory chapter on financial markets \n\nFinancial Markets, that financial instruments are essentially contracts that promise to pay back funds to the investor that purchases it under conditions specified in the contract.","type":"content","url":"/markdown/fair-price-estimation#fundamental-models-for-fair-value-estimation","position":35},{"hierarchy":{"lvl1":"Fair value estimation","lvl3":"The present value theory of fair value","lvl2":"Fundamental models for fair value estimation"},"type":"lvl3","url":"/markdown/fair-price-estimation#the-present-value-theory-of-fair-value","position":36},{"hierarchy":{"lvl1":"Fair value estimation","lvl3":"The present value theory of fair value","lvl2":"Fundamental models for fair value estimation"},"content":"Even the most simple financial instrument, a promise to pay back a deterministic amount of money in a future fixed date, requires some theoretical hypothesis to estimate its fair value. The basic idea is that a unit of currency received today is worth more than the same unit received in the future, because it can be invested in the interim. Consider a risk-free deposit that pays a deterministic interest rate r. An amount of one unit invested today grows to (1+r)^T units after T periods, as far as interest rate payments are reinvested at the same rate (compounded interest). Conversely, receiving one unit in t periods is economically equivalent to receiving 1/(1+r)^T units today. This opportunity cost argument implies that any future cash flow must be discounted by the factor (1+r)^T to make it comparable with cash today. Such economic consideration is referred as the time value of money.\n\nFormally, for an investment that delivers a future cash flow C_T at time T, the present value PV satisfiesPV = \\frac{C_T}{(1+r)^T}\n\nwhere we have assumed that an interest payment of r C_T is paid each unit of time. The factor 1/(1+r)^T is called the discount factor, since it is used to discount future cash-flows. Present values becomes our fair value estimation within this framework.\n\nA typical hypothesis that provides useful mathematical simplifications is that of continuously accrued interest rates. This is also a good approximation for real situations where interests are paid daily, for instance in money market funds. Consider and account that pays interests each time period of size \\Delta. The interest paid is r \\Delta over a unitary notional. If we reinvest the interests, at time t we have accumulated (1+ r \\Delta)^{T/\\Delta}. If we now take the limit \\Delta \\rightarrow 0:\\lim_{\\Delta \\rightarrow 0} (1+ r \\Delta)^{T/\\Delta} = \\lim_{\\Delta \\rightarrow 0} e^{\\frac{T}{\\Delta} \\log (1 + r \\Delta)} =  e^{rT}\n\nThis gives us the expression of the discount factor for continuously paying interest rates, given by the inverse e^{-rT}. Under this approximation, the preset value of our simple financial instrument becomes:PV = e^{-rT} C_T\n\nExponential functions provide a lot of mathematical simplifications, hence the usefulness of this limit, particularly when applied to the computation of fair value for more complex financial instruments.\n\nIf we also include the price paid for the financial instrument, let us say in this case we pay initially C_0, we define net present value (NPV) as:NPV = e^{-rT} C_T - C_0\n\nNotice that, therefore, a rational investor will only be willing to invest in this financial instrument if C_0 \\leq e^{-rT} C_T, otherwise its NPV would be negative.\n\nWe can now generalize this expression for a financial instrument that pays a stream of deterministic cash-flows C_{t_i} at times t_i, i = 1, ..., N. This is the case for example of standard government bonds issued by most countries. The fair value given by present value becomes then:PV = \\sum_{i=1}^N e^{-r t_i} C_{t_i}\n\nWhat happens if these future cash-flows are contingent to information not known at present day? For instance, we can have bonds that pay floating interest rates depending on reference rates that don’t get fixed until a future date. Shares pay dividends contingent to the financial results of the corporation that issues the shares. And derivatives are financial instruments whose value depends on the future price of an underlying instrument, hence the name “derivative”. A simple naive extension of the theory of present value would consider these future cash-flows as functions of random variables, replacing its future value by an expectation of future value:PV_0 = {\\mathbb E}\\left[\\sum_{i=1}^N e^{-r t_i} C_{t_i}|F_0\\right] \\equiv {\\mathbb E}_0 \\left[\\sum_{i=1}^N e^{-r t_i} C_{t_i}\\right]\n\nwhere we have conditioned the expectation to the information available at the time of estimation, the filtration F_0. If discount factors are not stochastic themselves (an argument we will revisit in the last section of this chapter), this becomes:PV_0 = \\sum_{i=1}^N e^{-r t_i} {\\mathbb E}_0 \\left[C_{t_i}\\right]\n\nThe issue with this approach is that, once future cash-flows become uncertain, their expected value is a point estimation of their possible range of values that neglects the rest of potential scenarios that can happen. Recall from our discussion in chapter \n\nBayesian Modelling, that choosing to represent a random variable by their expected value in terms of decision theory (and fair value estimation is in the end linked to the decision to buy or sell a financial instrument) makes sense when the investor penalizes errors in the estimation using a square loss function. The behavior of real rational investors, though, shows a more asymmetric loss function, in which generally potential extra gains are valued less than the equivalent potential losses. This kind of behavior is better capture by using utility functions, as we will discuss in the next section.","type":"content","url":"/markdown/fair-price-estimation#the-present-value-theory-of-fair-value","position":37},{"hierarchy":{"lvl1":"Fair value estimation","lvl3":"The utility indifference theory of fair value estimation","lvl2":"Fundamental models for fair value estimation"},"type":"lvl3","url":"/markdown/fair-price-estimation#the-utility-indifference-theory-of-fair-value-estimation","position":38},{"hierarchy":{"lvl1":"Fair value estimation","lvl3":"The utility indifference theory of fair value estimation","lvl2":"Fundamental models for fair value estimation"},"content":"To ground the discussion in another example, let us consider a specific case of a future uncertain cash-flow whose value depends on the price of another instrument at the time of payment, S_T, for example a stock. The cash-flow is therefore C_T = f(S_T). Notice that this is a specific case of a derivative’s contract. The function f(S_T) is called the pay-off of the derivative, the instrument whose price is S_T is called the underlying of the derivative, and the time T is the expiry date of the derivative. Apart from the value S_T, it can also depend on other parameters that are deterministic. For instance, for an European call option we have C_T = (S_T-K)^+, where K is called the strike of the option. An european put options has a payoff C_T = (K-S_T)^+. There are also American options where the option can be exercised before the expiry date, which becomes itself a random variable.\n\nThis derivative pays in the future a quantity that is contingent to the future value of the underlying, whose value is known today but is uncertain in the future. To get an estimation, we need to use probability theory to put some bounds to our uncertainty, so we characterize S_T by a random distribution function g(S_T). As we saw in chapter \n\nStochastic Calculus, a popular model that allows us to compute such future distribution is a random walk model or a geometric random walk, the latter being a natural choice for prices that cannot be negative. In those cases the future distribution can be computed, being a normal distribution in the first case, and a log-normal distribution in the second, e.g. in the case of stocks. Although for short expiries the random walk can also be a good model for stocks. These models allow us to get sometimes closed-form solutions, but more realistic models that capture better empirical distributions of prices can be used.\n\nAs mentioned in the previous section, we cannot just value this cash-flow using the expected value of the pay-off, since it would ignore the risk-profile of the investor. As discussed in more detail in chapter\n\nStochastic optimal control, utility functions provide a mathematical formalism that allows us to capture realistic risk behaviors. Utility provides a description of the value that the cash-flows derived from the financial instrument have for the investor. Typical utility functions show the notion of marginally decreasing utility for increasingly larger cash-flows. In situations where cash-flows are random variables, such behavior models investors that are risk averse, meaning that they need to be compensated increasingly more to take on extra risk.\n\nTo apply the utility function framework to the problem of fair pricing, we need to compute expected utilities to characterize the value that the investor places on the contract. Using a exponential utility function for simplicity, this means:\\mathbb{E}_t[U] = 1- \\mathbb{E}_t[e^{-\\gamma_i \\left(e^{-r(T-t)}f(S_T)\\right)}]\n\nwhere \\gamma_i is the risk aversion coefficient of the investor. We have discounted the payoff at T by the discount factor e^{-r(T-t)} in order to consider the time value of money, as discussed in the previous section, although now we consider for generality a initial time t.\n\nThe fair value in this formalism is the so-called premium of the derivative, denoted C_t, that the investors is willing to pay (or be paid, depending on the pay-off function) to enter into the derivative’s contract today. This changes the utility calculation, since it needs to take into account the premium:\\mathbb{E}_t[U] = 1- \\mathbb{E}_t[e^{-\\gamma_i \\left(e^{-r(T-t)}f(S_T) -C_t\\right)}]\n\nWhen modelling rational risk-averse agents with utility functions, we model their decisions as those that maximize the expected utility. However, in this case this cannot be used to compute the premium, since naturally the premium that maximizes utility is C_t = -\\infty!. The problem is, of course, that it does not take into account the utility maximization of the dealer selling the derivative, who would not enter into the contract at this premium. Of course, the same framework could be used to model the dealer’s payoff, which is the reverse from the investor, albeit with a different dealer’s risk aversion, \\gamma_d:\\mathbb{E}_t[U] = 1- \\mathbb{E}_t[e^{-\\gamma_d \\left(C_t - e^{-r(T-t)}f(S_T) \\right)}]\n\nbut even introducing the dealer’s utility function, how could we compute the value of the premium?\n\nFor the answer, we need first to frame the problem in other terms: what is the maximum premium that the investor would be willing to pay to enter into the contract? Since the alternative to not entering into the contract implies a zero payoff with total certainty, whose expected utility in this framework is 0\\$, we can argue that the investor would be willing to buy the derivative as far as the premium makes him/her better off, i.e. \\mathbb{E}_t[U] > 0. For a value of the premium such that \\mathbb{E}_t[U] = 0 , the investor is indifferent to buy or not buy. This value of the premium is called the reservation price or the utility indifference price of the investor. Of course the same computation could be done for the dealer, obtaining a different reservation price. An agreement will only happen if the maximum premium that the investor is willing to pay is above the minimum premium that the dealer is willing to receive.\n\nLet us first see the problem from the dealer’s point of view. In real situations, it is typically the investor who comes to the dealer and request a price for the derivative. The minimum premium that the dealer would be willing to accept to provide the contract as a reference for derivatives pricing, i.e. the reservation price of the dealer, is the one that solves:1- \\mathbb{E}_0[e^{-\\gamma_d \\left(C_t - e^{-r(T-t)}f(S_T) \\right)}] = 0\n\nWe can now obtain a general expression for the premium:C_t^d =  \\frac{1}{\\gamma_d} \\log \\mathbb{E}_0[e^{\\gamma_d \\left(e^{-r(T-t)}f(S_T)\\right)}] = \\frac{1}{\\gamma_d} \\log \\int dS_T g(S_T) e^{\\gamma_d \\left(e^{-r(T-t)}f(S_T)\\right)}\n\nFor those dealers that have zero risk aversion, i.e. they are risk neutral, by taking the limit \\gamma_d \\rightarrow 0 we get:C_{t}(0) = \\mathbb{E}_0[ e^{-r(T-t)}f(S_T)] = \\int dS_T g(S_T) e^{-r(T-t)}f(S_T)\n\nAnd for small, but positive risk aversion:C_t^d = C_{t}(0) +  \\frac{\\gamma_d}{2}\\int dS_T g(S_T) e^{-2r(T-t)}f^2(S_T) + O(\\gamma_d^2)\n\nWe can derive the same expression for a investor we get:C_t^i =  -\\frac{1}{\\gamma_i} \\log \\mathbb{E}_0[e^{-\\gamma_i \\left(e^{-r(T-t)}f(S_T)\\right)}] = -\\frac{1}{\\gamma_i} \\log \\int dS_T g(S_T) e^{-\\gamma_i \\left(e^{-r(T-t)}f(S_T)\\right)}\n\nIf the investor has a small but positive risk aversion:C_t^i = C_{t}(0)- \\frac{\\gamma_i}{2}\\int dS_T g(S_T) e^{-2r(T-t)}f^2(S_T) + O(\\gamma_i^2)\n\nWe see immediately that C_t^i \\leq C_d^i, so there is only agreement if both investor and dealer are risk neutral, or at least one is risk prone, which is not a normal situation. Therefore, according to this theory of pricing, there would not be trading of derivatives! However, we know empirically that it is not the case. So what was wrong in our theory? We will see that the dealer is not simply taking the opposite bet than the investor, and therefore we need to modify this analysis. Before that, though, let us see particular examples of the computation of the premium for investors.","type":"content","url":"/markdown/fair-price-estimation#the-utility-indifference-theory-of-fair-value-estimation","position":39},{"hierarchy":{"lvl1":"Fair value estimation","lvl4":"Example: pricing of a simple contingent claim","lvl3":"The utility indifference theory of fair value estimation","lvl2":"Fundamental models for fair value estimation"},"type":"lvl4","url":"/markdown/fair-price-estimation#example-pricing-of-a-simple-contingent-claim","position":40},{"hierarchy":{"lvl1":"Fair value estimation","lvl4":"Example: pricing of a simple contingent claim","lvl3":"The utility indifference theory of fair value estimation","lvl2":"Fundamental models for fair value estimation"},"content":"A contingent claim is a contract that pays off only under the realization of an uncertain event. Many derivatives contracts like options are contingent claims. The most simple contingent claim pays 1$ under the realization of a specific uncertain event, and zero in all other cases. These contingent claims are called Arrow-Debreu securities, and have a theoretical interest since we could in principle decompose any contingent claim as a linear combination of these securities. Therefore, if we know the prices (premiums) of Arrow-Debreu securities, we could price any contingent claim. We say that in this case we have a complete market, where we can trade instruments linked to any future state of the market.\n\nFor our purposes, though, we just want to discuss a simple example of reservation prices. Let us consider a contingent claim in which the dealer pays the investor 1$ if we get heads when tossing a fair coin in the present. In our framework, the underlying now is the side of the coin, heads or tails, with probabilities p_H = p_T = 1/2. We also make T = t since we toss the coin in the present. The value of the reservation price for the investor reads then:C_t^i = -\\frac{1}{\\gamma_i} \\log \\left(\\frac{1}{2}e^{-\\gamma_i} + \\frac{1}{2} \\right) = \\frac{1}{2} - \\frac{1}{\\gamma_i}\\log \\cosh \\left(\\frac{\\gamma_i}{2}\\right)\n\nFor a risk-neutral investor, by making \\gamma_i \\rightarrow 0, we get simply C_t^i = 1/2, which makes sense: the investor is willing to pay 0.5$ to make the game fair. Or in other terms, to make the expected value of the game zero. A fully risk averse investor for whom \\gamma_i \\rightarrow \\infty has C_t = 0, i.e. only is willing to buy the contract when there is guarantee of no losses under any scenario. In the middle, the premium lies between those two values: the investor will be willing to pay more than 0$ to trade, as far as the payoff is skewed in its favor.","type":"content","url":"/markdown/fair-price-estimation#example-pricing-of-a-simple-contingent-claim","position":41},{"hierarchy":{"lvl1":"Fair value estimation","lvl4":"Example: Forward on a non-dividend paying stock","lvl3":"The utility indifference theory of fair value estimation","lvl2":"Fundamental models for fair value estimation"},"type":"lvl4","url":"/markdown/fair-price-estimation#example-forward-on-a-non-dividend-paying-stock","position":42},{"hierarchy":{"lvl1":"Fair value estimation","lvl4":"Example: Forward on a non-dividend paying stock","lvl3":"The utility indifference theory of fair value estimation","lvl2":"Fundamental models for fair value estimation"},"content":"Let us now focus on a more realistic case and find the maximum premium that a risk averse investor would be willing to pay for a forward contract on a non-dividend paying stock . The buyer of a forward has the obligation to buy a stock at the expire T at a pre-agreed price K. Therefore, the payoff function reads:f(S_T) = S_T - K\n\nThe maximum premium that the investor is willing to pay reads then:C_t^i = - \\frac{1}{\\gamma_i} \\log \\int dS_T g(S_T) e^{-\\gamma_i e^{-r(T-t)}(S_T-K)}\n\nwhich in the case of a risk-neutral investor reduces to:C_{t}^i(0) = \\int dS_T g(S_T) e^{-r(T-t)}(S_T-K) = e^{-r(T-t)}(E[S_T] - K)\n\ni.e. the price is simply the discounted expected pay-off. The expectation represents the belief from the investor on the value of the stock at expiry. It is model-free, meaning that don’t need to specify a model for the evolution of the stock to compute the maximum premium, although of course an investor could use a model to compute it. The value of the premium has the following dependencies:\n\nThe larger the expected value of the stock at T, the more the investor is willing to pay for the forward\n\nThe larger the risk-free interest rate r, the lower the investor is willing to pay for the forward, since the present value of the payoff is reduced\n\nThe larger the strike, the less attractive is the forward purchase and therefore the less the investor is willing to pay for it.","type":"content","url":"/markdown/fair-price-estimation#example-forward-on-a-non-dividend-paying-stock","position":43},{"hierarchy":{"lvl1":"Fair value estimation","lvl4":"Example: European Call Options","lvl3":"The utility indifference theory of fair value estimation","lvl2":"Fundamental models for fair value estimation"},"type":"lvl4","url":"/markdown/fair-price-estimation#example-european-call-options","position":44},{"hierarchy":{"lvl1":"Fair value estimation","lvl4":"Example: European Call Options","lvl3":"The utility indifference theory of fair value estimation","lvl2":"Fundamental models for fair value estimation"},"content":"The calculation for a forward was relatively tractable since the payoff of the derivative was linear on the stock. What about non-linear payoffs? This is the case for instance of an European Call option on a stock, which has the payoff:f(S_T) = (S_T - K)^+\n\nwhere K is the strike of the option. The risk-averse investor will be willing to pay the dealer a maximum premium of:C_t^i = -\\frac{1}{\\gamma_i} \\log \\int dS_T g(S_T) e^{-\\gamma_i e^{-r(T-t)}(S_T-K)^+}\n\nIn the limit of a risk-neutral investor, the premium is:C_{t}^i(0) = \\int dS_T g(S_T) e^{-r(T-t)} (S_T-K)^+\n\nLet us consider the case of a non-dividend paying stock, which we model as Geometrical Brownian Motion to ensure non-negative prices are allowed:d S_t = \\mu S_t dt + \\sigma S_t d W_t\n\nwhere \\mu and \\sigma are the drift and volatility of the stock, respectively, and W_t a Wiener process. Integrating this SDE up to T:S_T = S_t e^{(\\mu - \\frac{\\sigma^2}{2})(T-t) + \\sigma \\sqrt{T-t} Z}\n\nwhere Z \\sim N(0,1). We can further decompose the expression of the premimum as:C_{t}^i(0) = \\int_0^\\infty dS_T g(S_T) e^{-r(T-t)}  (S_T-K)^+ = \\int_K^\\infty dS_T g(S_T) e^{-r(T-t)}  (S_T - K)= e^{-r(T-t)} \\left(\\int_K^\\infty dS_T g(S_T) S_T - K \\int_K^\\infty dS_T g(S_T)\\right)\n\nLet us now change variables from S_T to Z in the integration. The first integral becomes:\\int_K^\\infty dS_T g(S_T) S_T = S_t \\int_{\\frac{1}{\\sigma \\sqrt{T-t}} \\left(\\log \\frac{K}{S_t} - (\\mu - \\frac{\\sigma^2}{2})(T-t)\\right)}^\\infty \\frac{dZ}{\\sqrt{2\\pi}} e^{-\\frac{Z^2}{2}} e^{(\\mu - \\frac{\\sigma^2}{2})(T-t) + \\sigma \\sqrt{T-t} Z}= S_t e^{(\\mu - \\frac{\\sigma^2}{2})(T-t)} \\int_{-d_2(\\mu)}^\\infty \\frac{dZ}{\\sqrt{2\\pi}} e^{-\\frac{Z^2}{2}} e^{ \\sigma \\sqrt{T-t} Z} = S_t e^{\\mu (T-t)} \\int_{-d_2(\\mu)-\\sigma\\sqrt{T-t}}^\\infty \\frac{dZ}{\\sqrt{2\\pi}} e^{-\\frac{(Z-\\sigma \\sqrt{T-t})^2}{2}}= S_t e^{\\mu (T-t)} \\left(1-N(-d_1(\\mu))\\right)\n\nwhere we have defined the functions:d_1(\\mu) = \\frac{1}{\\sigma \\sqrt{T-t}} \\left(\\log \\frac{S_t}{K} + (\\mu - \\frac{\\sigma^2}{2})(T-t)\\right)d_2(\\mu) = \\frac{1}{\\sigma \\sqrt{T-t}} \\left(\\log \\frac{S_t}{K} + (\\mu + \\frac{\\sigma^2}{2})(T-t)\\right)\n\nand N(x) is the cumulative distribution function of the standard normal distribution. The second integral is then:\\int_K^\\infty dS_T g(S_T) = \\int_{-d_2(\\mu)}^\\infty \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{Z^2}{2}} = 1-N(-d_2(\\mu))\n\nWrapping up all we get:C_{t}^i(0)=S_t e^{(\\mu-r)(T-t)}N(d_1(\\mu))-K e^{-r(T-t)} N(d_2(\\mu))\n\nwhere we have used the property 1-N(-x) = N(x). Let us analyze this formula, which recall is the maximum premium that a investor is willing to pay for the call option. It depends on the following parameters:\n\nCurrent stock price S_t: as the current stock price increases, the premium increases. This is because the call option gives the right to buy the stock at the strike price, so a higher current stock price makes this option more valuable.\n\nStrike price K: as the strike price increases, the premium decreases. A higher strike price makes the option less valuable since it would cost more to exercise the option.\n\nTime to maturity T-t: as the time to maturity increases, the premium  increases. More time to maturity means more opportunity for the stock price to move favorably.\n\nRisk-free interest rate r: as the risk-free interest rate increases, the premium decreases. A higher risk-free rate reduces the present value of the option payoff, making the option less attractive.\n\nExpected stock drift \\mu: as the stock drift increases, the premium increases, since it is more likely that the option will be exercised.\n\nExpected volatility \\sigma: as volatility increases, the premium increases. Higher volatility increases the probability of the stock price moving significantly, which benefits the option holder.\n\nWe plot those dependencies in the following picture:\n\n\n\nFigure 2:Dependencies of a call option premium for a risk neutral investor, derived as the maximum premium the investor is willing to pay (reservation or indifference price). We use the parameters S_t=100, K =100, T-t = 1 in years, r = 0.05, \\mu = 0.1, \\sigma = 0.2.","type":"content","url":"/markdown/fair-price-estimation#example-european-call-options","position":45},{"hierarchy":{"lvl1":"Fair value estimation","lvl3":"The arbitrage-free theory of derivatives pricing","lvl2":"Fundamental models for fair value estimation"},"type":"lvl3","url":"/markdown/fair-price-estimation#the-arbitrage-free-theory-of-derivatives-pricing","position":46},{"hierarchy":{"lvl1":"Fair value estimation","lvl3":"The arbitrage-free theory of derivatives pricing","lvl2":"Fundamental models for fair value estimation"},"content":"When we were applying the utility indifference theory of derivatives pricing to both parties agreeing in the transaction, the investor and the dealer, we found the issue that according to this theory, there would be a trade only if both of them are risk averse, since they are taking opposite sides of the bet on the payoff result. In reality, both dealers are investors are generally risk-averse and we observe transactions, so what are we missing in our theory?\n\nThe answer, as we anticipated above, is that the dealer is not really simply taking the opposite of the bet. There are mainly two ways a dealer will conduct this business:\n\nIf the market on derivatives is relatively liquid in both sides, with plenty of investors willing to take the long or short position in the derivative over periods much shorter than the expiry, the dealer will act as a market-maker of the derivatives. In this case, the dealer will quote bid and asks prices for the derivatives that compensate it for the provision of liquidity with its own capital, incurring potential inventory risk or information asymmetry risk\n\nIf the market is one-sided and / or illiquid, in the sense of having a small number of potential transactions before the expiry of the contract, the dealer will try to hedge the risk using other financial instruments available.\n\nIn practice, a combination of both situations will also happen often\n\nAs we discussed in the first part of this chapter, if we are in the first situation, we might not need a derivatives pricing model since we can extract the prices of derivatives directly from observations of trades or request for quotes that are not closed. It is the second case where we need a theory of derivatives pricing that takes into account potential hedging strategies that mitigate the risk of the dealer. We anticipate then that in this framework, the minimum price or premium that the dealer will accept to sell the derivative will be one that compensates it for the costs of hedging plus the residual risk.\n\nMore interestingly, we will see that in some cases, under certain theoretical situations, a perfect hedging strategy might exist, so the minimum price will be exactly the cost of the hedging strategy, which in this setup is also called the perfect replication strategy (since a perfect hedging implies no risk, and therefore a replication of the payoff using other financial instruments). A consequence of the existence of such replication strategy is that dealers are forced to price derivatives consistently, otherwise they would be generating risk-free arbitrage opportunities where other dealers who price correctly the derivative trade with the one miss-pricing it, and pocket the difference without risk. Hence, this theory of derivatives pricing is also called the arbitrage-free theory of derivatives pricing.\n\nLet us revisit the case of forwards and options under this optic.","type":"content","url":"/markdown/fair-price-estimation#the-arbitrage-free-theory-of-derivatives-pricing","position":47},{"hierarchy":{"lvl1":"Fair value estimation","lvl4":"Example: Forward on a non-dividend paying stock","lvl3":"The arbitrage-free theory of derivatives pricing","lvl2":"Fundamental models for fair value estimation"},"type":"lvl4","url":"/markdown/fair-price-estimation#example-forward-on-a-non-dividend-paying-stock-1","position":48},{"hierarchy":{"lvl1":"Fair value estimation","lvl4":"Example: Forward on a non-dividend paying stock","lvl3":"The arbitrage-free theory of derivatives pricing","lvl2":"Fundamental models for fair value estimation"},"content":"Under the modelling hypothesis used in the previous sections to value the premium of a forward, namely, that 1) the interest risk is locked during the period of the forward, 2) there is no counter-party risk, i.e. no risk that the investor will not satisfy its obligations, then there is actually a simple replication strategy that hedges all the risk of the contract. If the dealer is selling the forward to the investor, therefore guaranteeing a price K to buy a share at time T, then:\n\nBorrows S_t dollars in the repo monetary markets at interest rate r during the period of the forward. We assume that interest accrues daily but is settled at the expiry. The stock bought is used as collateral in the repo.\n\nBuys the underlying stock with this money at inception, paying S_t\n\nAt the expiry of the forward it delivers the stock to the investor, receives K and repays the loan plus remaining interests\n\nIf we consider that daily accruing of interest can be well approximated by continuous accruing, then the investor needs to repay at T an amount S_t e^{r(T-t)}. The payoff for the dealer at the expiry is therefore:K - S_t e^{r(T-t)}\n\nwhich is deterministic under the hypotheses of the model. A rational dealer of course will not accept a determinist loss, so the minimum premium that will command for this contract is the discounted value of this payoff:C_{F,t} = K e^{-r(T-t)} - S_t\n\nIn practice, forward markets work by quoting the strike such that the premium is zero, hence:F_t \\equiv K = S_t e^{r(T-t)}\n\nThis is the arbitrage-free price of a forward contract. As mentioned above, it is called arbitrage-free since any other price would represent a risk-free arbitrage opportunity for other dealer. For example, let’s assume this dealer quotes a \\bar{K}_t < K_t. Another dealer could buy the forward from the dealer, and use the opposite replication strategy:\n\nBorrow the stock in repo and sell it in the market at price S_t to fund the repo.\n\nAt time T close the repo with the stock delivered by the dealer selling the forward, receive S_t e^{r(T-t)} in cash and pay \\bar{K}_t.\n\nThe payoff for this dealer is then:S_t e^{r(T-t)} - \\bar{K}_t >  S_t e^{r(T-t)} - S_t e^{r(T-t)}  = 0\n\ni.e. a risk-free profit!","type":"content","url":"/markdown/fair-price-estimation#example-forward-on-a-non-dividend-paying-stock-1","position":49},{"hierarchy":{"lvl1":"Fair value estimation","lvl4":"Example: European Call Options","lvl3":"The arbitrage-free theory of derivatives pricing","lvl2":"Fundamental models for fair value estimation"},"type":"lvl4","url":"/markdown/fair-price-estimation#example-european-call-options-1","position":50},{"hierarchy":{"lvl1":"Fair value estimation","lvl4":"Example: European Call Options","lvl3":"The arbitrage-free theory of derivatives pricing","lvl2":"Fundamental models for fair value estimation"},"content":"In the case of options there is no such obvious static replication strategy if we are only allowed to use the underlying stock and repo contracts. By static replication strategy we mean that we don’t need to modify the positions of the replication portfolio (the stock and the repo) during the life of the forward. If we are able to trade other derivatives there is actually a static replication strategy. If the dealer sells the call option to an investor, then immediately\n\nBuys to other dealer a put option with the same strike and expiry\n\nBuys to another dealer a forward with the same strike and expiry\n\nThe payoff at the expiry is:-(S_T-K)^+ + (K-S_T)^+ + (S_T-K) = 0\n\nmeaning that the replication portfolio of a put and a forward replicates the call option. At inception, the dealer is paid for the call a premium C_{C,t} and pays C_{P,t} for the put and C_{F,t} for the forward, hence the payoff at inception is:C_{C,t} - C_{F,t} - C_{F,t}\n\nIn order to avoid losses, the minimum premium that must command is therefore:C_{C,t} = C_{F,t} + C_{F,t}\n\nwhich is called the put-call parity relationship. The premium for the forward can be derived as discussed in the previous section, however we are left with a sort of chicken and the egg problem with regards to the call and put premiums: given one, we can determine the other, but we don’t have yet a replication strategy for the put to derive the call premium, and vice-versa.\n\nIf no static replication is available, is it possible to find a dynamical one that reproduces the payoff without uncertainty? Or are we left with strategies that, though they might minimize uncertainty, they don’t remove it and therefore we need to go back to our utility indifference theory?","type":"content","url":"/markdown/fair-price-estimation#example-european-call-options-1","position":51},{"hierarchy":{"lvl1":"Fair value estimation","lvl4":"The Black - Scholes - Merton Model","lvl3":"The arbitrage-free theory of derivatives pricing","lvl2":"Fundamental models for fair value estimation"},"type":"lvl4","url":"/markdown/fair-price-estimation#the-black-scholes-merton-model","position":52},{"hierarchy":{"lvl1":"Fair value estimation","lvl4":"The Black - Scholes - Merton Model","lvl3":"The arbitrage-free theory of derivatives pricing","lvl2":"Fundamental models for fair value estimation"},"content":"Fisher Black and Myron Scholes \n\nBlack & Scholes, 1973, and separately Robert Merton \n\nMerton, 1973 provided an answer to this question: under certain theoretical conditions, we can indeed find a dynamic replication strategy based on the underlying stock and a risk free account, that reproduces the payoff with no uncertainty. The main conditions are the following:\n\nThe stock price follows a Geometric Brownian Motion dynamics:d S_t = \\mu S_t dt + \\sigma S_t d W_t\n\nWe have considered the drift $\\mu$ and volatility $\\sigma$ constant, but the model can be generalized to time-dependent deterministic drifts and volatilities. Other dynamics can also be considered within the same framework. Also, we will consider a non-dividend paying stock, but the model can be adjusted to consider deterministic dividends.\n\nThe replicating portfolio is composed of a position \\Delta_t in the underlying stock and a cash account \\beta_t from which we can borrow or lend money freely at a determinist risk-free rate r:\\Pi_t = \\Delta_t S_t + \\beta_t\n\nThe model can be generalized easily to time-dependent risk-free interest rates. The original model considers a generic cash account, although in practice is more realistic to assume a repo on the stock is used for funding.\n\nThe position in the stock can be adjusted continuously over the life of the option, without restriction like market trading hours, etc. There are also no transaction costs when buying or selling the stock. There are no restrictions on shorting the stock.\n\nThe replication portfolio is self-financing, meaning that no cash is added or withdrawn from the portfolio after the initial investment. Any proceeds from selling at immediately reinvested in the portfolio.  Mathematically, it means that:d\\Pi_t = \\Delta_t d S_t + d \\beta_t = \\Delta_t d S_t + r \\beta_t dt = \\Delta_t d S_t + r (\\Pi_t - \\Delta_t S_t) dt\n\nNotice that the position in the stock depends on time, but is always adjusted ex-post, i.e. after the market moves.\n\nThere is no counterparty default risk, i.e. the investor and the dealer will pay whatever obligations they have at the end of the option\n\nIf the portfolio \\Pi_t indeed replicates the option payoff at the maturity T in any scenario, we must have \\Pi_T = C_T = (S_T - K)^+, where we have considered a Call option but the argument applies to any other payoff function. But since by construction the portfolio \\Pi_t is self-financing, then if such dynamic strategy exists we must have \\Pi_t = C_t for any time t \\leq T. Otherwise, there would be a risk-free arbitrage opportunity, e.g. selling the option at price C_t in the case C_t > \\Pi_t, buying with the cash from the premium the replicating portfolio \\Pi_t and making an instantaneous gain of the difference C_t - \\Pi_t. The same reasoning applies C_t < \\Pi_t, only in this case we buy the option paying a discounted premium. Therefore, if we can find such strategy we immediately solve the problem of option pricing, since the premium is equal to the cost of replication by using the non-arbitrage opportunity argument.\n\nThe condition \\Pi_t = C_t is equivalent to \\Pi_T = C_T and d \\Pi_t = dC_t. Additionally, this equality implies that C_t = C(t, S_t), i.e. the premium of the option has to be a function of time and the contemporary value of the stock. We can then use Ito’s lemma to further understand the requirements for such strategy to exist:d C_t = \\frac{\\partial C_t}{\\partial t} dt + \\frac{\\partial C_t}{\\partial S_t} dS_t + \\frac{1}{2}\\frac{\\partial^2 C_t}{\\partial^2 S_t} \\sigma^2 S_t^2 dt = d\\Pi_t = \\Delta_t d S_t + r\\beta_t dt\n\nGrouping the terms dependent on dt and d S_t separately we have:\\left(\\frac{\\partial C_t}{\\partial t} + \\frac{1}{2} \\sigma^2 S_t^2 \\frac{\\partial^2 C_t}{\\partial^2 S_t} +r\\beta\\right) dt  + \\left(\\frac{\\partial C_t}{\\partial S_t} - \\Delta_t\\right) dS_t = 0\n\nSince the equality must apply for any arbitrary dt and dS_t, each term in parenthesis must cancel separately. Starting with the right hand term we have:\\Delta_t = \\frac{\\partial C_t}{\\partial S_t}\n\nwhich is call the delta-hedging condition, given its obvious connection with linear hedging strategies where we try to neutralize the exposure of a financial portfolio to a risk factor like the stock price in this case. Delta hedging the portfolio we ensure that its value is independent on the random dynamics of the stock, at least during an infinitesimal time. However, a strategy that delta-hedges at every time the portfolio is not necessarily self-financing, i.e. we might need to add extra cash during the life of the portfolio. The problem is that a priori the cash required for delta-hedging would depend on the path of the stock, making it a random variable. In that case the premium would also be a random quantity with a certain distribution.\n\nIn order to have a deterministic premium the portfolio has to be self-financing, which requires that the first term of the equality above is also zero, namely:\\frac{\\partial C_t}{\\partial t} + \\frac{1}{2} \\sigma^2 S_t^2 \\frac{\\partial^2 C_t}{\\partial^2 S_t} + r S_t \\frac{\\partial C_t}{\\partial S_t} = rC_t\n\nwhere we have used the self-financing condition \\beta_t = \\Pi_t - \\Delta_t S_t = C_t - \\frac{\\partial C_t}{\\partial S_t} S_t. The resulting equation is the celebrated Black-Scholes-Merton (BSM) partial differential equation. Solving this equation with the terminal condition C_T = (S_T - K)^+ allows us to compute the value of the premium for any time t \\leq T deterministically. Therefore, by virtue of the replicating portfolio and non-arbitrage opportunity arguments, if the Black-Scholes-Merton conditions are satisfied the price of an option is no longer a random quantity as in the utility indifference framework.\n\nBefore discussing the solution to this equation, we can get another insight simply by inspecting it: the option premium does not depend on the drift \\mu of the stock, only the volatility. In the utility indifference framework, the estimation of the drift plays a big role in the price that the investor is willing to pay for the option, since it affects the probability of exercising or not the option. However, for a dealer pricing the option, as far as the BSM framework holds, the directionality of the market is irrelevant since the strategy guarantees a replication of the payoff in any scenario by investing the premium into the BSM dynamic portfolio and implementing the dynamic strategy.","type":"content","url":"/markdown/fair-price-estimation#the-black-scholes-merton-model","position":53},{"hierarchy":{"lvl1":"Fair value estimation","lvl4":"Solving the Black-Scholes-Merton equation","lvl3":"The arbitrage-free theory of derivatives pricing","lvl2":"Fundamental models for fair value estimation"},"type":"lvl4","url":"/markdown/fair-price-estimation#solving-the-black-scholes-merton-equation","position":54},{"hierarchy":{"lvl1":"Fair value estimation","lvl4":"Solving the Black-Scholes-Merton equation","lvl3":"The arbitrage-free theory of derivatives pricing","lvl2":"Fundamental models for fair value estimation"},"content":"There are different ways to solve the BSM equation. Most introductory textbooks on the topic (see for example \n\nJoshi, 2003, \n\nWilmott, 2007) follow the derivation used in the seminal paper that uses an ansatz for the solution that transforms the equation into the heat-equation, whose analytical solution is well-known \n\nEvans, 2010. Here, we will take a different approach and use the Feynman - Kac theorem introduced in Chapter (#stochastic_calculus), section (#feynman_kac). Recall that the Feynman - Kac theorem provides a general solution to a family of partial differential equations in term of an expected value. In the interest of the reader we review it again here: the solution to the PDE\\frac{\\partial u}{\\partial t} + \\mu(x,t) \\frac{\\partial u}{\\partial x} + \\frac{1}{2} \\sigma^2(x,t) \\frac{\\partial^2 u}{\\partial x^2} - r(x,t)u = 0\n\nwith the boundary condition u(x,T) = f(x), is the following expected valueu(x,t) = \\mathbb{E}\\left[ e^{-\\int_t^T r(X_s,s) ds} f(X_T) \\Big| X_t = x \\right]\n\nwhere X_t satisfies the general stochastic differential equation:d X_t = \\mu(X_t, t) dt + \\sigma(X_t, t) dW_t\n\nThe BSM equation is a specific case of this general PDE where X_t \\rightarrow S_t, u(x,t) \\rightarrow C(s, t), \\mu(x,t) \\rightarrow r, \\sigma(x,t) \\rightarrow \\sigma, r(x,t) \\rightarrow r. The solution of the BSM equation can be expressed as:C(s, t) = \\mathbb{E}\\left[ e^{-r(T-t)} (S_T - K)^+ \\Big| S_t = s \\right]\n\nwhere S_t satisfies the SDE:d S_t = r S_t dt + \\sigma S_t dW_t\n\nThe first crucial observation is that the solution is remarkably close to the one analyzed in the context of utility indifference pricing for a risk-neutral investor, with one caveat: the expected value is taken with respect to a SDE for the stock price that has the drift \\mu replaced by the risk-free interest r. In the former, the investor was taking the risk of the contract, but was indifferent to risk. In the latter, the dealer might be risk averse, but since the BSM dynamic hedging strategy neutralizes the risk (if the hypotheses are correct), there is no risk taken, only a deterministic cost to execute the hedging strategy, which is charged to the investor as the premium (plus potentially a margin for the service). In the jargon, the dealer prices the derivative as a risk-neutral investor if it computes probabilities in a different probability measure, where the drift is r. Such measure is usually called the risk-neutral measure. This framework for derivatives pricing can be generalized to other derivatives, allowing dealers to compute prices directly skipping the construction of the hedging portfolio and solving the PDE. It is appropriately referred as risk-neutral derivatives pricing.\n\nFrom a financial point of view, though, it is convenient not to lose the connection to the dynamic hedging strategy, since the validity of this theory is linked to the validity of the hypotheses done to derive the PDE. If we are interested in introducing more realistic dynamics into the pricing, like non-continuous hedging and transaction costs, we need to start again from the point of view of the replication portfolio and the hedging strategy. When those hypotheses are relaxed, though, we come back to a probability distribution to characterize the premium of the option instead of a deterministic one, the latter being one of the main remarkable results of the BSM theory.\n\nThe computation of the expectation value for the option premium can be reused from the one done in the context of utility indifference pricing, simply by substituting \\mu \\rightarrow r in the expressions:C(S_t,t)= S_t N(d_1(\\mu))-K e^{-r(T-t)} N(d_2(\\mu))\n\nwhere:d_1(\\mu) = \\frac{1}{\\sigma \\sqrt{T-t}} \\left(\\log \\frac{S_t}{K} + (r - \\frac{\\sigma^2}{2})(T-t)\\right)d_2(\\mu) = \\frac{1}{\\sigma \\sqrt{T-t}} \\left(\\log \\frac{S_t}{K} + (r + \\frac{\\sigma^2}{2})(T-t)\\right)\n\nThis is the infamous Black-Scholes-Merton option pricing formula. It provides dealers with option prices that depend on\n\nthe current stock price S_t\n\nthe expected volatility of the stock \\sigma\n\nthe risk-free interest rate r\n\nthe time to maturity T-t\n\nthe option strike K.\n\nIt does not depend on the stock drift \\mu, i.e. the expected trend of the market. This is because in the BSM framework, the dealer uses the dynamic hedging strategy to make the portfolio risk-free and therefore shielded from any market trend.\n\nAgain, we must emphasize that despite the similarity with the risk-neutral option premium computed in the utility indifference framework, in the BSM framework the dealer is not computing the premium based on real probability scenarios of stock prices. The connection between replicating strategies and expectation values provided by the Feynman - Kac theorem is a useful mathematical result that simplifies the computation of derivatives premia in the BSM framework. But they shall not be interpreted in the sense of real probabilities (or probabilities in the real probability measure, as they are also referred to).\n\nThe dependencies of the option formula for a European call option with respect to the parameters are similar to the ones seen in the context of utility indifference pricing, with the exception of the dependence with respect to the risk-free interest rate r, whose role in the formula goes now beyond cash-flow discounting. In the following picture we compare the premia derived with BSM versus utility indifference pricing:\n\n\n\nFigure 3:Option price premium calculated using the utility indifference pricing versus arbitrage-free theory. The results are shown for an european call option with parameters S_t=100, K =100, T-t = 1 in years, r = 0.05, \\mu = 0.1, \\sigma = 0.2.\n\nThe first main differences is, as commented, the dependence with respect to the risk-free interest: in BSM theory, the premium is more valuable as interest rates increase, wheres in utility indifference pricing it was the other way around. In the latter, interest rates enter the formula to discount the value of future cash-flows. As interest rates grow, the opportunity cost of investing the premium for the future cash-flows of the options increases, since a risk-free deposit generates comparatively a larger yield. In other words, for an investor, the option becomes less attractive and is willing to pay less for it. However, in BSM theory used by dealers to price options, the opposite happens: bigger interest rates make funding the hedging strategy more costly, therefore requiring a larger compensation in terms of premium to execute the replication strategy.\n\nThe second big difference is of course the dependence with respect to the expected market drift, since in the BSM theory the premium is independent of it. The resulting plot is interesting because it points out to the resolution of the question of why are options traded in practice, assuming both dealers and investors have the same view of the market. As mentioned before, if both were to value the option as an investment, there would be no agreement on the premium to pay, since they hold opposite sides in the trade. However, if the dealer uses BSM theory there is room for agreement, at least for those investors whose expectations on market drifts make the premium requested by the dealer attractive. In the picture, such situation corresponds to those expected drifts that make the premium that an investor is willing to pay larger than the one commanded by the dealer.\n\nFrom a classical Economics point of view, those frameworks fit well together to explain the derivatives market in terms of demand and supply. Demand for options is driven by investors looking to generate returns on investment, whereas supply comes from dealers that “fabricate” those options using replication strategies. The BSM premium is essentially the cost of “fabricating” the option, in analogy to the language used in the production of goods.","type":"content","url":"/markdown/fair-price-estimation#solving-the-black-scholes-merton-equation","position":55},{"hierarchy":{"lvl1":"Fair value estimation","lvl4":"An alternative derivation: the market price of risk","lvl3":"The arbitrage-free theory of derivatives pricing","lvl2":"Fundamental models for fair value estimation"},"type":"lvl4","url":"/markdown/fair-price-estimation#an-alternative-derivation-the-market-price-of-risk","position":56},{"hierarchy":{"lvl1":"Fair value estimation","lvl4":"An alternative derivation: the market price of risk","lvl3":"The arbitrage-free theory of derivatives pricing","lvl2":"Fundamental models for fair value estimation"},"content":"An alternative derivation of the BSM equation that can be helpful to gain intuition on the theory uses the financial concept of market price of risk. The market price of risk is essentially a Sharpe ratio, commonly used in the theory of investment. The Sharpe ratio computes the excess returns of an investment, i.e. the expected returns devoted fom the risk-free interest, over their risk defined as the volatility of the returns. For the stock that is the underlying of the option, and using continuos time, this is:\\lambda_{S} = \\frac{ \\mathbb{E}[\\frac{dS_t}{S_t}]- rdt}{\\sqrt{Var[\\frac{dS_t}{S_t}]}} = \\frac{\\mu_t - r}{\\sigma}\\sqrt{dt}\n\nWe can now use Ito’s formula to compute the market price of risk of the option:\\lambda_{C} = \\frac{ \\mathbb{E}[\\frac{dC}{C}]- rdt}{\\sqrt{Var[\\frac{dC}{C}]}} = \\frac{\\frac{\\partial C}{\\partial t}+ \\mu_t S_t\\frac{\\partial C}{\\partial S_t}+\\frac{1}{2}\\sigma^2 S_t^2 \\frac{\\partial^2 C}{\\partial S_t^2} - rC}{\\sigma S_t \\frac{\\partial C}{\\partial S_t}} \\sqrt{dt}\n\nWe can now apply a different version of the arbitrage-free theory. Since the value of the option is essentially derived from the underlying stock, which the only risk factor affecting the option price in the BSM theory, then as investment opportunities both should have the same Sharpe ratio or market price of risk, i.e. \\lambda_S = \\lambda_C. Otherwise, investors would bid up the price of the one with the largest Sharpe ratio until both of them equalize. Applying this equality the terms proportional to the drift \\mu_t cancel and we get back to the BSM differential equation.\n\nOne could of course have used the argument in reverse, reorganizing the BSM equation in terms of market prices of risk to prove that the equality of those is a consequence of the arbitrate-free argument used when building the replication portfolio.","type":"content","url":"/markdown/fair-price-estimation#an-alternative-derivation-the-market-price-of-risk","position":57},{"hierarchy":{"lvl1":"Fair value estimation","lvl4":"Using the BSM framework in practice","lvl3":"The arbitrage-free theory of derivatives pricing","lvl2":"Fundamental models for fair value estimation"},"type":"lvl4","url":"/markdown/fair-price-estimation#using-the-bsm-framework-in-practice","position":58},{"hierarchy":{"lvl1":"Fair value estimation","lvl4":"Using the BSM framework in practice","lvl3":"The arbitrage-free theory of derivatives pricing","lvl2":"Fundamental models for fair value estimation"},"content":"The Black - Scholes - Merton pricing theory supposed a change of paradigm for dealers creating liquidity in option markets. The theory allows for a consistent pricing of derivatives beyond options, providing not only a way to calculate the premium but a hedging strategy that neutralizes the risk of the derivative, or from a different angle, a recipe to synthesize those derivatives from liquid tradable instruments.\n\nHowever, the BSM theory is based on multiple hypothesis that are not necessarily realistic, so the dealer needs to take into account how relevant is for the pricing and hedging of derivatives that those hypothesis are not consistent with reality. For instance:\n\nThe BSM theory does not take into account liquidity and transaction costs of the hedging instruments, which will increase the costs of the replication strategy in a non-deterministic way. The premium becomes dependent on specific market microstructure details of how the instruments of the hedging portfolio are traded, for instance if they are traded in limit order book based markets, the fees of orders, the bid-ask spread, the tick size, etc. How the dealer executes those trades becomes also relevant, for instance, which types or orders are used, or if execution algorithms are to be used, in whose case the theory needs to incorporate an estimation of the cost of execution of those strategies, based on transaction cost analysis (TCA)\n\nThe BSM theory assumes continuous hedging, which is not feasible in practice, both because of the costs mentioned in the previous point, and because markets in financial instruments usually are not open 24 hours per day. Even when they do, liquidity tends to vary widely across trading hours. In practice, dealers tend to hedge less frequently (e.g. daily), deviating from the pure BSM paradigm\n\nThe BSM theory assumes a specific dynamics for the evolution of the underlyings, namely the Geometric Brownian Motion in the case of stocks (for underlyings that can become negative, like interest rates or inflation, a Brownian Motion is also typically used). Markets in practice follow more complicated dynamics, for instance they exhibit fatter tails in the distribution of returns or they might show sudden jumps, particularly in the overnight gap .\n\nThe original BSM theory assumes hedging strategies based on the underlying. The theory can be applied equally if we assume hedging with other derivatives as far as they share the same underlyings, or in a more fundamental level, the same risk factors driving the underlyings. In practice, dealers might use liquid derivatives to hedge non-liquid ones. For instance, european options with non-standard strikes or maturities with respect to liquid ones traded in exchanges. As an exercise for the reader, we propose to prove that the Black-Scholes-Merton differential equation can be derived using a hedging portfolio where instead of the underlying we trade another option with a different strike.\n\nIn general, these deviations from the assumptions make the premium no longer deterministic, since they introduce uncertainty in its estimation. Dealers typically will need to estimate how much do they need to increase the BSM premium to compensate for those risks. In the following plots we show the histogram of differences between the replication portfolio and the option payoff at maturity, when different assumptions of Black - Scholes - Merton theory are violated, namely:\n\nContinuous re-hedging, which can be violated by using increasingly smaller frequencies of re-hedging\n\nBSM volatility (the one used in the BSM pricing and hedging formulae) equals to market realized volatility\n\nLognormal stock dynamics, which can be challenged using a different dynamics like for instance the Heston model, which considers that volatility of the log-normal process is also stochastic, with its variance (volatility squared) following a CIR mean-reverting process:d S_t = \\mu S_t dt + \\sqrt{V_t} S_t dW_{1,t} \\\\\nd V_t = \\kappa (\\theta - V_t) + \\chi \\sqrt{V_t} dW_{2,t} \\\\\n \\mathbb{E}\\left[dW_{1,t}dW_{2,t}\\right] = \\rho dt\n\nZero transaction costs, which can be violated for instance by assuming the dealer pays the half bid-ask spread of the market every time the underlying is bought or sold when rebalancing the portfolio.\n\n\n\nFigure 4:Histograms showing the difference between the replication portfolio using the BSM dynamic hedging strategy, and the actual payoff of an European call option. Each plot shows the impact of violating a different hypothesis of the BSM theory. We run 10000 simulations for each case. We use the parameters S_t=100, K =100, T-t = 1 in years, r = 0.05, \\mu = 0.1, \\sigma = 0.2 for the baseline scenario where we expect close to perfect replication when re-hedging continuously. To test the effect of different realized volatilities we use use \\sigma = 0.19 and \\sigma = 0.21, respectively. To test the effect of transaction cost we add a constant bid-ask half-spread of 0.005%. To test the effect of a different market dynamics we use a Heston model with parameters \\kappa = 20 (mean reversion rate of the volatility squared), \\theta = 0.04 (long-term volatility squared mean), \\xi = 0.2 (volatility of volatility squared), \\rho = -0.7 (correlation between stock and stochastic volatility risk factors).\n\nIn the plots, we can see the different impacts that the violations produce on the distribution of the residuals. Whereas a less frequent re-hedging increases the dispersion of the residual, those are still unbiased and symmetrical. Other violations produce skewed distributions with non-zero mean. When introducing transaction costs, residuals have a negative mean, meaning that the BSM premium is insufficient to cover the actual costs of replication, as expected since the BSM derivation does not take into account such transaction costs. The effect of a different dynamics for the underlying is more nuanced: depending on the actual process, the distribution of residuals might have positive or negative mean, meaning that the BSM premium over-estimates or under-estimates, respectively, the costs of replication. For instance, if the realized volatility is lower than the one used for pricing and hedging in the BSM model, the mean of the residual is positive. This is, again, intuitive, since as we seen the premium of the option increases with volatility, so overestimating it means charging a larger premium than necessary for replication. Notice that this is not necessarily good for a dealer in competition, since if other dealers have a better estimation of market volatilities, they will be able to offer more competitive prices to clients and close more deals.","type":"content","url":"/markdown/fair-price-estimation#using-the-bsm-framework-in-practice","position":59},{"hierarchy":{"lvl1":"Fair value estimation","lvl3":"The stochastic discount factor (SDF) pricing framework","lvl2":"Fundamental models for fair value estimation"},"type":"lvl3","url":"/markdown/fair-price-estimation#the-stochastic-discount-factor-sdf-pricing-framework","position":60},{"hierarchy":{"lvl1":"Fair value estimation","lvl3":"The stochastic discount factor (SDF) pricing framework","lvl2":"Fundamental models for fair value estimation"},"content":"This pricing framework estipulates a fundamental pricing equation for any asset (and, in particular, financial instruments) with a similar form as the naive pricing equation for the present value pricing framework when applied to assets with uncertain future cash-flows:p_t = {\\mathbb E}_t \\left[\\sum_{i=1}^N m_{t_i} C_{t_i}\\right]\n\nThe difference being that now m_{t_i} is a stochastic discount factor that does not necessarily has the form derived using the argument based on the time value of money, namely e^{-rt_i}. That such pricing equation is general enough to price any asset can be derived based on two hypotheses:\n\nThe law of one price, which states that states that the price of an instrument delivering two cash-flows C_1 and C_2 -which may be uncertain and contingent on different future states- is equal to the sum of the prices of two assets delivering each cash-flow separately:p(X_{C_1 + C_2}) = p(X_{C_1}) + p(X_{C_2})\n\nwhere X denotes a generic instrument and p(\\cdot) its market price. This property reflects the absence of arbitrage opportunities: two instruments that generate identical payoffs in all states of the world must have the same price.\n\nThe market is complete, meaning that for every possible future state of the world there exists a tradable instrument whose payoff is contingent on that state. As we discussed previously,a complete market admits a set of Arrow–Debreu securities, each of which pays one unit of currency if and only if a specific state of the market s_i \\in S, and zero otherwise. The pay-offs of such instruments can be written as the indicator function 1_{s_i}. Under market completeness, any instrument X can be represented as a linear combination of Arrow–Debreu securitiesX = \\sum_{i} 1_{s_i} C_i\n\nwhere C_i denotes the pay-off of the instrument in state s_i. For example, an instrument with deterministic cash-flows at times t_i can be interpreted as having pay-offs contingent on time states s_i \\equiv t_i.\n\nWe can proceed now with the derivation of the pricing equation. If the law of one price holds, we can express the price of any generic instrument as:p(X) = \\sum_{i}  p(1_{s_i}) C_i\n\nIf now we multiply and divide by the probabilities of each state s_i, denoted \\pi_i:p(X) = \\sum_{i}  \\frac{p(1_{s_i})}{\\pi_i} \\pi_i C_i \\equiv {\\mathbb E}[m X]\n\nwhere we have defined the stochastic discount factor as m_i \\equiv \\frac{p(1_{s_i})}{\\pi_i}. Notice that in order to avoid having arbitrage opportunities, this stochastic discount factor has to be strictly positive, m_i > 0. To prove it, notice that in order to avoid arbitrages, any instrument with strictly positive cash-flows C_i > 0 has to have a positive price. Using our pricing equation:p(X) = \\sum_i m_i \\pi_i C_i > 0 \\rightarrow m_i > 0, \\forall i\n\nsince C_i is postive and \\pi_i is non-negative. Notice that condition is referred as the fundamental theorem of asset pricing \n\nCochrane, 2005.\n\nGiven the role that time has in structuring financial instruments cash-flows, it makes sense to include splicitly the time dimension into the pricing equation. Let us add a time dimension to the market states, s_{t,i} \\in S_t, so now we have a complete set of possible market states for each time t, which for the moment we consider them to belong to a discrete time grid. A generic instrument is expressed now as:X = \\sum_{t,i} 1_{s_{t,i}} C_{t,i}\n\nLet us focus for the moment on a single Arrow-Debreu security paying 1_{s_t, i}. If we simply extend our pricing function to add the explicit time component we would have p_{t_0}(1_{s_t, i}) = {\\mathbb E}_{t_0}[m_t 1_{s_t, i}]. However, let us consider an intermediate time t_0 < t' < t. According to this definition, pricing at time t' of the same instrument would be  p_{t'}(1_{s_t, i}) = {\\mathbb E}_{t'}[m_t 1_{s_t, i}]. If this is the case, nothing prevent us to define a financial instrument that simply pays p_{t'}(1_{s_t, i}) at time t`. Using our pricing formula, the price of this instrument at time t_0 is {\\mathbb E}_{t_0}[m_{t'} p_{t'}(1_{s_t, i})]. But this should be consistent with simply using the Tower law in our original pricing formula:p_{t_0}(1_{s_t, i}) = {\\mathbb E}_{t_0}\\left[{\\mathbb E}_{t'}[m_t 1_{s_t, i}]\\right] =  {\\mathbb E}_{t_0}\\left[p_{t'}(1_{s_t, i})\\right] \\neq {\\mathbb E}_{t_0}\\left[m_{t'} p_{t'}(1_{s_t, i})\\right]\n\nThe issue can be overcome by deflating our pricing formula by the discount factor at the pricing time, so we have:p_{t_0}(1_{s_t, i}) = {\\mathbb E}_{t_0}\\left[\\frac{m_t}{m_{t_0}} 1_{s_t, i}\\right]\n\nUsing this corrected formula:p_{t_0}(1_{s_t, i}) = {\\mathbb E}_{t_0}\\left[{\\mathbb E}_{t'}\\left[\\frac{m_t}{m_{t_0}}  1_{s_t, i}\\right]\\right] =  {\\mathbb E}_{t_0}\\left[\\frac{m_{t'}}{m_{t_0}} p_{t'}(1_{s_t, i})\\right]\n\nwhich is now consistent. The intuition behind this adjustment is that the stochastic discont factor implicitly defines the numeraire of the economy - that is, a reference asset used as a unit of account, that ensures that prices are consistent across time. Deflating by the discount factor at the pricing date ensures that all values are measured in the same unit of account, so that prices observed at different times can be consistently compared and aggregated.\n\nWe can now extend the pricing formula to our generic instrument X:p_{t_0}(X) =  {\\mathbb E}_{t_0}\\left[\\sum_{t > t_0}\\frac{m_t}{m_{t_0}} C_{t}\\right]\n\nNotice that this equation implies a recursive pricing equation:p_{t_0}(X) = {\\mathbb E}_{t_0}\\left[\\frac{m_{t_0+1}}{m_{t_0}}\\left(p_{t_0+1}(X') + C_{t_0+1}\\right)\\right]\n\nWhen using the temporal representation, it is mathematically convenient to derive a continuous-time representation of the pricing formula. For that we introduce a time step \\Delta t so that t_k = t_0 + k \\Delta t, k = 1, ..., N and a cash-flow rate C_{t_k} = c_{t_k} \\Delta t. Our pricing equation becomes:m_{t_0} p_{t_0}(X) =  {\\mathbb E}_{t_0}\\left[\\sum_{k = 1}^N m_{t_k} c_{t_k} \\Delta t\\right]\n\nNow we take the continuous limit \\Delta t \\rightarrow 0 so we get:m_{t_0} p_{t_0}(X) =  {\\mathbb E}_{t_0}\\left[\\int_{t_0}^T m_{t} c_{t} dt\\right]\n\nwhere T \\equiv t_0 + N \\Delta t. The one-period recursive equation now becomes:0 = m_t C_t + {\\mathbb E}_{t}[d(m_t p_t)]\n\nwhich in the absence of cash-flows between t and t+dt becomes simply:0 = {\\mathbb E}_{t}[d(m_t p_t)]\n\nor, equivalently:0 = m_t {\\mathbb E}_{t}[d p_t] + p_t {\\mathbb E}_{t}[d m_t] + {\\mathbb E}_{t}[d m_t d p_t]","type":"content","url":"/markdown/fair-price-estimation#the-stochastic-discount-factor-sdf-pricing-framework","position":61},{"hierarchy":{"lvl1":"Fair value estimation","lvl4":"Practical applications of the SDF pricing framework","lvl3":"The stochastic discount factor (SDF) pricing framework","lvl2":"Fundamental models for fair value estimation"},"type":"lvl4","url":"/markdown/fair-price-estimation#practical-applications-of-the-sdf-pricing-framework","position":62},{"hierarchy":{"lvl1":"Fair value estimation","lvl4":"Practical applications of the SDF pricing framework","lvl3":"The stochastic discount factor (SDF) pricing framework","lvl2":"Fundamental models for fair value estimation"},"content":"As we have discussed extensively in this chapter, we need fundamental pricing frameworks when we don’t have access to liquid market prices of financial instruments, otherwise we can just extract the fair values using filtering techniques. If we have complete markets, the general idea is to compute the stochastic discount factor using available prices of instruments, and then use those prices to price illiquid instruments which share the same risk factors as those of tradable instruments.\n\nNotice that if the market is not complete, we can still use this framework to find a projection of the discount factor on the subspace of instruments with available market prices. This discount factor can be used to find a consistent price of instruments that have some risk factors out of this subspace, by decomposing the general discount factor in:\n\nthe projection in the subspace\n\nan orthogonal component\n\nThis will provide us with a price that has the minimum uncertainty given the available prices.\n\nIn complete markets, we have the guarantee that a stochastic discount factor exists. Let us compute it for some representative financial instruments.","type":"content","url":"/markdown/fair-price-estimation#practical-applications-of-the-sdf-pricing-framework","position":63},{"hierarchy":{"lvl1":"Fair value estimation","lvl5":"Bond pricing","lvl4":"Practical applications of the SDF pricing framework","lvl3":"The stochastic discount factor (SDF) pricing framework","lvl2":"Fundamental models for fair value estimation"},"type":"lvl5","url":"/markdown/fair-price-estimation#bond-pricing","position":64},{"hierarchy":{"lvl1":"Fair value estimation","lvl5":"Bond pricing","lvl4":"Practical applications of the SDF pricing framework","lvl3":"The stochastic discount factor (SDF) pricing framework","lvl2":"Fundamental models for fair value estimation"},"content":"We consider a standard bond paying a fixed coupon rate c at periodic times t_i = 1, ..., N. The day-count fraction \\gamma_i is the annualized fraction of days between coupon payments. The bond is referred to a notional M, so the coupon cash-flows are C_i = \\gamma_i c M and the principal paid at maturity T = t_N is M.\n\nThe payoff is therefore:X = \\sum_{i=1}^{N} \\gamma_i c M 1_{t_i} + M 1_T\n\nThe price at time t is therefore given by:B_t = {\\mathbb E}_t\\left[ \\sum_{i=1}^{N} \\frac{m_{t_i}}{m_t}\\gamma_i c M 1_{t_i} + \\frac{m_{T}}{m_t} M 1_T \\right] =  \\sum_{i=1}^{N} {\\mathbb E}_t\\left[ \\frac{m_{t_i}}{m_t}\\right] \\gamma_i c M + {\\mathbb E}_t\\left[\\frac{m_{T}}{m_t}\\right] M\n\nWe define the discount factor as D(t, t_i) \\equiv {\\mathbb E}_t\\left[ \\frac{m_{t_i}}{m_t}\\right], so we have:B_t = \\sum_{i=1}^{N} D(t, t_i) \\gamma_i c M + D(t, T) M\n\nHow to proceed from here depends on our modelling choices regarding the risk factors that are relevant for pricing as well as the set of liquid instruments with available prices. For example, if we have a set of N bonds from the same issuer paying coupons at the same dates t_i but with different maturities, we could simply write the N pricing equations and solve for the discount factors D(t, t_i), without having to compute explicitely the SDF. This could be used to price non-standard bonds (e.g. with different deterministic coupons or day-count fraction conventions) as far as they pay on the same time grid. If they pay at different times, we need to make some theoretical hypothesis to be able to interporlate the value of the discount factors, or directly build a model of the SDF.\n\nA first simple model is assuming that bonds only depend on a single risk factor, an overall macroeconomic interest rate r_t, for example a short-term interbank rate  (e.g. one linked to collateralized contracts like overnight index swaps, see chapter {ref}`intro_financial_instruments).  For the moment, we consider it deterministic and constant: r_t = r. Let us assume in this market we have access to a money-market account that accrues interest continuously. The pay-off at time T of the money market account is \\beta_T = \\beta_t e^{r(T-t)}, for a initial investment \\beta_t, which is also naturally the price of this instrument at time t. Therefore, the pricing equation is given by:m_t \\beta_t=  {\\mathbb E}_t\\left[ m_T \\beta_T \\right] = m_T \\beta_T\n\nwhere, in the second step, we have applied that interest rates are deterministic and also the only risk factor in our model, so the SDF becomes deterministic as well. Therefore, the SDF is given by:m_T = m_t e^{-r(T-t)}\n\nwhose dynamics is: dm_t = - r m_t dt. We can then simply compute the discount factors at any arbitrary time as D(t, t_i) = e^{-r(t_i -t)}, and the price of a standard bond simplifies to:B_t = \\sum_{i=1}^{N} e^{-r(t_i-t)} \\gamma_i c M + e^{-r(T-t)} M\n\nNotice that we have recovered the pricing equation derived in the present value pricing framework. As already anticipated, the SDF framework is general enough to incorporate this pricing framework, which corresponds to the case of a simple market with only one tradable instrument, the money-market account, and the hypothesis that interest rates are deterministic and constant. It is actually not difficult to generalize this result to time-dependent deterministic interest rates r_t. Using dm_t = -r_t m_t dt, we have m_T = m_t e^{-\\int_t^T r_t dt}, i.e. D(t, t_i) =  e^{-\\int_t^{t_i} r_t dt}.\n\nIn practice, though, it is too simplistic to consider that the price of bonds, even those issued by governments with sound finances, does not have an idiosyncratic country risk factor. This can be seen empirically, since the price of traded bonds don’t usually matches the discounting of their future cash-flows using interbank rates. The standard practice is to introduce their own interest rate risk factors, defined by the so-called yield curve y(t, T_k), which by definition is the interest rate that matches market prices of standard bonds with maturities T_k:B_{k,t}^{mkt} = \\sum_{i=1}^{N} e^{-y(t, T_k)(t_i-t)} \\gamma_i c_k M_k + e^{-y(t, T_k)(T_k-t)} M_k\n\nAgain, in order to extend this pricing framework to other instruments with non-liquid prices, we need to be able to interporlate the yield curve to other maturities. Market practitioners might directly use interpolation schemes that ensure the yield curve is well behaved, e.g. does not produce prices that are arbitragable. There is also a large literatur on term-structure interest rate models from which consistent yield curve parametric functions can be derived, that are then fitted to market prices. We refer the reader to \n\nBrigo & Mercurio, 2006 \n\nAndersen & Piterbarg, 2010 \n\nAndersen & Piterbarg, 2010 for more details.\n\nFor the purpose of our discussion on how to build stochastic discount factor models, let us consider one of the most simple instances of such term-structure models, the Vasicek model \n\nVasicek, 1977. This model assumes that the entire yield curve is driven by a single risk factor, represented by an instantaneous continuously compounded short rate r_t that drives the movements of the full yield curve y(t, T). The short rate r_t is modeled as a stochastic process following an Ornstein–Uhlenbeck mean-reverting dynamics, as discussed in \n\nStochastic Calculus:dr_t = \\kappa (\\theta - r_t) + \\sigma dW_t\n\nwhere \\kappa > 0 is the speed of mean reversion, \\theta the long run mean level, \\sigma > 0  the volatility and W_t a Wiener process. Notice that this short-rate is not anymore a interbank reference rate, but a funding rate linked to the issuer. As mentioned above, an alternative model could try to keep an explicit decomposition as r_t = r_t^{ois} + s_t, where now r_t^{ois} is the interbank rate and s_t the spread associated with the specific issuer, linked to specific funding, credit and liquidity characteristics of the issuer. But we will not follow this path in this section.\n\nIn order to find the SDF, we still assume there is a money-market account \\beta_t now linked to the funding short-rate r_t of the issuer. Additionally, we define so-called zero-coupon bonds (ZCBs) that pay only a principal of 1 at maturity T$, whose prices are directly the discount factors, since:P_t(1_{T}) = {\\mathbb E}_t \\left[\\frac{m_T}{m_t} \\right] = D(t, T)\n\nWe propose the simplest ansatz for the SDF that preserves non-arbitrability, i.e. as we discussed in chapter \n\nStochastic Calculus, a log-normal process whose stochastic differential equation is given by:\\frac{d m_t}{m_t} = \\mu_t(r_t) dt + \\lambda_t(r_t) dW_t\n\nwhere \\mu and \\lambda are, for the moment, generic functions of time and the short-rate.\n\nOur SDF has to price all the instruments in our market: the money-market account and the zero-coupon bonds. We first apply the continuous-time version of our pricing equation to the money-market equation, namely:{\\mathbb E}[d(m_t \\beta_t)] = 0\n\nwhere, recall, d\\beta_t = r_t \\beta dt. Applying Ito’s Lemma:d(m_t \\beta_t) = dm_t \\beta_t + m_t d\\beta_t + dm_t d\\beta_t = \\beta_t m_t (\\mu_t(r_t)  + r_t) dt + \\beta_t  m_t  \\lambda_t(r_t) dW_t\n\nApplying the pricing equation, we get a condition on the SDF:{\\mathbb E}[d(m_t \\beta_t)] =  \\beta_t m_t (\\mu_t(r_t)  - r_t) dt = 0 \\rightarrow \\mu_t(r_t) = - r_t\n\nLet us apply it now to the ZCBs. We make the ansatz D(t, T) = f(t, r_t) given that the SDF itself is Markovian on r_t. Applying Ito’s lemma to this expression, we get:d f(t, r_t) = \\frac{\\partial f}{\\partial t} dt + \\frac{\\partial f}{\\partial r_t} dr_t + \\frac{1}{2}  \\frac{\\partial^2 f}{\\partial^2 r_t} \\sigma^2 dt\n\nwhere we have used the SDE for r_t given by the Vasicek model. Now we apply Ito’s on d(m_t f(t, r_t)):d (m_t f(t, r_t)) = m_t \\left(\\frac{\\partial f}{\\partial t} + \\kappa (\\theta - r_t) \\frac{\\partial f}{\\partial r_t} +  \\frac{1}{2}   \\sigma^2 \\frac{\\partial^2 f}{\\partial^2 r_t}\\right) + m_t \\sigma  \\frac{\\partial f}{\\partial r_t} dW_t + f m_t (-r dt + \\lambda_t dW_t) + \\lambda_t m_t \\sigma  \\frac{\\partial f}{\\partial r_t} dt\n\nWe use now the pricing equation:{\\mathbb E}[d(m_t D(t, T))] = 0\n\nto get the following partial differential equation for f(t, r_t):\\frac{\\partial f}{\\partial t} + \\kappa (\\theta - r_t) \\frac{\\partial f}{\\partial r_t} +  \\frac{1}{2}   \\sigma^2 \\frac{\\partial^2 f}{\\partial^2 r_t} - f r  + \\lambda_t \\sigma  \\frac{\\partial f}{\\partial r_t} = 0\n\nwith terminal condition f(T, r_T) = 1. To solve this equation we make a farther simplification and consider that \\lambda_t(r_t) is affine in r_t, meaning it is a linear function:\\lambda_t(r_t) = \\lambda_0 + \\lambda_1 r_t\n\nIn this case, the exponential ansatz f(t, r_t) = A(t, T) e^{-B(t, T)r_t} transform the problem into the following two PDEs:\\dot{B}(t, T) = 1 - (\\kappa + \\lambda_1 \\sigma) B(t, T)\\dot{A}(t, T) = A(t, T)[\\kappa \\theta B(t, T) - \\frac{1}{2}\\sigma^2 B(t, T)^2 - \\lambda_0 \\sigma B(t, T)]\n\nwith terminal conditions B(T, T) = 0 and A(T, T) = 1. The solution reads:B(t, T) = \\frac{1- e^{-(\\kappa + \\sigma \\lambda_1)(T-t)}}{\\kappa + \\sigma \\lambda_1}A(t, T) = \\left(\\frac{\\kappa \\theta - \\sigma \\lambda_0}{\\kappa + \\sigma \\lambda_1} - \\frac{\\sigma^2}{2 (\\kappa + \\sigma \\lambda_1)}\\right)(B(t, T) - (T-t)) - \\frac{\\sigma^2}{4(\\kappa + \\sigma \\lambda_1)}B(t, T)^2\n\nwith this solution, now we can fit the parameters \\lambda_0 and \\lambda_1 to prices of zero coupon bonds that can be themselves be extracted from liquid bond prices. As expected, though, with two parameters we will be able to fit only approximately this term structure. In order to fit the prices of any set of liquid bonds from a given issuer, we need a model that allows for further flexibility. One such model is for example the Hull & White model \n\nHull & White, 1990.","type":"content","url":"/markdown/fair-price-estimation#bond-pricing","position":65},{"hierarchy":{"lvl1":"Fair value estimation","lvl4":"Stock pricing","lvl3":"The stochastic discount factor (SDF) pricing framework","lvl2":"Fundamental models for fair value estimation"},"type":"lvl4","url":"/markdown/fair-price-estimation#stock-pricing","position":66},{"hierarchy":{"lvl1":"Fair value estimation","lvl4":"Stock pricing","lvl3":"The stochastic discount factor (SDF) pricing framework","lvl2":"Fundamental models for fair value estimation"},"content":"","type":"content","url":"/markdown/fair-price-estimation#stock-pricing","position":67},{"hierarchy":{"lvl1":"Fair value estimation","lvl4":"Option pricing","lvl3":"The stochastic discount factor (SDF) pricing framework","lvl2":"Fundamental models for fair value estimation"},"type":"lvl4","url":"/markdown/fair-price-estimation#option-pricing","position":68},{"hierarchy":{"lvl1":"Fair value estimation","lvl4":"Option pricing","lvl3":"The stochastic discount factor (SDF) pricing framework","lvl2":"Fundamental models for fair value estimation"},"content":"simple deterministic cashflows (bond)\n\nstock?\n\noption","type":"content","url":"/markdown/fair-price-estimation#option-pricing","position":69},{"hierarchy":{"lvl1":"Fair value estimation","lvl4":"Connection to previous pricing frameworks","lvl3":"The stochastic discount factor (SDF) pricing framework","lvl2":"Fundamental models for fair value estimation"},"type":"lvl4","url":"/markdown/fair-price-estimation#connection-to-previous-pricing-frameworks","position":70},{"hierarchy":{"lvl1":"Fair value estimation","lvl4":"Connection to previous pricing frameworks","lvl3":"The stochastic discount factor (SDF) pricing framework","lvl2":"Fundamental models for fair value estimation"},"content":"deterministic discount factor\n\nutility functions as sdf\n\nradom nykodim as sdf and the risk neutral measure","type":"content","url":"/markdown/fair-price-estimation#connection-to-previous-pricing-frameworks","position":71},{"hierarchy":{"lvl1":"Fair value estimation","lvl2":"Exercises"},"type":"lvl2","url":"/markdown/fair-price-estimation#exercises","position":72},{"hierarchy":{"lvl1":"Fair value estimation","lvl2":"Exercises"},"content":"Derive the optimal linear combination of predictors in the senses that minimizes the variance of the combined predictor, for the case in which the individual predictors are correlated.\n\nDerive the Black-Scholes-Merton differential equation by using the portfolio replication argument for a dealer that hedges the risk of an european option (call or put) with strike K_1 and maturity T, using another (liquid) option tradable in the market with strike K_2 and same maturity T. As in the original BSM derivation, the dealer uses a cash account to remunerate cash positions or borrow cash. Formally, the replication or hedging portfolio is \\Pi_t = \\Delta_t C_2(S_t, t) + \\beta_t, with the terminal condition \\Pi_T = C_1(S_T, T). Hint: link the result with the market price of risk for options introduced in this chapter.\n\nWe consider non-dividend paying stocks for simplicity, the extension of the theory to dividend paying stocks is relatively straightforward\n\nA well-known historical short-coming of the BSM framework is the implication that european options on the same underlying with different strikes and maturities should have the same implied volatility, equal to the expected volatility of the stock. The implied volatility is the one obtained by inverting the BSM formula given prices observed in the market, assuming for instance that there is a set of standard options that are traded in an exchange. In the first years of application of the BSM theory to price options, around the 1980s, this had the consequence of having very small premiums for options, particularly put options, with strikes very deep out-the-money (i.e. far from the underlying price at the time of quoting). This was a consequence of a Gaussian assumption on price returns, that predicted a very low probability of such options being exercised. In 1989, such prediction was contradicted when the market dramatically crashed, forcing dealers to readjust the prices with respect to the BSM formula. Multiple models such as local or stochastic volatility models, or models with jumps in the dynamics, have been proposed later to address these issues. One point to bear in mind is that the logic applied in the BSM framework can be still applied when introducing these more complex dynamics, and deterministic premiums can be derived as far as we add extra instruments in the hedging portfolio that allows the dealer to neutralize those risks (stochastic volatility, jumps, etc)","type":"content","url":"/markdown/fair-price-estimation#exercises","position":73},{"hierarchy":{"lvl1":"Generative Artificial Intelligence"},"type":"lvl1","url":"/markdown/generative-ai","position":0},{"hierarchy":{"lvl1":"Generative Artificial Intelligence"},"content":"","type":"content","url":"/markdown/generative-ai","position":1},{"hierarchy":{"lvl1":"Generative Artificial Intelligence","lvl2":"What is Gen AI?"},"type":"lvl2","url":"/markdown/generative-ai#what-is-gen-ai","position":2},{"hierarchy":{"lvl1":"Generative Artificial Intelligence","lvl2":"What is Gen AI?"},"content":"In simple terms, Generative AI consists of training sophisticated generative probabilistic models on large corpora of data and applying them to tasks traditionally associated with intelligence—such as content generation and reasoning—thereby constituting Artificial Intelligence systems. Let us understand each of these concepts to shed light on the term.","type":"content","url":"/markdown/generative-ai#what-is-gen-ai","position":3},{"hierarchy":{"lvl1":"Generative Artificial Intelligence","lvl3":"Artificial Intelligence","lvl2":"What is Gen AI?"},"type":"lvl3","url":"/markdown/generative-ai#artificial-intelligence","position":4},{"hierarchy":{"lvl1":"Generative Artificial Intelligence","lvl3":"Artificial Intelligence","lvl2":"What is Gen AI?"},"content":"Considered the father of the field of Artificial Intelligence, the computer scientist John McCarthy coined the term and provided a simple but compelling definition: Artificial Intelligence is “the Science and Engineering of Making Intelligent Machines”. What what does it mean intelligence? In general, here we are referring to human capabilities for learning, reasoning, interpreting, understanding and adapting.\n\nIt is key here to focus on the idea of “human intelligence”. Human beings are the benchmark for Artificial Intelligence, as the famous test proposed by Alan Turing in 1949, the imitation game, clearly showcases. Here, a human evaluator tries to differentiate the human and the machine in a text transcript of a natural language conversation between them. The machine passes the test if the evaluator cannot tell them apart with certain confidence. There are naturally many domains where human intelligence is quite limited, for instance when working with raw datasets trying to generate statistical inferences. A Turing test where one of the participants is able to extract patterns from a billion lines dataset will provide a clear hit to the evaluator on who is the machine. And despite this relatively obvious fact, during the last decades the field of Machine Learning has been almost synonymous of Artificial Intelligence.\n\nThis does not mean that Machine Learning has not been able to excel at some tasks that can be clearly labelled as human intelligence. The most impressive ones were propelled, though, by the use of artificial neural networks, which from the second decade of this century, thanks to improved algorithms for learning without over-fitting, and the availability of data and computing power, all blended together in the field of Deep Learning, started to pass or even beat human beings in specific tasks like object recognition in images, or playing games board games like Go. However, most of the applications of Machine Learning, at least in the business environment, have been on improving upon traditional statistical models, which are precisely good at learning patterns from large datasets. In these tasks, these systems can easily show super-human capabilities.\n\nDespite the advances of Deep Learning in some human intelligence tasks, these systems where considered too specialized in their capabilities. Yes, they could excel humans in recognizing hand-written characters, but such system trained to do so would be later incapable of generalize the knowledge to even similar tasks like object recognition in images. In that sense, we can talk about narrow intelligence, limited to a specific task, and general intelligence, which can extrapolate knowledge acquired to perform a set of tasks to others for which it has not been specifically trained. The problem is that such capability of extrapolation is seemingly key even when Artificial Intelligence systems are to be applied in specific domains of knowledge. That was one of the learnings after the rise and fall of so-called expert AI systems in 80s of the previous century. These systems were based on a large corpus of rules compiled from experts in a specific field, the idea being that these systems could be used to reason about new problems in those domains. But they ended up becoming too complex to maintain and lacking those extrapolation capabilities that are key for the necessary bit of creativity needed to tackle new problems.\n\nIn some sense, the philosophical theory of knowledge was already pointing out the problem since the Ancient Greece: to learn is to generate some abstractions or ideas in our minds, that extract regularities from what our senses perceive. Reasoning, then, is the combination of those abstractions to build further inferences or perform deductions. Neurologists now understand that human brains generate such abstractions by building or strengthening connections between neurons in our brains when exposed to regularities in perceptions. It does not come then as a surprise that the most successful machine models were built upon neural networks, that resemble at a high level the workings of the brain. They seemingly build their own abstractions as they are trained on a large number of data points, and have a sufficient scale to learn complex patterns coupled with mechanism for regularization, i.e. avoid to learn so well the patterns of the training data that will later not generalize to unseen datasets.\n\nDuring the second decade of this century, the combination of neural networks increasingly large trained on increasing large datasets started to provide surprising examples of a sort of emergent behaviour in terms of capabilities: a sort of threshold in data and parameters from which capabilities would become akin or superior to humans. Such observations however did not anticipate that such neural network systems, when trained upon language datasets with the seemingly simple task of predicting the next work given the previous ones, would suddenly become shockingly good at probably the most central skills considered in the domain of human intelligence: general language interpretation and reasoning.\n\nIt was not, though, without effort. The field of natural language processing (NLP) had been trying during years to build systems capable of generating realistic conversations by training statistical models to sequences of words. One of the main challenges was that those systems would quickly forget the context of the conversation, making sentences incoherent in their syntax and conversations that would jump randomly across topics. With the boom of Deep Learning, specific neural network architectures were built to try to address this shortcoming, such as recurrent neural networks and long short term memory (LSTM) cells. They improved upon previous systems, but did not provide major breakthroughs. The tipping point happened with the release of the paper “Attention is All You Need”, by researchers from Google, in 2017. They introduced the Transformer architecture, which could have been easily seen as yet another proposal to try to fix the memory issue of sequential neural networks. However, conveniently fine-tuned, when applied to a vast corpus of text to learn a massive amount of neural weights, researchers witnessed an emerging behaviour in reasoning and understanding that still surprises any casual user of them.\n\nWith the advent of such Large Language Models (LMMs), the field of Artificial Intelligence has come back to its roots of building systems that target human capabilities.  Naturally, this has raised the interest of corporations across the world that have a lot of interest in gaining understanding of these models to automate and optimize tasks that few years back were thought exclusively to belong to the human domain.","type":"content","url":"/markdown/generative-ai#artificial-intelligence","position":5},{"hierarchy":{"lvl1":"Generative Artificial Intelligence","lvl3":"Generative models","lvl2":"What is Gen AI?"},"type":"lvl3","url":"/markdown/generative-ai#generative-models","position":6},{"hierarchy":{"lvl1":"Generative Artificial Intelligence","lvl3":"Generative models","lvl2":"What is Gen AI?"},"content":"In the chapter on Bayesian theory, we already introduced the concept of generative probabilistic models in contrast to discriminative probabilistic models. At the heart is the way we try to use probabilistic models to perform inferences from datasets. Discriminative models focus on understanding the distribution of a subset of variables conditional to the other, i.e. the domain of so-called Supervised Learning using the Machine Learning terminology. In simple terms, if we have a dataset composed of two variables X, Y, a discriminative model seeks to understand the conditional distribution:P(Y|X)\n\nA generative model, however, tries to model the full dataset, akin to understanding the full data generation process, hence the name. It models the joint probability distribution:P(X,Y)\n\nA generative model is more general that a discriminative model, since correct modelling of the joint distribution allows us to derive the distribution of the discriminative model, by virtue of use of the product rule of probability:P(Y|X) = \\frac{P(X,Y)}{P(Y)}\n\nA point to notice is that were we only interested in computing P(Y|X), it might be more efficient to learn this directly distribution using any of the multiple available statistical or Machine Learning supervised models. Modelling the joint distribution is more complex and therefore the quality of the inferences of P(Y|X) might suffer. However, having the full generative model allows us to solve a wider range of inferences than the discriminative model.\n\nThe previous argument can also work in reverse: if we learn all the relevant discriminative distributions separately, we have knowledge of the generative model, since:P(X, Y) = P(Y|X) P(X)\n\nWe can generalize this result to a sequence of N variables:P(X_1, ..., X_N) = P(X_1| X_2, .., X_N) P(X_2| X_3, ..., X_N) ... P(X_{N-1}|X_N) P(X_N)\n\nwhere we have simply applied the product rule sequentially.\n\nThis structure already provides a hint on the connection between generative models and Gen AI models, particularly Large Language Models, mentioned in the previous section. Statistical language models try to compute the distribution of words (or tokens, which are more granular building blocks to decompose language that perform better in practical tasks). For instance, given the sentence “the cat had blue ...”, such models try to estimate the probability of any existing next word, conditional to the previous words, for instance:P(\\text{\"eyes\"}| \\text{\"the\"}, \\text{\"cat\"}, \\text{\"had\"}, \\text{\"blue\"})\n\nIn this case, a well estimated model needs to be able to compute such probability for any word in the English vocabulary. Of course a useful model would not be specifically trained to compute probabilities for this case. This means that it should be able to compute the probability of any other sequence, in particular P(\\text{\"blue\"}|\\text{\"the\"}, \\text{\"cat\"}, \\text{\"had\"}), P(\\text{\"had\"}|\\text{\"the\"}, \\text{\"cat\"}), P(\\text{\"cat\"}| \\text{\"the\"}) and P(\\text{\"the\"}). But using the previous equation this means that this model must be able to compute:P(\\text{\"the\"}, \\text{\"cat\"}, \\text{\"had\"}, \\text{\"blue\"}, \\text{\"eyes\"})\n\ni.e. is a generative model for language.\n\nHaving a generative model for human language opens up the possibility for multiple tasks. In particular, we can use it to generate language given some context of prompt. We can have an expert model that sticks to most likely facts by generating sequences taking the most likely word according to the model. Or we can build creative systems that sample the distribution according to the computed probabilities. A common practice to control the degree of creativity of these models is to transform the probability distribution for the next word or token into a Gibbs distribution. The motivation is the transformation:P_i = e^{\\log P_i} \\rightarrow \\hat{P}_i = \\frac{e^{-s_i /T}}{Z}\n\nwhere we have defined the scores s_i = -\\log P_i per word i in the vocabulary, T is an external parameter called the Temperature in analogy to Statistical Physics, where the Gibbs distribution was introduced, and Z = \\sum_i e^{-s_i /T} is a normalization constant, also called the partition function using the terminology of Statistical Physics. As we discussed in the chapter on Bayesian probability, the Gibbs distribution is the one that maximizes entropy when the average score is a constraint. By varying the temperature, we can control the degree of creativity:\n\nFor T\\rightarrow 0, all terms tend to zero, however the one with the smallest score s_i is the slowest. Therefore, we get \\hat{P_i} \\rightarrow \\delta_{i, i_{max}}, where i_{max} is the word with the lowest score, which is the largest probability since s_i = -\\log P_i. In this case the model is less creative, sticking to the most likely next words\n\nFor T = 1, we recover the original distribution of probabilities \\hat{p}_i = p_i\n\nFor T \\rightarrow \\infty the exponential terms tend to 1, so the distribution is uniform over all words once normalization is accounted. In this case, the model is random over the distribution of all possible words.","type":"content","url":"/markdown/generative-ai#generative-models","position":7},{"hierarchy":{"lvl1":"Generative Artificial Intelligence","lvl3":"Gen AI models","lvl2":"What is Gen AI?"},"type":"lvl3","url":"/markdown/generative-ai#gen-ai-models","position":8},{"hierarchy":{"lvl1":"Generative Artificial Intelligence","lvl3":"Gen AI models","lvl2":"What is Gen AI?"},"content":"","type":"content","url":"/markdown/generative-ai#gen-ai-models","position":9},{"hierarchy":{"lvl1":"Generative Artificial Intelligence","lvl2":"Large Language Models"},"type":"lvl2","url":"/markdown/generative-ai#large-language-models","position":10},{"hierarchy":{"lvl1":"Generative Artificial Intelligence","lvl2":"Large Language Models"},"content":"","type":"content","url":"/markdown/generative-ai#large-language-models","position":11},{"hierarchy":{"lvl1":"Generative Artificial Intelligence","lvl3":"Traditional language modelling","lvl2":"Large Language Models"},"type":"lvl3","url":"/markdown/generative-ai#traditional-language-modelling","position":12},{"hierarchy":{"lvl1":"Generative Artificial Intelligence","lvl3":"Traditional language modelling","lvl2":"Large Language Models"},"content":"N-grams, HMMs, first neural network architectures","type":"content","url":"/markdown/generative-ai#traditional-language-modelling","position":13},{"hierarchy":{"lvl1":"Generative Artificial Intelligence","lvl3":"The Transformer Architecture","lvl2":"Large Language Models"},"type":"lvl3","url":"/markdown/generative-ai#the-transformer-architecture","position":14},{"hierarchy":{"lvl1":"Generative Artificial Intelligence","lvl3":"The Transformer Architecture","lvl2":"Large Language Models"},"content":"","type":"content","url":"/markdown/generative-ai#the-transformer-architecture","position":15},{"hierarchy":{"lvl1":"Generative Artificial Intelligence","lvl3":"Decoder-only models","lvl2":"Large Language Models"},"type":"lvl3","url":"/markdown/generative-ai#decoder-only-models","position":16},{"hierarchy":{"lvl1":"Generative Artificial Intelligence","lvl3":"Decoder-only models","lvl2":"Large Language Models"},"content":"","type":"content","url":"/markdown/generative-ai#decoder-only-models","position":17},{"hierarchy":{"lvl1":"Generative Artificial Intelligence","lvl3":"Fine-tuning models with RLHL","lvl2":"Large Language Models"},"type":"lvl3","url":"/markdown/generative-ai#fine-tuning-models-with-rlhl","position":18},{"hierarchy":{"lvl1":"Generative Artificial Intelligence","lvl3":"Fine-tuning models with RLHL","lvl2":"Large Language Models"},"content":"","type":"content","url":"/markdown/generative-ai#fine-tuning-models-with-rlhl","position":19},{"hierarchy":{"lvl1":"Generative Artificial Intelligence","lvl3":"Reasoning models","lvl2":"Large Language Models"},"type":"lvl3","url":"/markdown/generative-ai#reasoning-models","position":20},{"hierarchy":{"lvl1":"Generative Artificial Intelligence","lvl3":"Reasoning models","lvl2":"Large Language Models"},"content":"","type":"content","url":"/markdown/generative-ai#reasoning-models","position":21},{"hierarchy":{"lvl1":"Generative Artificial Intelligence","lvl3":"Evaluation of Large Language Models","lvl2":"Large Language Models"},"type":"lvl3","url":"/markdown/generative-ai#evaluation-of-large-language-models","position":22},{"hierarchy":{"lvl1":"Generative Artificial Intelligence","lvl3":"Evaluation of Large Language Models","lvl2":"Large Language Models"},"content":"Perplexity score: Perplexity is a metric to evaluate probabilistic models. It is closely related to cross-entropy and for some specific distributions, likelihood. Its main advantage comes in terms of offering a more interpretable score. The idea is to quantify the degree in which an estimated probability distribution for the data is perplexed (i.e. surprised) when it sees new data supposedly coming from the same generative process. The perplexity of a model that assigns probability p(x_i) to a sample of a sample of test data D = \\{x_1, ..., x_N\\} which we assume is independent and identically distributed (iid):PP(D) = b^{-\\frac{1}{N}\\sum_{i=1}^N \\log_b p(x_i)}\n\nwhere b is the base for the logarithm, e.g. b = 2 or b = e being typical choices. We can see readily how is related to the likelihood L(D) by rewriting it as:PP(D) = \\left(\\Pi_i p(x_i) \\right)^{-1/N} = L(D)^{-1/N}\n\nBy inverting the likelihood and normalizing to the length of the sample data, we have a metric that 1) has a interpretation in terms of surprise, since data that has higher probability under the model is therefore less surprising, and 2) it can be better compared across different datasets with different lengths.\n\nPerplexity is also related to the cross-entropy between the proposed distribution and the empirical distribution of the test data, which is given by the empirical frequencies of the different outcomes x: \\hat{p}(x) = \\frac{n(x)}{N}, with n(x) the number of occurrences of outcome x in the test sample D. Its cross entropy is then defined as:\n\nH(\\hat{p}, p) = -\\sum_x \\hat{p}(x) \\log_b p(x) = -\\frac{1}{N} \\sum_i \\log_b p(x_i)\n\nThis relationship is useful if we express the cross entropy in terms of the Kullback - Leibler (KL) divergence:\n\nH(\\hat{p}, p) = -\\sum_x \\hat{p}(x) \\log_b \\hat{p}(x) + \\sum_x \\hat{p}(x) \\log_b \\frac {\\hat{p}(x)}{p(x)} = H(\\hat{p}) + D_{KL}(\\hat{p}, p)\n\nwhere H(\\hat{p}) is the entropy of the distribution \\hat{p}. Cross-entropy is minimized when p(x) = \\hat{p}(x), where D_{KL} = 0. Given that perplexity can be expressed in terms of cross-entropy as:\n\nPP(D) = b^{H(\\hat{p}, p)}\n\nwe see readily that perplexity is minimized when the empirical and the model probability distributions agree.\n\nThe range of values for perplexity is [1, \\infty), with PP(D) =1 for models that assign probability 1 to the dataset, meaning that from the point of view of the model, there is no uncertainty in the prediction of the dataset. On the other hand, models that assign very low probabilities to the dataset have an unbounded upper limit in their perplexity score. Perplexity has also a natural interpretation in terms of simple benchmark models, helping to provide a natural scale to interpret the quality of the models: for example, a random guessing over a set of K possible outcomes yields a perplexity:PP(D) = b^{-\\frac{1}{N}\\sum_{i=1}^N \\log_b \\frac{1}{K}} = K\n\nApplying perplexity to sequence models like those modelling natural language simply requires us to compute the conditional probabilities to each element of the sequence conditional to the previous elements, i.e. p(s_i|s_1, ..., s_{i-1}), which decouples the sequence into i.i.d elements:\n\np(s_1, ..., s_N) = \\Pi_i p(s_i|s_1, ..., s_{i-1})\n\nThe perplexity of this model is:PP(D) = b^{-\\frac{1}{N}\\sum_{i=1}^N \\log_b p(s_i|s_1, ..., s_{i-1})}\n\nIn the context of large language models, the perplexity score offers a simple interpretation of the quality of the model: if a score PP is obtained in the evaluation, it means that the model has a perplexity equivalent to a random guess among PP possible tokens. The smaller is this score compared to the actual size of the token vocabulary space of the model, the better is the quality of the model.","type":"content","url":"/markdown/generative-ai#evaluation-of-large-language-models","position":23},{"hierarchy":{"lvl1":"Generative Artificial Intelligence","lvl2":"Working with Large Language Models"},"type":"lvl2","url":"/markdown/generative-ai#working-with-large-language-models","position":24},{"hierarchy":{"lvl1":"Generative Artificial Intelligence","lvl2":"Working with Large Language Models"},"content":"","type":"content","url":"/markdown/generative-ai#working-with-large-language-models","position":25},{"hierarchy":{"lvl1":"Generative Artificial Intelligence","lvl3":"Prompt Engineering","lvl2":"Working with Large Language Models"},"type":"lvl3","url":"/markdown/generative-ai#prompt-engineering","position":26},{"hierarchy":{"lvl1":"Generative Artificial Intelligence","lvl3":"Prompt Engineering","lvl2":"Working with Large Language Models"},"content":"","type":"content","url":"/markdown/generative-ai#prompt-engineering","position":27},{"hierarchy":{"lvl1":"Generative Artificial Intelligence","lvl3":"Chain of Thought","lvl2":"Working with Large Language Models"},"type":"lvl3","url":"/markdown/generative-ai#chain-of-thought","position":28},{"hierarchy":{"lvl1":"Generative Artificial Intelligence","lvl3":"Chain of Thought","lvl2":"Working with Large Language Models"},"content":"","type":"content","url":"/markdown/generative-ai#chain-of-thought","position":29},{"hierarchy":{"lvl1":"Generative Artificial Intelligence","lvl3":"Retrieval - Augmented Generation (RAG)","lvl2":"Working with Large Language Models"},"type":"lvl3","url":"/markdown/generative-ai#retrieval-augmented-generation-rag","position":30},{"hierarchy":{"lvl1":"Generative Artificial Intelligence","lvl3":"Retrieval - Augmented Generation (RAG)","lvl2":"Working with Large Language Models"},"content":"","type":"content","url":"/markdown/generative-ai#retrieval-augmented-generation-rag","position":31},{"hierarchy":{"lvl1":"Generative Artificial Intelligence","lvl3":"Agentic systems","lvl2":"Working with Large Language Models"},"type":"lvl3","url":"/markdown/generative-ai#agentic-systems","position":32},{"hierarchy":{"lvl1":"Generative Artificial Intelligence","lvl3":"Agentic systems","lvl2":"Working with Large Language Models"},"content":"","type":"content","url":"/markdown/generative-ai#agentic-systems","position":33},{"hierarchy":{"lvl1":"Glossary"},"type":"lvl1","url":"/markdown/glossary","position":0},{"hierarchy":{"lvl1":"Glossary"},"content":"Term: Description","type":"content","url":"/markdown/glossary","position":1},{"hierarchy":{"lvl1":"Hedging strategies"},"type":"lvl1","url":"/markdown/hedging","position":0},{"hierarchy":{"lvl1":"Hedging strategies"},"content":"","type":"content","url":"/markdown/hedging","position":1},{"hierarchy":{"lvl1":"Hedging strategies","lvl2":"Introduction"},"type":"lvl2","url":"/markdown/hedging#introduction","position":2},{"hierarchy":{"lvl1":"Hedging strategies","lvl2":"Introduction"},"content":"","type":"content","url":"/markdown/hedging#introduction","position":3},{"hierarchy":{"lvl1":"Hedging strategies","lvl2":"Mininum Variance Hedging"},"type":"lvl2","url":"/markdown/hedging#mininum-variance-hedging","position":4},{"hierarchy":{"lvl1":"Hedging strategies","lvl2":"Mininum Variance Hedging"},"content":"","type":"content","url":"/markdown/hedging#mininum-variance-hedging","position":5},{"hierarchy":{"lvl1":"Hedging strategies","lvl2":"Factor models"},"type":"lvl2","url":"/markdown/hedging#factor-models","position":6},{"hierarchy":{"lvl1":"Hedging strategies","lvl2":"Factor models"},"content":"","type":"content","url":"/markdown/hedging#factor-models","position":7},{"hierarchy":{"lvl1":"Hedging strategies","lvl3":"Principal Component Analysis","lvl2":"Factor models"},"type":"lvl3","url":"/markdown/hedging#principal-component-analysis","position":8},{"hierarchy":{"lvl1":"Hedging strategies","lvl3":"Principal Component Analysis","lvl2":"Factor models"},"content":"","type":"content","url":"/markdown/hedging#principal-component-analysis","position":9},{"hierarchy":{"lvl1":"Hedging strategies","lvl2":"Hedging with transaction costs"},"type":"lvl2","url":"/markdown/hedging#hedging-with-transaction-costs","position":10},{"hierarchy":{"lvl1":"Hedging strategies","lvl2":"Hedging with transaction costs"},"content":"","type":"content","url":"/markdown/hedging#hedging-with-transaction-costs","position":11},{"hierarchy":{"lvl1":"Hedging strategies","lvl2":"Pre-hedging"},"type":"lvl2","url":"/markdown/hedging#pre-hedging","position":12},{"hierarchy":{"lvl1":"Hedging strategies","lvl2":"Pre-hedging"},"content":"","type":"content","url":"/markdown/hedging#pre-hedging","position":13},{"hierarchy":{"lvl1":"Hedging strategies","lvl2":"Skew vs hedge in the Avellaneda-Stoikov framework"},"type":"lvl2","url":"/markdown/hedging#skew-vs-hedge-in-the-avellaneda-stoikov-framework","position":14},{"hierarchy":{"lvl1":"Hedging strategies","lvl2":"Skew vs hedge in the Avellaneda-Stoikov framework"},"content":"","type":"content","url":"/markdown/hedging#skew-vs-hedge-in-the-avellaneda-stoikov-framework","position":15},{"hierarchy":{"lvl1":"Hedging strategies","lvl2":"Non linear hedges"},"type":"lvl2","url":"/markdown/hedging#non-linear-hedges","position":16},{"hierarchy":{"lvl1":"Hedging strategies","lvl2":"Non linear hedges"},"content":"","type":"content","url":"/markdown/hedging#non-linear-hedges","position":17},{"hierarchy":{"lvl1":"Hedging strategies","lvl2":"Exercises"},"type":"lvl2","url":"/markdown/hedging#exercises","position":18},{"hierarchy":{"lvl1":"Hedging strategies","lvl2":"Exercises"},"content":"","type":"content","url":"/markdown/hedging#exercises","position":19},{"hierarchy":{"lvl1":"Bayesian Modelling"},"type":"lvl1","url":"/markdown/intro-bayesian","position":0},{"hierarchy":{"lvl1":"Bayesian Modelling"},"content":"","type":"content","url":"/markdown/intro-bayesian","position":1},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl2":"Bayesian probability"},"type":"lvl2","url":"/markdown/intro-bayesian#bayesian-probability","position":2},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl2":"Bayesian probability"},"content":"The greek philosopher Aristotle formally introduced the field of Logic as the tool of deduction and therefore one of the the main tools of scientific progress.  Logic allows to derive conclusions based on propositions that are taken for granted. Those propositions might have been themselves derived in a logical way, or might be principles that cannot be derived themselves, the starting building blocks of any theory or discipline.\n\nA fundamental principle of logic is that of the “excluded third”, for which a given proposition can only be either true or false, but there is not a third option. Therefore, logic is based on the assumption of complete certainty about the propositions, both the premises and the conclusions. But most of our knowledge (if not all, as defended by empiricists like David Hume) is uncertain, therefore we need a tool that allows us to derive conclusions from uncertain premises. Under the so-called Bayesian school of probability, such a tool is probability theory, whose rules, that where inferred for the analysis of more specific toy cases like games of chance, we will see that turn out to apply to the wider case of reasoning under uncertainty.\n\nIn the Bayesian interpretation, a probability is a statement or our degree of belief in a proposition. For instance, the proposition “Apple stock will close tomorrow at a higher price than today” is, from today’s point of view, neither true or false. The application of logic is limited to situations where we have already observed the price of Apple tomorrow, and therefore we know if the proposition is true or false. However, we might be interested in making decisions today based on our confidence in the proposition without having to wait to know its trueness: we might trigger different trading strategies whose profits are linked to Apple stock depending on the degree of confidence on the proposition. Such way of reasoning happens in all spheres of our lives, where we are always looking ahead to un uncertain future: from minor decisions like which clothes to wear on a given day, to major ones like which professional career to follow.\n\nWhat probability theory offers is a tool to be able to derive consistently degrees of belief on certain outcomes based on the degrees of belief on the premises. In our previous example, with probability theory we could potentially compute the probability of profit from a certain trading strategy based on the probability of Apple trading higher tomorrow. Our brains are capable of doing such calculations intuitively in familiar situations, but such intuition breaks down quickly in unfamiliar domains, when probabilities are very small, or when there is a large number of assumptions or outcomes. Probability as an extension of logic can compute these probabilities in any situation.\n\nAs we mentioned above, the actual rules to operate consistently with probabilities were inferred initially in the analysis of specific problems like games of chance or combinatorial problems, as done by mathematicians like Bernoulli and de Moivre in the 18th century. In the same century, reverend Thomas Bayes would write about application of probability theory to more general problems of reasoning under uncertainty, hence the name Bayesian school. It is important, however, to remark that Bayes was not the first author to attach such interpretation to probability, nor the one that would really formalize it as a proper theory of inference. That would be really Laplace, who successfully showed how to use the rules of probability to assign probabilities to propositions of interest to the scientific domain, the most famous probably being his work inferring the mass of Saturn from noisy observations.\n\nThat we can operate with the rules of probability theory to work on degrees of belief was not, however, properly analyzed until the work of the physicist Richard Cox in the 20th century. In his work from 1946, he starts from the basic idea of quantifying our degree of belief in a proposition by associating a real number to it, without necessarily linking it to probabilities. The only condition is that the larger the number, the larger must be our degree of belief. Then, he moves onto imposing consistency rules using logical reasoning:\n\nIf we specify how much we believe a proposition X is true, we are implicitly specifying how much we believe it is false\n\nIf we specify how much we believe a proposition Y is true, and then how much another proposition X is true, given that Y is true, then we are implicitly specifying how much we believe both X and Y to be true\n\nThen, using Boolean logic and ordinary algebra, he found that consistency constraints this real numbers associated to our degree of belief to follow the rules of probability theory:\\begin{aligned}\nP(X) + P(\\bar{X}) = 1 \\; \\textrm{(sum rule)} \\\\  \nP(X,Y) = P(X|Y) P(Y)  \\; \\textrm{(product rule)}\n\\end{aligned}\n\nwhere ̄X denotes the proposition that X is false. Therefore, if we are bound to use the rules of logical reasoning, we can see that our degrees of belief can be rigorously mapped to probability theory.\n\nThese rules are just the basic building blocks from probability theory. Many other results can be derived from them, for instance the infamous Bayes’ theorem, which is a simple consequence of the product rule. Since\nwe can write the product rule either way in terms of X or Y:P(X,Y) = P(X|Y) P(Y) = P(Y|X) P(X)\n\nThen:P(X|Y) = \\frac{P(Y|X) P(X)}{P(Y)}\n\nwhich is Bayes’s theorem. We also could derive the marginalization rule:\n\nby taking the continuum limit starting from a discrete and complete set of events. We leave it as an exercise for the interested reader.\n\nThe work of Cox was continued by E.T. Jaynes, who would extensively work on the interpretation of probability as an extension of deductive logic, and tackle many of the controversial views on this interpretation --like the role of prior probabilities that we will discuss later on. Another typical criticism is its supposedly lack of objectivity  as a universal theory of knowledge, since degrees of belief are attached to individuals and therefore they lack the objectivity needed for consensual agreement on the validity of a proposition. As Jaynes points out, such subjectivity disappears if we introduce explicitly the set of information available to the observers: two rational observers with the same information available must agree on the degrees of belief assigned to a proposition, otherwise either they are not rational or they don’t share the same information. We can formalize this by introducing the set of information available, I into the definition of the probabilities, namely P(X|I).","type":"content","url":"/markdown/intro-bayesian#bayesian-probability","position":3},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl3":"Probability as an extension of logic","lvl2":"Bayesian probability"},"type":"lvl3","url":"/markdown/intro-bayesian#probability-as-an-extension-of-logic","position":4},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl3":"Probability as an extension of logic","lvl2":"Bayesian probability"},"content":"So far we have seen how we can map the rules of consistent reasoning based on degrees of belief to probability theory. Let us now go back to the link to deductive logic. A basic syllogism in logic isA \\implies B\n\nSo if A is true, then B is true. Let us discuss a few ways this syllogism is affected by uncertain.\n\nA first situation happens when the proposition A \\implies B itself is true, but we only have a degree of belief on A, expressed by probability P(A). If we are interested in the degree of belief on B, We can use the marginalization rule to write P(B) = P(B|A) P(A) + P(B|\\bar{A})P(\\bar{A}). Since the proposition A \\implies B is translated into P(B|A) = 1, hence P(B) = P(A) + P(B|\\bar{A})P(\\bar{A}) = P(A)(1- P(B|\\bar{A})) + P(B|\\bar{A}). As expected, if P(A) = 1 then P(B) = 1, in agreement with deductive logic. However, when P(A) < 1 the degree of belief on B depends on the other potential mechanisms that can trigger B when A is false, quantified by P(B|\\bar{A})\n\nA second situation is when proposition A \\implies B is true, but we observe B and not A. In this case we can use Bayes theorem:P(A|B) = \\frac{P(B|A) P(A)}{P(B|A) P(A) + P(B|\\bar{A})P(\\bar{A})}\n\nAgain, since P(B|A) = 1, this simplifies to:P(A|B) = \\frac{P(A)}{P(A) + P(B|\\bar{A})P(\\bar{A})}\n\nIn this case, the probability of A contingent to the observation of B depends, as in the previous case, on the other ways that B can be triggered when A is false, but also on the so-called prior probability of A, P(A), i.e. the degree of belief on A that we had before observing B. In the same language, P(A|B) is called a posterior probability. A key consequence of this result is that P(A|B) > P(A), since P(B|\\bar{A}) < 1 (otherwise we could not have P(B|A) = 1), and therefore the denominator P(A) + P(B|\\bar{A})P(\\bar{A}) < P(A) + P(\\bar{A}) = 1. Despite the uncertainty, knowledge of B always increases our degree of belief on A, i.e. we can derive useful knowledge out of the domain of deductive logic.\n\nPrior and posterior probabilities play a key role in Bayesian probability theory. Bayes theorem allows us to consistently update our prior degrees of belief (probabilities) based on new observations, in the form of posterior degrees of belief. As it happens with premises in deductive logic, prior probabilities can themselves be the result of previous inferences, although at some point they might need to be provided based on different considerations than previous observations, that might not be recorded systematically. Since they are inherent to Bayesian reasoning, critics of this theory have pointed them out as a weakness, an artificial requirement that biases the conclusions that other theories could just infer from the available observations.\n\nIn reality, those alternative theories are the ones that make hidden assumptions that effectively set the prior probabilities to naive choices in an uncontrolled way. Bayesian probability is only mathematically stating the obvious: our degree of belief on a proposition is not only driven by a given set of observations, there is always potentially prior information that can be used to improve our estimations. And in the rare case when it is not, there are also ways to assign consistently prior probabilities, as we will see in the next section.","type":"content","url":"/markdown/intro-bayesian#probability-as-an-extension-of-logic","position":5},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl3":"Assigning prior probabilities","lvl2":"Bayesian probability"},"type":"lvl3","url":"/markdown/intro-bayesian#assigning-prior-probabilities","position":6},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl3":"Assigning prior probabilities","lvl2":"Bayesian probability"},"content":"Theoretically, every prior probability could be written as a posterior probability conditional to previously observed data. This chain of probabilities can be exhausted in two ways: 1) because we don’t have on record any relevant data from which to compute posterior probabilities, 2) because such data does not exist, i.e. the proposition under analysis is completely agnostic to any data previously observed. Of course the latter makes for a deep philosophical discussion, which could be easily carried out to the beginning of the observed universe. In all practical terms, is case 1) the one we are faced with.\n\nWhen there is not relevant data recorded to compute posterior probabilities that serve as priors for later computations, we need to find a different way to assign prior probabilities. The general idea is to exploit known properties or constraints of the problem analyzed.\n\nA popular idea is assigning priori probabilities based on symmetries of the problem. In particular, we can use De Moivre’s computation of probability as a ratio of possible outcomes where a certain proposition is true:P(\\textrm{A}) = \\frac{\\#\\textrm{outcomes where A happens}}{\\#\\textrm{total number of outcomes}}\n\nThis definition is suitable for situations like dice games where 1) the outcomes are countable, 2) there is a symmetry in each outcome, in the sense that they can be considered equiprobable.\n\nA more general principle is that of maximum entropy, which generalizes the Moivre’s rule to more complicated propositions where our prior knowledge is based on limitations or constraints on the valid outcomes. Let us consider for simplicity a discrete set of outcomes i = 1, ..., N (which are themselves an exhaustive set of exclusive propositions), each with an a priori probability p_i that we want to determine. The entropy of this probability distribution is given, using Shanon’s definition, by:S = -\\sum_i p_i \\log p_i\n\nThe motivation for the entropy function is that it can be seen as arguably the most simple functional form that  consistently quantifies the uncertainty of a probability distribution, versus other potential assignments like \\sum_i p_i^2 for instance that can be seen to run into inconsistencies.\n\nIf we admit not having more information about a specific problem apart from the possible outcomes, we can assign as prior probabilities those that maximize entropy, conditional of course to be a complete set of probabilities, i.e. they sum up to 1. We include this restriction by using a Lagrange multiplier, so our objective function reads:L = -\\sum_i p_i \\log p_i + \\lambda (\\sum_i p_i -1)\n\nWe can know maximize this Lagrangian with respect to each p_i, although out of symmetry we can see that any permutation i \\rightarrow j leaves the functional invariant, so the solution must be p_1 = ... = p_N and using the constraint this means p_i = 1/N which recovers De Moivre’s rule.\n\nMore interesting are cases where we have extra restrictions. For instance, let us assume that our prior knowledge includes a restriction on the average of a random variable that takes values x_1, ..., x_N under each outcome, so we know:\\bar{X} = \\sum_i p_i x_i\n\nOur constraint maximization problem now reads:L = -\\sum_i p_i \\log p_i + \\lambda_1 (\\sum_i p_i -1) + \\lambda_2 (\\sum_i p_i x_i - \\bar{X})\n\nNotice that the symmetry argument does no longer hold, so we take derivatives to find the extreme:\\frac{\\partial L}{\\partial p_i} = -1 - \\log p_i + \\lambda_1 + \\lambda_2 x_i = 0\n\nThe solution isp_i = \\frac{e^{\\lambda_2 x_i}}{Z}\n\nwhere Z is a normalization constant, usually called the partition function in Statistical Physics applications. This is an exponential or Gibbs distribution for the random variable x. The Lagrange multiplier \\lambda_2 is related to the average \\bar{X} by means of the constraint:\\sum_i x_i p_i = \\frac{1}{Z} \\sum_i x_i e^{\\lambda_2 x_i} = \\frac{d \\log Z}{d \\lambda_2} = \\bar{X}\n\nThis also points out to a general property of the partition function in which the moments of the distribution can be computed by taking derivatives of the partition function with respect of \\lambda_2. It is usual to rewrite this term as \\lambda_2 = 1 / T, with T called the temperature in analogy to Statistical Physics.\n\nLet us consider a final case in which we add a second restriction on the second momentum of the distribution of the random variable x, namely \\sum_i p_i x_i^2 = \\bar{X_i^2}. The restricted optimization target now reads:L = -\\sum_i p_i \\log p_i + \\lambda_1 (\\sum_i p_i -1) + \\lambda_2 (\\sum_i p_i x_i - \\bar{X}) + \\lambda_3 (\\sum_i p_i x_i^2 - \\bar{X^2})\n\nComputation of the extreme yields:\\frac{\\partial L}{\\partial p_i} = -1 - \\log p_i + \\lambda_1 + \\lambda_2 x_i + \\lambda_3 x_i^2 = 0\\frac{\\partial L}{\\partial p_i} = -1 - \\log p_i + \\lambda_1 + \\lambda_2 x_i + \\lambda_3 (x_i + \\frac{\\lambda_2}{2\\lambda_3})^2 = 0\n\nThe solution can be written as:p_i \\propto e^{\\lambda_3(x_i + \\frac{\\lambda_2}{2\\lambda_3})^2}\n\nwhich properly normalized can be quickly recognized as a Gaussian distribution. As pointed out by Jaynes, that the Gaussian distribution is the maximum entropy distribution for a problem in which we know the mean and variance (via its second order momentum), can be seen as a motivation for its pervasiveness in all sorts of domains. It is a natural prior distribution for problems where we have knowledge of the scale of likely values of the phenomenon under observation.\n\nTo close this discussion, let us touch upon the question of non-informative priors, which seek to describe a situation in which there is not relevant knowledge a prior about the problem. It turns out this is not a simple topic, and such situation needs to be described with more precision. At the very least, we need to specify what we are ignorant about. Take the example of a parameter we are uncertain about, and we parametrize our belief about this parameter using a distribution with the form: f(\\nu, \\sigma), where \\nu is a location parameter and \\sigma is a scale parameter. This setup encompasses multiple real cases where we express our knowledge in terms of a most likely range of values. For instance, if f corresponds to a normal distribution, the range given by \\nu \\pm \\sigma has a probability of approximately 68% of containing the true value of the parameter, according to our beliefs. What does it mean to have a non-informative prior in this case? Since we are already introducing some prior information in the structure of the problem, we can not really say that we are fully ignorant, so we need to be more precise.\n\nA useful way to express such ignorance with precision is by using transformation groups and invariants of the problem. In this example, we could argue that having knowledge about the scale \\sigma means that our prior f(\\nu, \\sigma) changes under a transformation of scale: \\sigma \\rightarrow a \\sigma. As commented by Jaynes (page 379), if a scale transformation makes the problem appear different to us, then we must have some prior information about the absolute scale. The same argument can be applied to the location \\nu: if a change of location \\nu \\rightarrow \\nu + b affects our view of the problem, then we must have some prior information about the location of the parameter. If we change variables in a distribution it must hold:g(\\nu', \\sigma') d\\nu' d\\sigma' = f(\\nu, \\sigma) d\\nu d\\sigma\n\nIn our case, since \\nu' = \\nu + b and \\sigma' = a \\sigma, computing the Jacobian:g(\\nu', \\sigma') = a^{-1} f(\\nu, \\sigma)\n\nNow we can express the conditions for a non-informative prior by stating that such change of variables shall not make any difference to our prior distribution, since otherwise we would have indeed some knowledge about the scale or location of the problem. This means:g(\\nu, \\sigma) = f(\\nu, \\sigma)\n\nTherefore:f(\\nu + b, a\\sigma) = a^{-1} f(\\nu, \\sigma)\n\nThis equation must hold for any a and b. Let us take a = 1, in this case f(\\nu + b, \\sigma) = f(\\nu, \\sigma) which means that the function cannot depend on \\nu, hence f(\\nu, \\sigma) = \\phi(\\sigma). The resulting equation for a general a is \\phi(a \\sigma) = a^{-1} \\phi(\\sigma) whose general solution is \\phi(\\sigma) =  \\text{const} \\,\\sigma. Therefore, the non-informative prior for this problem is:f(\\nu, \\sigma) = \\frac{\\text{const}}{\\sigma}\n\nwhich corresponds to the so-called Jeffrey’s non-informative prior for this particular problem. As mentioned, this prior is relevant for instance in the particular but very common case when we model our beliefs using a Gaussian distribution. Ignorance about the location or scale of the distribution of this parameter can only be consistently encoded by using the Jeffrey’s prior.","type":"content","url":"/markdown/intro-bayesian#assigning-prior-probabilities","position":7},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl3":"Bayesian inference and hypothesis testing","lvl2":"Bayesian probability"},"type":"lvl3","url":"/markdown/intro-bayesian#bayesian-inference-and-hypothesis-testing","position":8},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl3":"Bayesian inference and hypothesis testing","lvl2":"Bayesian probability"},"content":"Bayes’ theorem can be used as the main tool to guide a theory of rational inference under uncertainty, providing us with a way to quantify the quality of a hypothesis H about the world based on the evidence (observations) available E:P(H|E) = \\frac{P(E|H)}{P(E)} P(H)\n\nA high score is achieved in different ways:\n\nWhen the prior probability of the hypothesis is high, meaning that the hypothesis was belief to be true even before analyzing the evidence.\n\nWhen the ratio \\frac{P(E|H)}{P(E)} is high, meaning that the probability of the evidence under the hypothesis is higher than the probability in general of the evidence. This means that evidence that is generally thought to be unlikely (meaning that P(E) was small), when explained by the hypothesis (i.e. P(E|H) is now high), provides a stronger score to the hypothesis than those cases in which the evidence was generally probable in any case (P(E) is high). Of course the reverse is also true: when a theory fails to explain evidence that is generally considered likely, then the score for this hypothesis will be lower.\n\nOne strength of Bayesian hypothesis testing that correctly captures many attributes attached to the way scientific research is normally conducted:\n\nScientists seek to test hypothesis by deriving predictions that are relatively unlikely. For the most precise case of a deterministic prediction, this means that P(E|H) = 1, and P(E) is small. This way, the ratio P(E|H) / P(E) is large, providing a strong support to the hypothesis in case the prediction is verified empirically.\n\nScientists that disagree on the validity of a hypothesis will score different prior probabilities to H. However, if predictions that are strongly attributed to this hypothesis (and weekly to alternative ones) are confirmed, their posterior probabilities will tend to converge since the scientist that attached a lower prior probability for H will also attach naturally a lower probability to the prediction overall, P(E), by virtue of Bayes’ theorem:P(E) = P(E|H)P(H) + P(E|\\bar{H})P(\\bar{H}) = P(H) + P(E|\\bar{H})P(\\bar{H})\n\nThis scientist therefore will need to update her posterior probability by a factor 1/P(E) larger than the one that originally believed in the truth of the hypothesis, since, assuming that P(E|\\bar{H}) < 1, then:P(H) + P(E|\\bar{H})P(\\bar{H}) =  P(H)(1-P(E|\\bar{H})) + P(E|\\bar{H})\n\nand hence P(E) is smaller for the scientist who attributes a lower prior probability to P(H). This implies a larger update of the posterior relative to the prior for the skeptic scientist, which implies a convergence in posteriors with the other scientist.\n\nScientists score higher hypotheses whose predictions have been validated multiple times independently, but the marginal increase in the score tends to decrease with the number of repetitions. In terms of Bayes’ theorem, this is reflected in the fact that as a prediction is validated thanks to evidences E_i in agreement with the prediction, P(E_1) < P(E_2) < P(E_3) <..., and given the upper bound P(E) \\leq 1, the convergence rate necessarily decreases when enough evidence is accumulated. The mechanics is the same as in the previous argument regarding scientists with different priors for a given hypothesis.\n\nFinally, scientists tend to prefer simple global explanations that cover multiple evidences to ad-hoc ones. The Bayesian framework incorporates this idea by attaching a small prior probability to ad-hoc mechanisms. This way, if we have a general hypothesis H and we attach to it an ad-hoc hypothesis H_A to explain some specific evidence, then, since P(H \\cap H_A) \\leq \\min(P(H), P(H_A)), the combined hypothesis starts with a small prior probability due to the introduction of the ad-hoc hypothesis. A classical example of ad-hoc hypotheses is the use of epicycles by Ptolemaeous in the 2nd century AD to account for the observed planet orbits under the general hypothesis that the sun and the rest of the planets were orbiting around the Earth. A Bayesian approach would directly imply a low probability for this ad-hoc extension of the theory to account for the observations.\n\nA typical criticism of Bayesian hypothesis testing is that theoretically, to compute P(E), we would need to consider all the possible alternative hypotheses, including many we might not be aware of. However, in many practical inference scenarios, the question of interest is to score relatively conflicting pairs of hypothesis, e.g. H_1 and H_2. But a relative score can be computed by dividing their posterior probabilities under the same evidence:\\frac{P(H_1|E)}{P(H_2|E)} = \\frac{P(E|H_1)}{P(E|H_2)}\\frac{P(H_1)}{P(H_2)}\n\nWhen using this relative score, then P(E) is cancelled since it is common to both scores, removing the need to consider other hypotheses.","type":"content","url":"/markdown/intro-bayesian#bayesian-inference-and-hypothesis-testing","position":9},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl3":"Bayesian decision theory","lvl2":"Bayesian probability"},"type":"lvl3","url":"/markdown/intro-bayesian#bayesian-decision-theory","position":10},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl3":"Bayesian decision theory","lvl2":"Bayesian probability"},"content":"Bayesian theory is a tool for reasoning, but reasoning itself is a tool to make decisions. When we make inferences about the world, we are mostly trying to use this information to guide our behavior, our choices. Examples abound, ranging from the quotidian:\n\nWe would like to predict tomorrow’s weather, because we want to decide if we will be working from home or commuting to the office\n\nor the academic:\n\nA scientist wants to assess if a new hypothesis or theory has enough support to be accepted with respect to previous ones with respect to an observed phenomenon.\n\nIn the context of the topics covered in this book, some examples could be the following:\n\nA trader (human or algorithmic) wants to estimate the value of an economic indicator and compare it with the market consensus, in order to make decisions about which positions to take\n\nA dealer wants to predict demand for a financial instrument in order to set prices.\n\nBayesian probability as discussed so far only evaluates probabilities taking into account our state of knowledge. To use these probabilities to make decisions we need to extend our framework. Following the work of Abraham Wald, a framework for decisions consists on the triplet (\\theta_i, D_j, L_{ij}) where:\n\n\\theta_i enumerates the potential states of the world that are relevant to the outcome associated with a decision. In our previous examples, for instance, the different levels of demand for the financial instrument, or if actual inflation is above or below market consensus. Within the Bayesian probability framework, we also assign posterior probabilities P(\\theta_i |I) to each state of the world, where I is the set of information available at the time of making the decision.\n\nD_j enumerates the set of possible decisions that can be made. In the first example, the positions taken by the trader to benefit from the inflation forecast. In our second example, the decision is the price that will be quoted for the instrument.Notice that nothing in the framework prevents that the probabilities P(\\theta_i |I) depend on the decision taken, since the decision is taken as well using information contained in I. For example, when choosing a price for an instrument, the actual demand will likely depend on the price chosen, i.e. the state of the world is affected by our decision.\n\nL_{ij} is a function that assigns a numerical loss (or gain, if negative) to making decision j when the state of the world turns out to be i. In the example of the trader, we can quantify the profit or loss associated to the positions taken after the actual inflation indicator is released and the market moves accordingly. The dealer quoting a financial instrument might want to maximize profits, i.e. price times realized demand at this price, in a full round trip in which the instrument is bought and later sold.\n\nIn our present discussion of decision theory, we will limit ourselves to setups where we can isolate the effect of a single decision in the loss function, i.e. future decisions are considered independently. If the loss function depends not only on current decisions but also future ones, the problem becomes more complex and is the subject of Stochastic Optimal Control Theory, that we will addressed in a separate chapter. Such a problem is also the domain of Reinforcement Learning Theory, which we will also address later, but the approach is different in the sense that this framework skips the explicit probabilistic modelling of the environment and directly tries to find which decisions minimize losses.\n\nOnce we have specified the elements necessary for decision making, we need a specific criterion to evaluate the quality of the decisions. A natural one is minimization of expected loss under the posterior probability distribution:D^* = \\text{argmin}_{D_j} \\sum_i P(\\theta_i|D_j, I) L_{ij}\n\nwhere we have explicitly introduced the decision D_j in the posterior probability to reflect the possibility that our decisions influence the state of the world that will be selected.\n\nThis decision rule is general enough to capture a variety of real situations. Let us consider for instance the case in which we know that the state of the environment is not selected at random but there is an adversary that will select the state that maximizes our loss given our choice. In this scenario P(\\theta_i|D_j, I) = 1_{i = \\text{argmax}_i L_{ij}}. Plugging this expression into the decision rule:D^* = \\text{argmin}_{D_j} \\text{max}_i L_{ij}\n\nThis is called the minimax rule, since the optimal strategy is to choose the decision that corresponds to the lowest maximum loss across the states of the world.\n\nAnother typical setup is related to parameter estimation. In Bayesian theory, when making a probabilistic model for a specific problem, we model the parameters of the distribution as random variables themselves, in order to quantify our degrees of belief in the different values that these parameters can have, both a priori before we see any relevant data and a after that. Bayes theorem is again the tool to consistently update our beliefs given the new empirical evidence. At some point, though, we might need to make decisions that involve a representative point estimation of the value of the parameters. For instance, in our pricing example, client demand is a probabilistic model that links demand with prices quoted and other features. When we want to decide on an optimal price to quote, we need to decide on a specific set of parameters that describe the demand. A natural choice could be to take the mean of the parameters, meaning that our optimal prices will maximize expected demand. But such estimation does not take into account potential trade-offs when using parameters that turn out to deviate from the ones actually driving the process. The business impact of quoting a too conservative price and therefore reducing the number of clients might be asymmetric to the case where the price is too aggressive. In this context, the optimal point estimation (or estimator, using the jargon of statistics) depends on the cost of choosing a value that differs from the actual one, which is of course unknown. The optimal estimation problem reads then:\\hat{\\theta} = \\text{argmin}_{\\hat{\\theta}} \\int d\\theta P(\\theta| I) L(\\theta, \\hat{\\theta})\n\nwhere we have used the continuous version of the expression which suits better the problem of parameter estimation. Notice that in this case the decision is about the value of the parameters that we will use as estimator, and the state of the world is the the actual value of the parameter. Depending on the loss function chosen, different estimators solve this equation. Let us discuss the most typical setups:\n\nQuadratic error: L(\\theta, \\hat{\\theta}) = (\\theta - \\hat{\\theta})^2. This error is symmetric and penalizes more larger errors with respect to the correct value of the parameter. The penalty increases marginally for larger errors as well. The estimation equation corresponds to the mean squared error (MSE)  functional:\\hat{\\theta} = \\text{argmin}_{\\hat{\\theta}} \\int d\\theta P(\\theta| I)  (\\theta - \\hat{\\theta})^2\n\nWe take the derivative with respect to \\hat{\\theta} to find the extreme of this equation:\\int d\\theta P(\\theta| I)  (-2) (\\theta - \\hat{\\theta}) = 0 \\rightarrow \\hat{\\theta} = \\int d\\theta P(\\theta| I) \\theta = \\bar{\\theta}\n\nwhich can be easily proved to be a minimum. Hence, the optimal estimator for this loss function is the mean of the posterior distribution. The mean is a natural candidate as an estimator in many real-life problems. This derivation shows under which conditions it makes sense for parameter estimation, namely when estimation errors are penalized quadratically.\n\nAbsolute error: L(\\theta, \\hat{\\theta}) = |\\theta - \\hat{\\theta}|. This loss function also penalizes more larger errors and is symmetrical, but the marginal rate of growth of the penalty is constant, not linear as in the quadratic case. The estimation equation corresponds to the mean absolute error (MAE) functional:\\hat{\\theta} = \\text{argmin}_{\\hat{\\theta}} \\int d\\theta P(\\theta| I)  |\\theta - \\hat{\\theta}|\n\nWe can rewrite this equation to simplify finding the extreme:\\int d\\theta P(\\theta| I)  |\\theta - \\hat{\\theta}| = \\int_{\\hat{\\theta}}^{\\infty} P(\\theta| I) (\\theta - \\hat{\\theta})- \\int_{-\\infty}^{\\hat{\\theta}} P(\\theta| I) (\\theta - \\hat{\\theta})\n\nTaking the derivative with respect to \\hat{\\theta}, we get the condition:\\int_{\\hat{\\theta}}^{\\infty} P(\\theta| I) = \\int_{-\\infty}^{\\hat{\\theta}} P(\\theta| I)\n\nwhich is the definition of the median, hence \\hat{\\theta} = \\text{median}(P(\\theta|I)). This sheds light on the potential use of this loss function: the median is usually considered a more robust estimator against the presence of outliers than the mean. An estimator based on the quadratic error gets more influenced by the behavior of the distribution in the tails than the absolute error.\n\nDelta error: L(\\theta, \\hat{\\theta}) = -\\delta(\\theta - \\hat{\\theta}). When using this error, we only care about being exactly right. If we are wrong, we don’t care about the magnitude of the error. The optimal estimation equation reads then:\\hat{\\theta} = \\text{argmin}_{\\hat{\\theta}} -\\int d\\theta P(\\theta| I) \\delta(\\theta - \\hat{\\theta})= \\text{argmax}_{\\hat{\\theta}} P( \\hat{\\theta}| I)\n\nwhich we recognize directly as the mode of the posterior distribution: \\hat{\\theta} = \\text{mode}(P(\\theta|I)). Interestingly, if we apply Bayes’ theorem and we considered a prior P(\\theta) that is constant around the high likelihood region, we have:\\text{argmax}_{\\hat{\\theta}} P( \\hat{\\theta}| I) \\simeq \\text{argmax}_{\\hat{\\theta}} P( I | \\hat{\\theta})\n\nwhich is the well-known Maximum Likelihood Estimator (MLE), used extensively in most real life statistical modelling and Machine Learning problems. Its wide recognition as a useful parameter estimation technique must be acknowledged, with the disclaimer of the many assumptions that it requires to be considered an optimal estimator. Bayesian theory and decision theory help us to understand the assumptions required to consider this family of estimator an optimal choice, in contrast with other statistical frameworks which only focus on desirable mathematical properties of the estimators derived.","type":"content","url":"/markdown/intro-bayesian#bayesian-decision-theory","position":11},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl3":"Example: estimating the probability of heads in a coin toss experiment","lvl2":"Bayesian probability"},"type":"lvl3","url":"/markdown/intro-bayesian#example-estimating-the-probability-of-heads-in-a-coin-toss-experiment","position":12},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl3":"Example: estimating the probability of heads in a coin toss experiment","lvl2":"Bayesian probability"},"content":"Let us discuss the classical probability example of a coin toss experiment. First we discuss the setup, which is typically omitted in most discussions. The setup of the problem will shape our prior knowledge. Let us assume some individual will toss a coin N times, and our goal is to predict the probability that the next toss will yield heads (“H”) and not tails (“T”). This is written as:P(H|D_N)\n\nwhere D_N is a sequence of N observations, heads or tails. Part of our prior knowledge assumes that the individual tossing the coin is not particularly skilled, so the initial conditions of each experiment are relatively randomized. Our object of interest can be reduced to the distribution of mass in the coin, which makes it a fair or unfair coin. Since such distribution, in the end, influences the probability that a given toss yields heads or tails, which we denote p_H, we can make this quantity our object of inference. Notice that given a specific coin with initial random toss conditions, we can assume that p_H is completely determined by the physical properties of the coin, so it can be known if those properties are observed. Of course we don’t have such information so we can only provide estimations in the form of probabilities that p_H have different values. Such probabilities are Bayesian, in the sense that they reflect our relative degrees of belief on each potential value of p_H, in the form of a probability distribution P(p_H).\n\nNotice that this setup is not something that might be necessarily taken for granted. As discussed by Jaynes, one could potentially prepare a setup where a robot or a highly skilled person can control the initial conditions, to the point of being able to determine the outcome each time. If this is the case, our inference is not necessary uniquely about the mass distribution of the coin, but also about the skill or setup involved in the experiment.\n\nAnother source of potential prior knowledge concerns the possibility that the coin has identical faces, i.e. two tails or two heads. If the individual has shown us the coin beforehand, such knowledge belongs to the prior information, e.g. if we have seen that it has tails and heads then we already know that P(p_H = 0) = 0 and P(p_H = 1) = 0. If we have seen only one face, e.g. tails, we can discard P(p_H = 1) but not P(p_H = 0). It seems clear that in many practical situations prior knowledge about the problem consists on statements about the probability at the corners, i.e. p_H=0 or p_H=1. We can use then the maximum entropy principle to propose a prior distribution that incorporates this knowledge.\n\nInstead of directly working with restrictions on p_H, let us encode them in terms of the logarithm, \\log p_H. The rational is that it makes more natural to discuss probabilities that are infinitesimally close to 0 and 1. Arguably, working on the logarithm of probabilities is more natural to human brains, the same as happens with other types of intensities like noise, where the use of logarithms (i.e. decibels) is standard. Human brains struggle to see the difference between p_H = 0.001 or p_H = 0.0001, but in many practical applications the distinction is relevant. Jaynes actually proposes to work on logarithms of probabilities in general.\n\nFor consistency with the previous discussion on the maximum entropy principle, let us work in a set of discrete values for the probabilities p_{H,i}, i = 1, N,  although we could generalize to a continuous functional. The Lagrangian for the problem reads:L = \\sum_i P(p_{H,i}) \\log P(p_{H,i}) + \\lambda_1 (\\sum_i P(p_{H,i})-1) + \\lambda_2 (\\sum_i P(p_{H,i})\\log p_{H,i} - ELP_0) + \\lambda_3 (\\sum_i P(p_{H,i}) \\log (1 - p_{H,i}) - ELP_1)\n\nwhere we have set the restrictions over the expected value of the log-probabilities, i.e. E[\\log p_H] = \\sum_i P(p_{H,i})\\log p_{H,i} and E[\\log p_T] = \\sum_i P(p_{H,i})(1-\\log p_{H,i}).\n\nThe extreme of this function with respect to P(p_{H,i}), which is the unknown, is:1 + \\log P(p_{H,i}) + \\lambda_1 + \\lambda_2 \\log p_{H,i}\n+ \\lambda_3 \\log (1-p_{H,i}) = 0\n\nwhose solution is:P(p_{H,i}) = e^{1 + \\lambda_1}p_{H,i}^{\\lambda_2}\n(1-p_{H,i})^{\\lambda_3}\n\nIf we apply the normalization constraint and redefine \\lambda_1 and \\lambda_2 we get as a result a Beta distribution:P(p_{H,i}) = \\frac{p_{H,i}^{\\alpha-1}\n(1-p_{H,i})^{\\beta-1}}{B(\\alpha, \\beta)}\n\nwhere B(\\alpha, \\beta) is the beta function. We can now go back to the continuous limit so we have:P(dp_{H}) = \\frac{p_{H}^{\\alpha-1}\n(1-p_{H})^{\\beta-1}}{B(\\alpha, \\beta)} dp_H = f(p_H) d p_H\n\nwhere f(p_H) is the probability density function (pdf) of p_H. That the beta distribution is the maximum entropy prior is particularly convenient for the inference of p_H, since it has the interesting property of being conjugate to the likelihood of observations following a Bernoulli distribution, which is the natural model for observations of identically and independently distributed (i.i.d.) random variables, each representing a coin toss. If we have a sequence of N observations of coin tosses, which we denoted as D_N above, our best inference for p_H is given by:f(p_H|D_N) = \\frac{P(D_N|p_H) f(p_H)}{P(D_N)}\n\nwhere we have applied Bayes’ theorem. The likelihood function is P(D_N|p_H) which for a Bernoulli random variable (and a given specific sequence of coin tosses) reads:P(D_N|p_H) = p_H^{n_H} (1-p_H)^{N-n_H}\n\nwhere n_H is the number of times we have observed heads H in the sequence. When we say that the Beta distribution is conjugated to the Bernoulli likelihood we mean that the posterior distribution is again a Beta distribution with updated parameters. The denominator in Bayes theorem can be computed using:p(D_N) = \\int P(D_N|p_H) f(p_H) dp_H = \\frac{1}{B(\\alpha, \\beta)}\\int p_H^{n_H + \\alpha - 1} (1-p_H)^{N - n_H + \\beta - 1} dp_H = \\frac{B(n_H + \\alpha, N - n_H + \\beta)}{B(\\alpha, \\beta)}\n\nTherefore the posterior is:f(p_{H}) = \\frac{p_{H}^{n_H + \\alpha-1}\n(1-p_{H})^{N-n_H + \\beta-1}}{B(n_H + \\alpha, N - n_H + \\beta)}\n\nwhich is Beta distribution with updated parameters. This result not only simplifies inference by a simple rule of updating the parameters of the distribution, it also provides an intuitive interpretation for the Beta prior parameters: we can interpret \\alpha as a prior number of heads observations, and correspondingly \\beta is a prior number of tails. This does not mean that we have really observed those, since otherwise it would not be part of the prior, but it is a convenient parametrization of the prior belief. For example, \\alpha = \\beta = 1, which in the Beta distribution corresponds to an uniform prior, would be equivalent to someone who has observed one instance of heads and one of tails. This is not a minor piece of information, since it means that both values are plausible, discarding for example the possibility that we have a two heads or two tails coin.\n\nTherefore, the uniform prior, an intuitive candidate for a non-informative prior in this setup, is actually not such: it can actually be interpreted as a prior where we assume that both heads and tails are plausible. A full non-informative prior is, in this case, best parametrized by \\alpha = \\beta = 0, i.e. no observations at all, which yields a distribution of the form:f(p_H) \\propto \\frac{1}{p_H (1- p_H)}\n\nThis distribution is unfortunately ill-defined, since it cannot be normalized. It corresponds to Jeffrey’s non-informative prior for this setup: as shown by Jaynes (pages 382-385), it can be derived using an invariance argument describing a state of knowledge in which any evidence does not provide relevant information about p_H. We leave to the interested reader to check the argument in his book.\n\nLet us now see the effect of the prior on the inference provided by Bayes’ theorem on the parameter p_H. As we have seen, when we use the Beta distribution to model the prior, the posterior distribution is also a Beta distribution with updated parameters. This simplifies considerably the calculations of the posterior probability. We could still use other priors that might suit better our modelling needs, at the price of numerical estimation of the posterior probability. We stick, though, with the Beta distribution prior. In order to have a simple description of the posterior, we use decision theory to choose the best point estimator that minimizes the square error, which as we discussed before corresponds to the mean of the posterior distribution. We could use other estimators if they suit better our particular use case. In the case of the mean, if we use the Beta prior it is given by:\\bar{p}_H = \\frac{n_H + \\alpha}{N + \\alpha + \\beta}\n\nwhich shows explicitly the influence of the prior on the estimation, via the parameters \\alpha and \\beta. Interestingly, the choice of \\alpha = \\beta = 1 does yield:\\bar{p}_H = \\frac{n_H + 1}{N + 2}\n\nwhich reinforces our interpretation of the Beta prior parameters as a number of effective prior observations of heads and tails, with the uniform prior corresponding to a single heads and tails. The non-informative prior, though, where \\alpha = \\beta = 0, corresponds to:\\bar{p}_H = \\frac{n_H}{N}\n\nwhich corresponds to the widespread rule according to which probabilities shall be estimated as frequencies. In this light, such interpretation only makes sense when using a non-informative prior and the squared error as our loss function to build point estimators.\n\nWe generate numerically random numbers following a Bernoulli distribution with probability p_H = 0.3, and we see the effect on the posterior distribution of having an increasingly large number of samples available, for different choices of priors.\n\nFor the first simulation, we start with the uniform prior, which as we have discussed is the one that is mistakenly considered as non-informative. This prior corresponds to \\alpha = \\beta = 1 in the Beta distribution, which plays the role of having effectively two prior observations, one where heads was observed, the other tails.  In the following figure, we can see that the posterior moves relatively quickly (with 50 observations) around the correct range of values and to have a relatively confident and correct estimation around 300 observations. We use the mean and the standard deviation to characterize the posterior probability distribution.\n\n\n\nFigure 1:Posterior probability for the probability of heads on a random simulation of a coin with underlying probability p_H = 0.3. Every subplot corresponds to 50 new observations. The first one corresponds to the prior, in this case a uniform one: \\alpha = \\beta = 1 in the Beta distribution prior. We characterize the posterior probability using the mean and the standard deviation of the posterior probability.\n\nA similar result is achieved using Jaynes’ non-informative prior, as shown in the next figure. This is achieved by taking the limit \\alpha, \\beta \\rightarrow 0, i.e. zero effective prior observations. Despite the strong bias towards the extreme values of the distribution, this does not prevent that the posterior incorporates quickly the information from the observations, at the same rate as the uniform prior in this numerical simulation.\n\n\n\nFigure 2:Posterior probability for the probability of heads on a random simulation of a coin with underlying probability p_H = 0.3. Every subplot corresponds to 50 new observations. The first one corresponds to the prior, for which we choose a non-informative one for this simulation, which we proxy using \\alpha = \\beta = 10^{-6}. We characterize the posterior probability using the mean and the standard deviation of the posterior probability. Despite the strong biased towards the extremes, this choice does not affect the speed of convergence towards the true value.\n\nFinally, we use a prior that contains meaningful prior information, albeit erroneous one, since the prior distribution assumes that p_H has a mean value of 0.5 with a standard deviation of 0.064. This corresponds to a Beta distribution with \\alpha = \\beta = 30, which we interpret as 60 effective prior observations. In this case, a larger number of real observations is required to override the effect of the prior. For instance, with 50 observations the estimation is still half way between the prior one and the real one in this simulation. For 400 observations, the estimation is still influenced by the prior. It takes around 1000 observations to converge to the real value.\n\n\n\nFigure 3:Posterior probability for the probability of heads on a random simulation of a coin with underlying probability p_H = 0.3. Every subplot corresponds to 50 new observations. The first one corresponds to the prior. In this case, we choose a prior relatively confident of a value of 0.5 for p_H, corresponding to a choice \\alpha = \\beta = 30. We characterize the posterior probability using the mean and the standard deviation of the posterior probability. The plots show a slow convergence to the true value due to the effect of the prior information into the estimation.\n\nSuch result could suggest that is always better to choose non-informative priors given the penalty of using wrong prior information. However, as shown in the following figure, having good prior estimation can speed up the process of learning with respect to the case of using non-informative priors. We choose in this case a prior with \\alpha = 18, \\beta = 42, which corresponds to a prior mean of 0.3 and standard deviation 0.059. This choice corresponds to the same number of effective prior observations as the previous case, to provide a meaningful comparison.\n\n\n\nFigure 4:Posterior probability for the probability of heads on a random simulation of a coin with underlying probability p_H = 0.3. Every subplot corresponds to 50 new observations. The first one corresponds to the prior, which in this case has a correct prior belief on the value of p_H. This corresponds in this case to a choice \\alpha = 18, \\beta = 42. We characterize the posterior probability using the mean and the standard deviation of the posterior probability. The plots show a quick convergence to the true value with high confidence.","type":"content","url":"/markdown/intro-bayesian#example-estimating-the-probability-of-heads-in-a-coin-toss-experiment","position":13},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl2":"Bayesian Machine Learning"},"type":"lvl2","url":"/markdown/intro-bayesian#bayesian-machine-learning","position":14},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl2":"Bayesian Machine Learning"},"content":"One of the main advantages of Bayesian theory is its simplicity and coherence. The toolkit presented in the previous section can be used in principle to evaluate the probability of any hypotheses conditional to the available information. In modern Machine Learning (ML), specific emphasis is placed on two families of hypotheses:\n\nHypotheses on the value of parameters of a given model. This is the learning task in the ML jargon. From a Bayesian point of view, as we saw in the previous section, they are simply stated as posterior distributions of parameters given the available information, in particular observations in a training set: P(\\theta|D). Recall that the outcome are probability distributions. If we want to reduce parameter estimation to point estimators, Bayesian theory needs to be complemented with Decision theory.\n\nHypotheses on the future value of observations. This is the prediction task in the ML jargon. In Bayesian theory, this can be again written in terms of a posterior probability, in this case on the values of unobserved data \\bf{x} given the available observed data: P({\\bf x}|D)\n\nBayesian Machine Learning is therefore an application of Bayesian theory to the hypotheses of interest in Machine Learning, namely learning and prediction. Let us have a closer look to them.","type":"content","url":"/markdown/intro-bayesian#bayesian-machine-learning","position":15},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl3":"Bayesian learning","lvl2":"Bayesian Machine Learning"},"type":"lvl3","url":"/markdown/intro-bayesian#bayesian-learning","position":16},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl3":"Bayesian learning","lvl2":"Bayesian Machine Learning"},"content":"As mentioned, in Bayesian theory, learning from data is a result of the application of the Bayes’ theorem to an existing prior probability distribution. In terms of Machine Learning problems, it can be applied both to supervised  learning problems, where we model the probability of a target t (continuous in regression, discrete in classification) conditional to a set of features \\bf{x} and some observed data \\it{D}:p(t|\\bf{x}, \\it{D})\n\nor to unsupervised machine learning problems, where we model the probability of a data point \\bf{x} given some observed data \\it{D}:p(\\bf{x}| \\it{D})\n\nWe start by modelling these probability distributions with a prior distribution, specified by a set of parameters \\bf{\\theta}. Recall that in the Bayesian paradigm, these parameters are also modelled with a probability distribution, whose prior is:P(\\bf{\\theta} | \\bf{\\alpha})\n\nwhere \\bf{\\alpha} are hyper-parameters of this prior distribution.\nNow we observe some data \\it{D} which includes observations of the target and the features. Bayesian learning means updating the distribution of the parameters that define the predictive distribution after observing new data, i.e.:P(\\bf{\\theta} | \\it{D}, \\bf{\\alpha}) = \\frac{P(\\it{D}| \\bf{\\theta}, \\bf{\\alpha}) P(\\bf{\\theta}|\\bf{\\alpha})}{P(\\it{D}| \\bf{\\alpha})}\n\nIn general, the posterior distribution of the parameters will not have a closed-form, due to the difficulty of computing the denominators (the evidence). There are typically four approaches to tackle this problem:\n\nAs we saw in the previous section, in the particular case where we use conjugate priors, the posterior can be computed analytically. For a conjugate prior, the posterior belongs to the same to the same family of distributions as the prior, with updated parameters.\n\nLaplace approximation: in this case the posterior distribution is approximated by a Gaussian by using a second order expansion of the log probability of the posterior around its maximum (MAP, Maximum a Posteriori). We will discuss it later in this section\n\nMonte Carlo methods, which aim to generate samples from the posterior distribution, despite not having a closed form. In particular, Markov Chain Monte Carlo (MCMC) methods are the most popular ones, which rely on building a Markov Chain that asymptotically converges to the posterior.\n\nVariational methods, which seek to analytically approximate the posterior by finding the closest one from a more simple family of distributions (e.g. Gaussian ones). For a distance between distributions, the Kullback–Leibler divergence is typically used. A particular case of a variational methods is that of Mean Field Theory, widely used in Statistical Physics, in which the posterior distribution is approximated by the distribution of a set of independent latent factors.\n\nFor an introduction to the most computational demanding methods like MCMC and Variational Methods, a good starting reference is [ONLINE BAYESIAN BOOK]","type":"content","url":"/markdown/intro-bayesian#bayesian-learning","position":17},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl4":"Bayesian online learning","lvl3":"Bayesian learning","lvl2":"Bayesian Machine Learning"},"type":"lvl4","url":"/markdown/intro-bayesian#bayesian-online-learning","position":18},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl4":"Bayesian online learning","lvl3":"Bayesian learning","lvl2":"Bayesian Machine Learning"},"content":"Bayesian learning is particularly suitable for online (or mini batch) learning. In this case, the prior distribution is the result of previous learning, and the posterior updates this prior as new data arrives. In the strictly online limit, this means every time a new observation arrives. Formally, this means:P(\\bf{\\theta} | \\it{D}_t, \\bf{\\alpha}) = P(\\bf{\\theta} | \\it{d}_t,\\it{D}_{t-1}, \\bf{\\alpha}) = \\frac{P(\\it{d}_t| \\it{D}_{t-1}, \\bf{\\theta}, \\bf{\\alpha}) P(\\bf{\\theta}|\\it{D}_{t-1},\\bf{\\alpha})}{P(\\it{d}_t| \\it{D}_{t-1}, \\bf{\\alpha})}\n\nwhere \\it{D}_t = \\{\\it{d}_t, \\it{d}_{t-1}, ..., \\it{d}_0\\} = \\{\\it{d}_t, \\it{D}_{t-1}\\}  represents the dataset indexed by the time t of arrival of new information, being \\it{d}_t the data acquired at time t.","type":"content","url":"/markdown/intro-bayesian#bayesian-online-learning","position":19},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl3":"Bayesian prediction","lvl2":"Bayesian Machine Learning"},"type":"lvl3","url":"/markdown/intro-bayesian#bayesian-prediction","position":20},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl3":"Bayesian prediction","lvl2":"Bayesian Machine Learning"},"content":"After observing some data and updating the probability distribution of the parameters of the model, the model can be used to perform inferences in supervised and unsupervised problems. The peculiarity of the Bayesian Machine Learning paradigm is that we don’t have a single predictive model, but a family of them parametrized by \\bf{\\theta}. By learning we aim to narrow down which of the possible models within this family explain better the observed data. But in contrast to classical statistics, we don’t need to narrow our choice to one of these models (like the most likely one), but we use the whole posterior distribution over parameters to improve predictions.\n\nIn the supervised machine learning problem, this translates in the following predictive distribution for the target:p(t|{\\bf x}, {\\it D}) = \\int d \\theta p(t|{\\bf x}, {\\it D}, \\theta) p(\\theta|{\\it D})\n\nSimilarly, in the unsupervised machine learning problem, the distribution of the target data given observations is calculated as:p({\\bf x}| {\\it D}) = \\int d \\theta p({\\bf x}| {\\it D}, \\theta) p({\\bf \\theta}|{\\it D})\n\nThese integrals are typically difficult to solve, except for some particular choices for the family of distributions. In case the integrals are intractable, one can resort either to numerical approximations or analytical ones.\n\nAmong the analytical approximations, the most simple one is using the MAP (*Maximum a Posteriori+) solution, which as discussed in the previous section means calculating the most likely value of the parameters using the posterior distribution:\\theta_{MAP} = {\\bf argmax}_{\\theta}  p(\\theta | {\\it D})\n\nThen we do the approximation:p(\\theta | {\\it D}) \\simeq \\delta (\\theta - \\theta_{MAP})\n\nand therefore:p(t|{\\bf x}, {\\it D}) = p(t|{\\bf x}, \\theta_{MAP})p({\\bf x}, {\\it D}) = p({\\bf x}| \\theta_{MAP})\n\nAn improvement upon this approximation is using the Laplace approximation for the posterior, which essentially consists on approximating the posterior using a second order Taylor series around the MAP estimator. Instead of dealing, though, directly with the posterior, it is better to work with its logarithm L. For simplicity, let us consider the case of a single parameter to estimate:L = \\log p(\\theta | {\\it D}) \\simeq L(\\theta_{MAP}) + \\frac{1}{2} \\frac{d L^2}{d \\theta} |_{\\theta_{MAP}} (\\theta - \\theta_{MAP})^2\n\nwhere the first order term is zero since we are expanding around the maximum, and the second derivative is negative. The Laplace approximation for the posterior then reads:p(\\theta | {\\it D}) \\simeq A \\exp \\left[ \\frac{1}{2} \\frac{d L^2}{d \\theta} |_{\\theta_{MAP}} (\\theta - \\theta_{MAP})^2 \\right]\n\nwhich is a Gaussian distribution. The predictive distribution in this case cannot be derived generally, it will depend on the shape of the likelihood function.\n\nNotice that in both approximations we are assuming that we are dealing with uni-modal distribution. In case of multi-modal distributions, these approximations no longer approximate well the posterior, and other techniques must be used.","type":"content","url":"/markdown/intro-bayesian#bayesian-prediction","position":21},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl3":"Example: coin toss experiment","lvl2":"Bayesian Machine Learning"},"type":"lvl3","url":"/markdown/intro-bayesian#example-coin-toss-experiment","position":22},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl3":"Example: coin toss experiment","lvl2":"Bayesian Machine Learning"},"content":"Let us come back to our previous example of estimating the probability of a coin toss resulting in heads or tails. By using a Beta conjugate prior to the likelihood, the posterior had a closed-form which is an updated Beta distribution:f(p|D_N) = \\frac{1}{B(\\alpha + n_H, \\beta + N - n_H)} p^{\\alpha + n_H-1} (1-p)^{\\beta + N - n_H -1}\n\nWith the information available so far, we want to predict the result of the next coin toss. The predictive distribution reads:P(t_{N+1} = H| D_N) = \\int dp P(t_{N+1} = H | p, D_N) f(p |D_N) = \\int dp p f(p |D_N) \\\\ = \\int dp \\frac{1}{B(\\alpha + n_H, \\beta + N - n_H)} p^{\\alpha + n_H} (1-p)^{\\beta + N - n_H -1}  = \\frac{ \\alpha + n_H} {\\alpha + \\beta + N}\n\nwhich in this case is the mean of the Beta distribution. Notice that this is not the same result that we would obtain in classical statistics using maximum likelihood even if we had a uniform prior (\\alpha = \\beta = 1), since the mean and the mode of the Beta distribution don’t coincide, the mode being \\frac{\\alpha - 1}{\\alpha + \\beta - 2} for \\beta > 1\n\nIn this case, due to the choice of a conjugate prior, we have a closed-form for the posterior and the predictive distribution can also be analytically calculated. However, let us assume this is not possible and use the Laplace approximation to obtain the predictive distribution. The MAP estimator for the posterior is, by finding the maximum of the Beta distribution:p_{MAP} = \\frac{n_H + \\alpha -1}{N + \\alpha + \\beta - 2}q_{MAP} \\equiv 1 - p_{MAP} = \\frac{ N - n_H + \\beta -1}{N + \\alpha + \\beta - 2}\n\nThe second order derivative of the log-likelihood evaluated at the maximum MAP then reads:\\frac{d L^2}{d p^2} |_{p_{MAP}} = - \\frac{N + \\alpha + \\beta - 2}{p_{MAP} q_{MAP}}\n\nThe posterior distribution is therefore approximated by the following Gaussian distribution:f(p | {\\it D}) \\simeq N(p_{MAP}, \\sqrt{\\frac{p_{MAP} q_{MAP}}{N + \\alpha + \\beta - 2}})\n\nThe predictive distribution is now easy to evaluate since it is just the mean of a Gaussian:P(t_{N+1} = H| D_N) \\simeq p_{MAP} = \\frac{n_H + \\alpha -1}{N + \\alpha + \\beta - 2}\n\nThe result differs slightly from the exact one, but for N, n_H >> 1 and / or \\alpha, \\beta >> 1 they converge, as it is actually expected since in this regime the Beta distribution has the Gaussian distribution as a limit, as discussed in the previous section.\n\nWe can test the validity of this result numerically by plotting the actual posterior and the Laplace approximation, for different number of observations and specification of the prior:\n\nCONTINUE","type":"content","url":"/markdown/intro-bayesian#example-coin-toss-experiment","position":23},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl2":"Bayesian Linear Regression"},"type":"lvl2","url":"/markdown/intro-bayesian#bayesian-linear-regression","position":24},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl2":"Bayesian Linear Regression"},"content":"","type":"content","url":"/markdown/intro-bayesian#bayesian-linear-regression","position":25},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl2":"Probabilistic Graphical Models"},"type":"lvl2","url":"/markdown/intro-bayesian#probabilistic-graphical-models","position":26},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl2":"Probabilistic Graphical Models"},"content":"","type":"content","url":"/markdown/intro-bayesian#probabilistic-graphical-models","position":27},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl2":"Latent variable models"},"type":"lvl2","url":"/markdown/intro-bayesian#latent-variable-models","position":28},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl2":"Latent variable models"},"content":"So far we have been working with probabilistic models where all variables are observable. Latent variable models are probabilistic models where some of the variables cannot be measured or can only be partially measured, i.e. there is missing data in some of the records of the dataset.\n\nIn both cases, formally we define latent models as a probability distribution over two sets of variables:p({\\rm x}, {\\rm z}| \\theta)\n\nwhere {\\rm x} are the set of observable variables and {\\rm z} the latent ones. \\theta are the parameters of the probability distribution, which we state explicitly for convenience later.","type":"content","url":"/markdown/intro-bayesian#latent-variable-models","position":29},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl3":"Partially observable latent variable models","lvl2":"Latent variable models"},"type":"lvl3","url":"/markdown/intro-bayesian#partially-observable-latent-variable-models","position":30},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl3":"Partially observable latent variable models","lvl2":"Latent variable models"},"content":"As mentioned above, in this case we are dealing with a problem of missing data, where some of the data points of a set of variables have not been recorded for potentially various reasons. Rubin \n\nLittle & Rubin, 2019 distinguishes between tree different situations regarding the generative model (i.e. the full probability distribution) for missing data:\n\nMissing Completely at Random (MCAR): there is no pattern in the missing data. The missing data is uncorrelated with observed and unobserved data. For instance, if some trades in a booking database are missing due to corruption of the physical support.\n\nMissing at Random (MAR): in this case there is pattern in the missing data that can be explained using observed data. Coming back to the example of a trades database, if the missing data is due to a particular sales person that tends to forget to register a voice operation in the systems\n\nMissing Not at Random (MNAR): in this case there is also a pattern in the missing data, but can only be explained using unobserved data (e.g. full latent variables). In the example of the booking database, if the issue comes from a certain sales person as in MAR, but the booking database does not record in any case the sales person identity.\n\nThe use of probabilistic methods for missing data is called multiple imputation and is described extensively in \n\nLittle & Rubin, 2019. The idea is essentially to estimate the model from the observed data using the techniques we will explore below in this section, and use it as a generative model for the unobserved variables, namely computing:p({\\rm x_{missing}} | {\\rm x_{observed}}, \\theta)\n\nImputation of missing data can then be done using some statistic of the distribution like the mean or the median, or generating random samples from the distribution.\n\nMultiple imputation works well for MCAR and reasonably (with some negligible bias) for MAR. The case of MNAR is more complicated, and can only be properly dealt with by introducing full latent variables that properly capture the MNAR mechanism, which is difficult if there is not a prior knowledge of the missing data mechanism.","type":"content","url":"/markdown/intro-bayesian#partially-observable-latent-variable-models","position":31},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl3":"Full latent variable models","lvl2":"Latent variable models"},"type":"lvl3","url":"/markdown/intro-bayesian#full-latent-variable-models","position":32},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl3":"Full latent variable models","lvl2":"Latent variable models"},"content":"In this case, there are not available observations of the latent variables. Models with full latent variables are closely related to probabilistic graphical models, since in many situations latent variables are introduced as prior knowledge in the structure of a model about a certain process, particularly when trying to model causal relationships. For instance, a model of two variables that are correlated but we believe not to be directly causally related, can be naturally extended by introducing a latent variable, a confounder, that influences both.\n\nWe can distinguish between two practical cases where latent variable models naturally emerge:\n\nOn the one hand, in problems where we know of the existence of a relevant variable, but it has not been observed and recorded in the data gathering process. An example, for instance, in the problem of demand estimation where only trades are observed, are the total interests or requests from clients to trade, that only in a fraction of cases (the hit & miss) generate a trade.\n\nOn the other hand, a latent variable can be an abstraction that cannot be directly measured but it might represent a common influence or effective factor.\n\nOne example is that of a market regime, which is modelled as a categorical variable with a set of states (e.g. high volatility and low volatility), and influences the price returns. Hidden Markov Models are latent variable models frequently used to model the effect of so-called regime switch of the statistics of price returns. They are typically applied as signal generators that alert of changes in market behavior.\n\nAnother example is the concept of mid or fair price. In real markets price information comes from trades, RfQs, limit order books, composite prices and so on. But the concept of mid / fair price of an instrument is unobservable. Kalman filters are typically used to infer the value of the mid-price from price informative observations.","type":"content","url":"/markdown/intro-bayesian#full-latent-variable-models","position":33},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl3":"Examples of Latent Variable Models","lvl2":"Latent variable models"},"type":"lvl3","url":"/markdown/intro-bayesian#examples-of-latent-variable-models","position":34},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl3":"Examples of Latent Variable Models","lvl2":"Latent variable models"},"content":"","type":"content","url":"/markdown/intro-bayesian#examples-of-latent-variable-models","position":35},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl4":"Gaussian Mixture Models (GMMs)","lvl3":"Examples of Latent Variable Models","lvl2":"Latent variable models"},"type":"lvl4","url":"/markdown/intro-bayesian#gaussian-mixture-models-gmms","position":36},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl4":"Gaussian Mixture Models (GMMs)","lvl3":"Examples of Latent Variable Models","lvl2":"Latent variable models"},"content":"The Gaussian Mixture Model postulates that data observations \\vec{x}_n, n = 1, ..., N are generated by K independent multivariate Gaussian distributions with unknown parameters \\vec{\\mu}_k, \\Sigma_k. The membership of each data point to a particular Gaussian is determined by latent categorical random variables z_n = {1, ..., K} with probabilities \\pi_k. Gaussian Mixture models are used to model unobserved subpopulations or segments from a given population. The probability distribution is therefore given by:p(\\vec{x_n}) = \\sum_{k=1}^K \\pi_k {\\mathcal N}(\\vec{x_n}|\\vec{\\mu}_k, \\Sigma_k)\n\nwhich is a linear combination of Gaussian distributions, hence the name mixture of Gaussians. Mixture models are actually more general in that other distributions can be used to model the mixture components, namely a Binomial, Dirichlet, Poisson, Exponential, etc.\n\nAs a graphical model, the so-called plate notation is usually employed to simplify the representation of the N random variables \\vec{x_n} and their associated K latent random variables z_n categorizing their mixture component membership. The following figure represents the mode in plate notation:\n\n\n\nFigure 5:Gaussian Mixture Model in plate notation.\n\nThe joint probability distribution of the model, including the latent variables, is the following:P(\\vec{x_n}, z_n) = \\sum_{k=1}^K \\pi_k^{\\delta_{z_n,k}} {\\mathcal N}(\\vec{x_n}|\\vec{\\mu}_k, \\Sigma_k)\n\nwhere \\delta_{z_n,k} is an indicator function: \\delta_{z_n,k} = 1 if  z_n = k, 0 otherwise\n\nGaussian mixture models have plenty of real-life applications. The most popular probably is its use as a clustering technique, and actually the most popular clustering technique itself, K-Means, is a particular case of the Gaussian Mixture model for spherical distributions, \\Sigma_k = \\epsilon I, in the limit \\epsilon \\rightarrow 0, when it is calibrated using the Expectation Maximization (EM) algorithm (see section below).\n\nAnother interesting application is anomaly detection, particularly for one-dimensional data. If we have theoretical reasons to expect the distribution of data to be a Gaussian, we can use a mixture of two Gaussians\ndistributions with the same mean but different standard deviations to capture anomalies from normality:p(x_n) =  \\pi_G {\\mathcal N}(x_n|\\mu, \\sigma_G) + (1-\\pi_G) {\\mathcal N}(x_n|\\mu, \\sigma_B)\n\nwhere G and B stand for good and bad, respectively, since the model is called the Good and Bad data model \n\nSilvia, 2006. After learning the parameters of the model using our data (\\pi_G, \\mu, \\sigma_G, \\sigma_G), and making by definition \\sigma_B \\geq \\sigma_G, we classify as anomalies those points in the dataset that are more likely to have been generated by the “bad” data Gaussian. This is done by inferring the probability of being “bad data” using Bayes’ theorem:p(\\text{bad}|x_n) = \\frac{p(x_n|\\text{bad}) p(\\text{bad})}{p(x_n)} = \\frac{ {\\mathcal N}(x_n|\\mu,\\sigma_𝐵) (1-\\pi_G)}{\\pi_G  {\\mathcal N}(x_n|\\mu,\\sigma_𝐺)+(1−\\pi_G)  {\\mathcal N}(x_n|\\mu,\\sigma_B)}\n\nand classifying as “bad data” or anomalies those points whose probability exceeds a given threshold, typically 0.5. This anomaly detection model works surprisingly well highlighting anomalies in data that humans could also recognize visually, making it a good candidate for automating human-driven anomaly detections.","type":"content","url":"/markdown/intro-bayesian#gaussian-mixture-models-gmms","position":37},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl4":"Hidden Markov Model (HMM)","lvl3":"Examples of Latent Variable Models","lvl2":"Latent variable models"},"type":"lvl4","url":"/markdown/intro-bayesian#hidden-markov-model-hmm","position":38},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl4":"Hidden Markov Model (HMM)","lvl3":"Examples of Latent Variable Models","lvl2":"Latent variable models"},"content":"A Hidden Markov Model explains a sequence of observations x_1, ..., x_T as driven by a hidden variable (the state) y_1, ..., y_T that has Markov dynamics:P(y_{t+1}|y_t, ..., y_1) =  P(y_{t+1}|y_t)\n\nwhere the latter is called the transition probability, which usually is assumed stationary in the model:P(y_{t+s+1}|y_{t+s})=P(y_{t+1}|y_t)\n\nThe observations only depend on the state of the hidden variable at the observation time:P(x_t|y_t, ..., y_1, x_{t-1}, ..., x_1) = P(x_t|y_t)\n\nThe joint probability distribution of the sequence therefore simplifies to:P(x_1, ..., x_N, y_1, ..., y_N)=P(y_1) P(x_1|y_1)\\Pi_{t=2}^T P(y_t|y_{t-1})P(x_t | y_t)\n\nAs a graphical model, the HMM has the following structure:\n\n\n\nFigure 6:Graph representation of the Hidden Markov Model\n\nThe hidden (latent) and observed variables can be either discrete or continuous. In many applications of HMMs, the latent variable is discrete. In this case, learning and inference is more tractable for general distributions. An example in the financial markets is explaining price returns as generated from a Gaussian noise whose volatility depends on a discrete hidden state, which is interpreted as a market or volatility regime.\n\nA typical question when studying HMMs is to infer the most likely sequency of hidden states given a time-series of observations. There are specialized algorithms to do this task efficiently, the most popular one being the Viterbi algorithm \n\nMurphy, 2013. For learning, a special case of the Expectation Maximization algorithms (EM) is used, called the Baum - Welch algorithm.","type":"content","url":"/markdown/intro-bayesian#hidden-markov-model-hmm","position":39},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl4":"The Kalman Filter","lvl3":"Examples of Latent Variable Models","lvl2":"Latent variable models"},"type":"lvl4","url":"/markdown/intro-bayesian#the-kalman-filter","position":40},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl4":"The Kalman Filter","lvl3":"Examples of Latent Variable Models","lvl2":"Latent variable models"},"content":"A Kalman filter is a particular instance of a filtering algorithm first proposed by Rudolf E. Kalman in 1960 in his seminal paper “A New Approach to Linear Filtering and Prediction Problems” \n\nKalman, 1960. It is a particular case of a Hidden Markov Model where both the hidden and observed variables are continuous and follow Gaussian distributions. In this simple case, inference and learning becomes tractable, making it a very efficient estimation algorithm. Actually, the term Kalman Filter applies rigorously to an estimation algorithm that takes a series of noisy observations and produces  estimations of the underlying variable by combining its previous estimate and the current observation. In order to be optimal, transition and observation noises have to be Gaussian.\n\nThe model in its graphical form is the same as shown in the HMM, being a particular case of this one. Mathematically, and considering for generality a vector of observations and hidden states, it can be written as:\\vec{y}_{t+1} = \\Phi \\vec{y}_t + \\vec{w}_t,  \\vec{w}_t \\sim {\\mathcal N}(0, Q)\\vec{x}_t = H \\vec{y}_t + \\vec{v}_t, \\vec{v}_t \\sim {\\mathcal N}(0, R)\n\nThe Kalman Filter is a recursive algorithm to estimate the hidden state value given observations, using two steps: the prediction and the update. First, using the best estimate at t-1, we predict the value of the hidden state at t and its covariance P:\\vec{y}_{t|t-1} = \\Phi \\vec{y}_{t-1|t-1}P_{t|t-1} = \\Phi P_{t-1|t-1} \\Phi^t + Q\n\nwhere we use the suffix \\{t|t-1\\} to distinguish between the optimal estimations and the underlying random variables. Then, given an observation at t, we update the estimation as:\\vec{y}_{t|t} = \\vec{y}_{t|t-1} + K_t (\\vec{x}_t - H\\vec{y}_{t|t-1} ) = (I-K_t H) \\vec{y}_{t|t-1} + K_t \\vec{x}_tP_{t|t} = (1-K_t H) P_{t|t-1}K_t =  P_{t|t-1} H ( H P_{t|t-1} H^t + R)^{-1}\n\nwhere K_t is called the Kalman gain. It controls the impact of the observation on the final estimate: if K_t = 0, then the final estimation is purely the prediction, whereas for K_t = I, the estimation is fully determined by the observation.\n\nThese so-called filtering equations can be derived using probability theory, computing the posterior distributions under the information available. For the predict step, this means computing:P(\\vec{y}_{t+1} | \\vec{x}_{0:t})\n\nwhere \\vec{x}_{0:t} denotes observations up to t. For the update state, we compute:P(\\vec{y}_{t+1} | \\vec{x}_{0:t+1})\n\ni.e. now we include the observation at time t+1 to update the estimation of the latent variables.\n\nThe filtering equations are central to Kalman filtering, as in most applications the goal is to estimate the most recent values of the latent variables given all available information up to the current time. However, there are situations where we may want to refine estimates of latent variables at earlier times by incorporating all available information, including observations collected after the time of interest. In such cases, the quantity of interest isP(\\vec{y}_{t} \\mid \\vec{x}_{0:T})\n\nwhere information up to time T is taken into account. The equations derived for this inference are known as smoothing equations, since the refinement produces a smoother estimate of the latent variables across time.\n\nIn the next subsection, we will derive both the filtering and the smoothing equations computing these probability distributions for a simple instance of the Kalman filter: the local level model.\n\nKalman filters have many applications in multiple domains. In financial markets, they can be used to combine different noisy observations of spot prices to infer a consistent estimate of a fair or mid price, to be used typically in the market-making of relatively illiquid instruments like bonds or some derivatives \n\nSinclair, 2010. Another use case is to infer prices of instruments when markets are closed, which can be useful for hedging indexes that have components with different time zones. In reference \n\nJavaheri et al., 2003, they have been applied to the estimation of term structure models. And in the context of investment strategies, they have a relatively popular application to improve pairs trading strategies \n\nChan, 2013. We will delve into some of these applications later in this book.","type":"content","url":"/markdown/intro-bayesian#the-kalman-filter","position":41},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl5":"The local level model","lvl4":"The Kalman Filter","lvl3":"Examples of Latent Variable Models","lvl2":"Latent variable models"},"type":"lvl5","url":"/markdown/intro-bayesian#the-local-level-model","position":42},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl5":"The local level model","lvl4":"The Kalman Filter","lvl3":"Examples of Latent Variable Models","lvl2":"Latent variable models"},"content":"A simple tractable model that illustrates well the general theory of the Kalman filter is the local level model, which despite its simplicity can have relevant applications, for instance in pricing. The model has the structure:y_{t+1} = y_t + w_t, w_t \\sim {\\mathcal N}(0, \\sigma_w^2)x_t = y_t + v_t, v_t \\sim {\\mathcal N}(0, \\sigma_v^2)\n\nDerivation of the forward filtering equations: predict and update\n\nThe predict equation can be derived by computing the  distribution of y_{t+1} conditional to the previous observations, which we denote x_{0:t}:p(y_{t+1}|x_{0:t})\n\nWe can compute this probability by using the marginalization rule over the latent variable y_t:p(y_{t+1}|x_{0:t}) = \\int dy_t p(y_t|x_{0:t})p(y_{t+1}|y_t)\n\nwhere we have exploited the graphical structure of the model to simplify p(y_{t+1}|y_t, x_{0:t}) = p(y_{t+1}|y_t) = {\\mathcal N}(y_{t+1}| y_t, \\sigma_w^2), since y_t is the only parent of y_{t+1}. The proof is based on induction: let us assume that p(y_t|x_{0:t}) \\sim {\\mathcal N}(\\hat{y}_{t|t}, \\sigma_{t|t}^2), i.e. it is Gaussian with mean \\hat{y}_{t|t} and standard deviation \\sigma_{t|t}. p(y_{t+1}|x_{0:t}) is therefore the result of the convolution of two Gaussian distributions, which is itself a Gaussian distribution with:p(y_{t+1}|x_{0:t}) = {\\mathcal N}(y_{t+1}|\\hat{y}_{t|t}, \\sigma_{t|t}^2 +\\sigma_w^2)\n\nThis completes the derivation of the predict equations, given by:\\hat{y}_{t+1|t} = \\hat{y}_{t|t}\\sigma_{t+1|t}^2 = \\sigma_{t|t}^2 +\\sigma_w^2\n\nwhere, as a reminder to the reader, we use the notation t_1 | t_2 to mean a variable at time t_1 conditional to information up to t_2. We have not yet, though, completed the induction proof, since we have not proven why p(y_t|x_{0:t}) follows a Gaussian distribution.\n\nThis is directly linked to the derivation of the update equation, which corresponds to the following inference:p(y_{t+1}|x_{0:t+1})\n\nWe can write p(y_{t+1}|x_{0:t+1}) = p(y_{t+1}|x_{0:t}, x_{t+1}) and apply Bayes’ theorem on x_{t+1}:p(y_{t+1}|x_{0:t+1}) = \\frac{p(x_{t+1}|y_{t+1}) p(y_{t+1}|x_{0:t})}{p(x_{t+1}|x_{0:t})}\n\nWe readily recognize the probability distribution derived in the predict step:p(y_{t+1}|x_{0:t}) = {\\mathcal N}(y_{t+1}|\\hat{y}_{t|t}, \\sigma_{t|t}^2 +\\sigma_w^2) \\equiv {\\mathcal N}(y_{t+1}|\\hat{y}_{t+1|t}, \\sigma_{t+1|t}^2)\n\nMoreover, from the model definition we have:p(x_{t+1}|y_{t+1}) = {\\mathcal N}(x_{t+1}|y_{t+1}, \\sigma_v^2)\n\nThis is the normalized product of two Gaussian density functions, which is also a Gaussian density function, since we can complete the square:\\frac{1}{2\\sigma_{t+1|t}^2}(y_{t+1} - \\hat{y}_{t+1|t})^2 + \\frac{1}{2\\sigma_v^2}(x_{t+1} - y_{t+1})^2 =\n\\frac{1}{2\\sigma_{t+1|t+1}^2}(y_{t+1} - \\hat{y}_{t+1|t+1})^2 + ...\n\nwhere we have omitted terms that are constant with respect to y_{t+1} and we have defined:\\hat{y}_{t+1|t+1} = \\sigma_{t+1|t+1}^2 (\\frac{\\hat{y}_{t+1|t}}{\\sigma_{t+1|t}^2} + \\frac{x_{t+1}}{\\sigma_v^2})\\sigma_{t+1|t+1}^2 = \\frac{\\sigma_v^2 \\sigma_{t+1|t}^2}{\\sigma_v^2 + \\sigma_{t+1|t}^2}\n\nIf we introduce the Kalman gain as:K_{t+1} =  \\frac{\\sigma_{t+1|t}^2}{\\sigma_v^2 + \\sigma_{t+1|t}^2}\n\nwe can write these equations as:\\hat{y}_{t+1|t+1} = \\hat{y}_{t+1|t} + K_{t+1}(x_{t+1} - \\hat{y}_{t+1|t})\\sigma_{t+1|t+1}^2 =  (1 - K_{t+1} )\\sigma_{t+1|t}^2\n\nwhich are the update equations of the Kalman filter, corresponding to the mean and variance of y_{t+1} conditional to observations up to t+1, which follows a Gaussian distribution:p(y_{t+1}|x_{0:t+1}) = {\\mathcal N}(y_{t+1}|\\hat{y}_{t+1|t+1}, \\sigma_{t+1|t+1}^2)\n\nAs a consequence, we have proven by induction that p(y_t|x_{0:t}) follows a Gaussian distribution, as long as the initial condition p(y_0) is also Gaussian.\n\nDerivation of the smoothing equations\n\nThe derivation of the smoothing equations follows the same lines as the forward filtering equations. In this case, we are interested in computing:p(y_t|x_{0:T})\n\ni.e., we seek to infer the distribution of the latent variable at 0 \\leq t \\leq T. We can exploit the structure of the graphical model to compute this expression by conditioning on y_{t+1}:p(y_t|x_{0:T}) = \\int d y_{t+1} p(y_t|y_{t+1}, x_{0:T}) p(y_{t+1}|x_{0:T}) = \\int d y_{t+1} p(y_t|y_{t+1}, x_{0:t}) p(y_{t+1}|x_{0:T})\n\nWhere we have used, in the second step, the fact that conditioning on y_{t+1} removes the dependence on x_{t+1:T}.\n\nWe assume that we have already done the forward filter path, so we have computed the distributions p(y_t|x_{0:t}) \\sim {\\mathcal N}(\\hat{y}_{t|t}, \\sigma^2_{t|t}) and p(y_{t+1}|x_{0:t}) \\sim {\\mathcal N}(\\hat{y}_{t+1|t}, \\sigma^2_{t+1|t}). In particular, notice that for t = T, the filtering and smoothing inferences are the same, given by p(y_{T}|x_{0:T}).\n\nTherefore, we will derive the smoothing equations backwards starting from p(y_{T}|x_{0:T}) \\sim {\\mathcal N}(\\hat{y}_{T|T}, \\sigma^2_{T|T}) as the initial condition. The key idea is to assume that we already have computed:p(y_{t+1}|x_{0:T}) \\sim {\\mathcal N}(\\hat{y}_{t+1|T}, \\sigma^2_{t+1|T})\n\nand use it to compute p(y_t|x_{0:T}). This means to compute the previous integral which depends on p(y_{t+1}|x_{0:T}) which we have already computed, and p(y_t|y_{t+1}, x_{0:t}), which we need to derive. We use the standard trick of computing the join probability p(y_t, y_{t+1}|x_{0:t}). Since y_{t+1} = y_t + w_t, where y_t and w_t are uncorrelated, this is simply:p(y_t, y_{t+1}|x_{0:t}) \\sim {\\mathcal N} \\left( \\left[ \\begin{matrix} \\hat{y}_{t|t} \\\\ \\hat{y}_{t+1|t} \\end{matrix} \\right], \\left[ \\begin{matrix} \\sigma^2_{t|t} & \\sigma^2_{t|t} \\\\ \\sigma^2_{t|t} & \\sigma^2_{t+1|t} \\end{matrix} \\right] \\right)\n\nwhere we have used that cov(y_t, y_{t+1}) = var(y_t) in this model. Now we can derive the conditional distribution using:p(y_t|y_{t+1}, x_{0:t}) = \\frac{p(y_t, y_{t+1}|x_{0:t})}{p(y_{t+1}|x_{0:t})}\n\nComputing this expression we get:p(y_t|y_{t+1}, x_{0:t}) \\sim {\\mathcal N}\\left( \\hat{y}_{t|t}+ \\frac{\\sigma^2_{t|t}}{ \\sigma^2_{t+1|t}}(y_t - \\hat{y}_{t+1|t}), \\sigma^2_{t|t}(1 - \\frac{\\sigma^2_{t|t}}{\\sigma^2_{t+1|t}})\\right)\n\nNow we have the expression for the two probabilities involved in the calculation of p(y_t|x_{0:T}), which can be carried by completing the squares on y_{t+1} and integrating it away. The final result is, unsurprisingly, a Gaussian distribution with the following backward iterative equations for the mean and variance, which conform the Rauch - Tung - Striebel smoothing equations:\\hat{y}_{t|T} = \\hat{y}_{t|t} + \\frac{\\sigma^2_{t|t}}{ \\sigma^2_{t+1|t}}\\left(\\hat{y}_{t+1|T} - \\hat{y}_{t+1|t}\\right)\\sigma^2_{t|T} = \\sigma^2_{t|t} + (\\frac{\\sigma^2_{t|t}}{ \\sigma^2_{t+1|t}})^2(\\sigma^2_{t+1|T} - \\sigma^2_{t+1|t})\n\nAs mentioned, we compute first the forward filtering equations until T, which are used to initialize the smoothing equations and proceed backwards. The smoothing equations provide a better estimation than the filtering ones, since they use the full information set up to T. This translates into smaller estimation variances. They cannot be used in an online context, though, which is where Kalman filters find a large number of applications. Their main application is parameter estimation, since they are required to compute the likelihood of the data when using maximum likelihood estimation.","type":"content","url":"/markdown/intro-bayesian#the-local-level-model","position":43},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl3":"Estimation of Latent Variable Models","lvl2":"Latent variable models"},"type":"lvl3","url":"/markdown/intro-bayesian#estimation-of-latent-variable-models","position":44},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl3":"Estimation of Latent Variable Models","lvl2":"Latent variable models"},"content":"Given observations and a latent variables model  typically represented as a probabilistic graphical model, we would like to estimate the parameters of the model. The difficulty, here, comes from the latent variables, for which we don’t have observations. There are different ways to estimate parameters in this case, but we will focus on the most popular ones: maximum likelihood estimation (MLE) and expectation maximization (EM), which computes an approximation of the MLE solution.","type":"content","url":"/markdown/intro-bayesian#estimation-of-latent-variable-models","position":45},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl4":"Maximum Likelihood Estimation (MLE)","lvl3":"Estimation of Latent Variable Models","lvl2":"Latent variable models"},"type":"lvl4","url":"/markdown/intro-bayesian#maximum-likelihood-estimation-mle","position":46},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl4":"Maximum Likelihood Estimation (MLE)","lvl3":"Estimation of Latent Variable Models","lvl2":"Latent variable models"},"content":"The presence of latent variables does not preclude the use of MLE. Our goal is still to find the parameters that maximize the probability of the observations given the parameters (or likelihood). If we could observe the latent variables, that would mean maximizing the so-called (in the context of latent variable models) complete likelihood:\\Pi_{n=1}^N P({\\rm x}_n, {\\rm z}_n| \\theta)\n\nSine the latter are not observed, we need to integrate them out:\\sum_{z_1, ..., z_N} \\Pi_{n=1}^N P({\\rm x}_n, {\\rm z}_n| \\theta)\n\nwhere we have assumed discrete distributions for the latent variables. For continuous ones, the sums have to be replaced by integrals. The latter is called the incomplete likelihood.\n\nWhereas maximizing incomplete likelihoods with respect to the parameters is in theory possible, in practice it is usually a computationally expensive method. Therefore the need to develop more efficient estimation methods like Expectation Maximization.","type":"content","url":"/markdown/intro-bayesian#maximum-likelihood-estimation-mle","position":47},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl4":"Expectation Maximization (EM)","lvl3":"Estimation of Latent Variable Models","lvl2":"Latent variable models"},"type":"lvl4","url":"/markdown/intro-bayesian#expectation-maximization-em","position":48},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl4":"Expectation Maximization (EM)","lvl3":"Estimation of Latent Variable Models","lvl2":"Latent variable models"},"content":"If we were to have observations of the latent variables, we could then maximize the complete likelihood and get maximum likelihood estimators for the parameters \\theta. However, we don’t have those observations and we need to maximize the incomplete likelihood, which is a harder problem.\n\nThe intuition behind the Expectation Maximization (EM) method lies in this idea of maximizing the complete likelihood. Since we don’t have observations of the latent variables, EM seeks to maximize the incomplete likelihood breaking the problem in two easier steps:\n\nE-step: use observations and current best estimates of the parameters to infer values for the latent variables\n\nM-step: Plug these values into the complete likelihood and maximize it to get new estimations for the parameters\nThese steps are repeated until convergence.\n\nMathematically, EM does not guarantee to find a global optimum for the incomplete likelihood, but under certain general conditions it can be shown that the EM solution is a lower bound for the global maximum.\n\nLet us now look at the algorithm in more detail. We start with the E-step. The best estimate for the hidden variables given the latest estimation of parameters \\theta^{(s)} and observations is given by the following conditional distribution:P({\\rm z}|\\{{\\rm x_n}\\},\\theta^{s})\n\nIt is called E-step because it computes the expectation of the complete (log)likelihood (the log to make it easier to compute) using this distribution, where the complete log-likelihood has yet unknown parameters \\theta, which we denote:Q(\\theta|\\theta^{(s)}) \\equiv \\mathbb{E}_s[\\log \\Pi^N_{n=1}P(x_n,z_n|\\theta)]=  \\sum_n \\sum_{z_n} P({\\rm z_n}|{\\rm x_n},\\theta_{t})\\log P(x_n,z_n|\\theta)\n\nwhere we have introduced the compact notation \\mathbb{E}_s[...] = \\mathbb{E}[...|\\{{\\rm x_n}\\},\\theta^{s}].\n\nThe M-step then maximizes this function with respect to \\theta, which become \\theta^{(s+1)} in the iterative algorithm.\n\nThe sketch to prove that the EM solution is a lower bound of the maximum incomplete likelihood is relatively easy, so we do it in the follow. We start from the incomplete log-likelihood:\\log \\Pi_{n=1}^N P({\\rm x}_n| \\theta) = \\sum_{n=1}^N \\log \\sum_{z_n}  P({\\rm x}_n, {\\rm z}_n| \\theta) = \\sum_{n=1}^N  \\log \\sum_{z_n} q({\\rm z_n}|{\\rm x}_n) \\frac{P({\\rm x}_n, {\\rm z}_n| \\theta)}{q({\\rm z_n}|{\\rm x}_n)}\n\nwhere q(z_n|x_n) is so far a generic distribution. Now since log is a concave function, we can use Jensen’s inequality, whose general form is:f(\\sum_n \\lambda_n x_n) \\geq \\sum_n \\lambda_n f(x_n)\n\nwhere \\lambda_n \\geq 0 and \\sum_n \\lambda_n = 1. Applied to our case, where \\lambda_n = q({\\rm z_n}|{\\rm x}_n):\\sum_{n=1}^N  \\log \\sum_{z_n} q({\\rm z_n}|{\\rm x}_n) \\frac{P({\\rm x}_n, {\\rm z}_n| \\theta)}{q({\\rm z_n}|{\\rm x}_n)} \\geq \\sum_{n=1}^N \\sum_{z_n} q({\\rm z_n}|{\\rm x}_n) \\log \\frac{P({\\rm x}_n, {\\rm z}_n| \\theta)}{q({\\rm z_n}|{\\rm x}_n)} \\equiv F(q,\\theta)\n\nwith F(q,\\theta) is called the free energy in analogy of Statistical Mechanics. The free energy is therefore a lower bound of the incomplete likelihood:\\log \\Pi_{n=1}^N P({\\rm x}_n| \\theta)  \\geq F(q,\\theta)\n\nIf we maximized the free energy we are optimizing a lower bound of the incomplete likelihood, which is essentially what we claim EM does. EM maximizes the free energy using coordinate gradient ascent, i.e. iteratively optimizing F in q and \\theta:\n\nE-step: q_{s+1} = {\\rm argmax}_q F(q, \\theta^{(s)})\n\nM-step: \\theta_{s+1} = {\\rm argmax}_\\theta F(q_{t+1}, \\theta)\n\nTo finish the proof, we only need to find the solution for the E-step. A simple proof consists in first noticing that the inequality also holds for \\theta = \\theta^{(s)}:\\log \\Pi_{n=1}^N P({\\rm x}_n| \\theta^{(s)})  \\geq F(q,\\theta^{(s)})\n\nNow let us take q_{s+1} = P(z_n | x_n, \\theta^{(s)}). Plugging it in into the free energy:\\sum_{n=1}^N \\sum_{z_n} P(z_n | x_n, \\theta^{(s)}) \\log \\frac{P({\\rm x}_n, {\\rm z}_n| \\theta^{(s)})}{P(z_n | x_n, \\theta^{(s)})}= \\sum_{n=1}^N (\\sum_{z_n} P(z_n | x_n, \\theta^{(s)})) \\log P({\\rm x}_n | \\theta^{(s)})= \\sum_{n=1}^N \\log P({\\rm x}_n | \\theta^{(s)}) = \\log \\Pi_{n=1}^N P({\\rm x}_n| \\theta^{(s)})\n\nwhich makes the inequality an equality, therefore maximizing the free energy!\n\nFor the M-step, we just need to sustitute q_{s+1} into the free energy:F(q_{s+1}, \\theta) = \\sum_{n=1}^N \\sum_{z_n} P(z_n | x_n, \\theta^{(s)}) \\log \\frac{P({\\rm x}_n, {\\rm z}_n| \\theta)}{P(z_n | x_n, \\theta^{(s)})}=\\sum_{n=1}^N \\sum_{z_n} P(z_n | x_n, \\theta^{(s)}) \\log P({\\rm x}_n, {\\rm z}_n| \\theta) - \\sum_{n=1}^N \\sum_{z_n} P(z_n | x_n, \\theta^{(s)}) \\log P(z_n | x_n, \\theta^{(s)})= Q(\\theta | \\theta^{(s)}) + H_{q_{s+1}}\n\nwhere the second term, H_{q_{s+1}}, the entropy of q_{s+1}, does not depend on \\theta. Therefore the maximum of this free energy wrt to \\theta is the same as the maximum of Q:\\theta_{s+1} = {\\rm argmax}_\\theta Q(\\theta|\\theta^{(s)})\n\ncompleting the proof.\n\nThe algorithm is run until some form of convergence is reached (or a maximum number of iterations). Two different criteria are used typically to evaluate convergence:\n\nConvergence in the incomplete log-likelihood l(\\theta^{(s)}), which we now from the previous proof that has to increase always on each EM iteration. The only issue is that potentially might be computationally expensive to evaluate, since it requires the integration over the latent variables.\n\nConvergence in parameters, meaning that two subsequent set of parameters are close enough using some distance metric, for instance Euclidean distance:||\\theta^{(s+1)} - \\theta^(s)||_2 <= \\text{threshold}\n\nIn the following examples we will see examples of a general rule when using EM, namely that the estimators of the parameters \\theta obtained in the M-step are the ones that we would get by optimizing the incomplete likelihood but with the values of the latent variables replaced by their expectations under P(z_n | x_n, \\theta^{(s)})","type":"content","url":"/markdown/intro-bayesian#expectation-maximization-em","position":49},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl5":"Example 1: Gaussian Mixture Model","lvl4":"Expectation Maximization (EM)","lvl3":"Estimation of Latent Variable Models","lvl2":"Latent variable models"},"type":"lvl5","url":"/markdown/intro-bayesian#example-1-gaussian-mixture-model","position":50},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl5":"Example 1: Gaussian Mixture Model","lvl4":"Expectation Maximization (EM)","lvl3":"Estimation of Latent Variable Models","lvl2":"Latent variable models"},"content":"As a first example we derive EM for the Gaussian Mixture Model (GMM). The complete log-likelihood for this model reads:\\log \\Pi_n P(\\vec{x_n}, z_n|\\theta) = \\log \\Pi_n \\Pi_{k=1}^K \\left(\\pi_k {\\mathcal N}(\\vec{x_n}|\\vec{\\mu}_k, \\Sigma_k)\\right)^{\\delta_{z_n,k}} =  \\sum_n \\sum_{k=1}^K  \\delta_{z_n,k} \\left( \\log \\pi_k + \\log {\\mathcal N}(\\vec{x_n}|\\vec{\\mu}_k, \\Sigma_k)\\right)\n\nThe set of parameters \\theta of the model are the segment probabilities \\pi_k and Gaussian parameters \\vec{\\mu}_k, \\Sigma_k. Given observations \\vec{x_n} and a current estimation of parameters \\theta^{(s)}, where s is the index for the current EM iteration, we infer the distribution of the latent variables (E-step):\\gamma_{n,k}^{(s)} \\equiv P(z_n=k|\\vec{x}_n, \\theta^{(s)}) = \\frac{\\pi_k^s {\\mathcal N}(\\vec{x_n}|\\vec{\\mu}_k^s, \\Sigma_k^s)}{\\sum_{k'=1}^K \\pi_{k'}^s {\\mathcal N}(\\vec{x_n}|\\vec{\\mu}_{k'}^s, \\Sigma_{k'}^s)}\n\nwhere we have used Bayes theorem. Now we write the expected log-likelihood with respect to this distribution, which again for simplicity of notation we denote as \\mathbb{E}_s[...]:Q(\\theta | \\theta^{(s)}) = \\mathbb{E}_s\\left[\\log \\Pi_n P(\\vec{x_n}, z_n|\\theta)\\right] =  \\sum_n \\sum_{k=1}^K \\mathbb{E}_s\\left[\\delta_{z_n,k}\\right] \\left( \\log \\pi_k + \\log {\\mathcal N}(\\vec{x_n}|\\vec{\\mu}_k, \\Sigma_k)\\right)=\\sum_n \\sum_{k=1}^K \\gamma_{n,k}^{(s)} \\left( \\log \\pi_k + \\log {\\mathcal N}(\\vec{x_n}|\\vec{\\mu}_k, \\Sigma_k)\\right)\n\nNow we apply the M-step, finding the estimators for the parameters that maximize the expected log-likelihood. For the case of \\pi_k we need to introduce the constraint \\sum_k \\pi_k = 1:\\max_{\\pi_k} Q(\\theta|\\theta^{(s)}) + \\lambda (\\sum_{k'} \\pi_{k'} -1) \\rightarrow \\pi_k^{s+1} = \\frac{1}{N} \\sum_{n=1}^N \\gamma_{n,k}^{(s)}\n\nSimilarly we can find the estimators for the Gaussian parameters:\\vec{\\mu}_k^{s+1} = \\frac{1}{N \\pi_k^{s+1}} \\sum_{n=1}^N \\gamma_{n,k}^{(s)} \\vec{x_n}\\Sigma_k^{s+1} = \\frac{1}{N \\pi_k^{s+1}} \\sum_{n=1}^N \\gamma_{n,k}^{(s)} (\\vec{x_n} - \\vec{\\mu}_k^{s+1})(\\vec{x_n} - \\vec{\\mu}_k^{s+1})^t\n\nFrom these estimators, we can easily verify that indeed they are of the form of the estimators we would obtain for the complete log-likelihood, but with the hidden variables replaced by their expectations under the posterior P(z_n=k|\\vec{x}_n, \\theta^{(s)}). Another interesting point to remark is how the EM algorithm for the GMM is essentially a “soft” version of K-means, where instead of having hard assignments to the cluster with the nearest centroid, we have soft assignments to each of the Gaussian distributions. We left to the student the task, as an exercise, to show how EM converges to K-means in the particular case of spherical distributions \\Sigma_k = \\epsilon I in the limit \\epsilon \\rightarrow 0.\n\nLet us now illustrate the algorithm in practice. We apply the Good and Bad Data Model (GBDM) to detect anomalies in the daily returns of BBVA stock.\n\nThe intuition behind GBDM is that, under normal market conditions, price returns approximately follow a Gaussian distribution, consistent with the Geometric Brownian Motion (GBM) model for stock prices first proposed by Samuelson. Yet, it is well established that financial markets can experience episodes of extreme stress in which returns deviate sharply from the predictions of GBM—for example, during the COVID-19 confinement, the Brexit referendum, or the first election of Donald Trump. The GBDM captures these empirical features by combining two components: a Gaussian distribution representing the good regime of normal market behavior, and a second Gaussian distribution accounting for the bad regime of stressed conditions. In this way, the model provides a simple but powerful generative framework for distinguishing between typical fluctuations and anomalous events in financial time series.\n\nSpecifically, we consider a fixed 10-year window, compute daily returns, and fit the model to this time series using the Expectation–Maximization (EM) algorithm. For initialization, we set the mean of the historical returns as the common mean of both the good and bad data components. The standard deviation of the good data distribution is initialized with the historical standard deviation of returns, while the bad data distribution is initialized with twice this value. As a prior probability, we assume that 95% of observations belong to the good regime. Importantly, the final results are quite robust to these initialization choices, which suggests that the model reliably captures the distinction between normal and anomalous returns.\n\nWe check that the EM algorithm behaves as expected, i.e. the incomplete log-likelihood always increases with each iteration of the algorithm, and it reaches convergence. This is shown in the following figure:\n\n\n\nFigure 7:Log-likelihood of the GBDM calculated with the parameters obtained at each iteration. As expected from the theoretical formulation of the algorithm, the log-likelihood cannot decrease at any iteration of EM.\n\nTo classify anomalies, we use a P(\\text{bad}|D) > 0.5 rule to classify data as abnormal. In the following two figures we can see the results of the model in both the histogram and time-series of daily returns of BBVA. The GBDM partitions the data in two clusters: one that captures normal financial conditions, and a second one that flags periods of stress as well as exuberance (abnormally high negative and positive returns, respectively). By inspection of the data in the accompanying notebook we can see that the model correctly classifies the aforementioned events, since it flags the 2020-03-16 (the first lock-down day in Spain), the 2016-06-24 (the first trading day after the Brexit referendum) and the 2016-11-09 (the first trading day after Trump’s election).\n\n\n\nFigure 8:Histogram of daily returns of BBVA’s stock over 10 years from 2015 to 2025. The GBDM model has been used to flag anomalous returns, which are depicted in red.\n\n\n\nFigure 9:Time-series of daily returns of BBVA’s stock over 10 years. Again, red points depict days that have been classified as having abnormally high and low returns.","type":"content","url":"/markdown/intro-bayesian#example-1-gaussian-mixture-model","position":51},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl5":"Example 2: Hidden Markov Model (the Baum - Welch algorithm)","lvl4":"Expectation Maximization (EM)","lvl3":"Estimation of Latent Variable Models","lvl2":"Latent variable models"},"type":"lvl5","url":"/markdown/intro-bayesian#example-2-hidden-markov-model-the-baum-welch-algorithm","position":52},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl5":"Example 2: Hidden Markov Model (the Baum - Welch algorithm)","lvl4":"Expectation Maximization (EM)","lvl3":"Estimation of Latent Variable Models","lvl2":"Latent variable models"},"content":"Let us turn now to Hidden Markov Models (HMMs). The complete log-likelihood looks:\\log P(y_{1}) P(x_{1}|y_{1})\\Pi_{t=2}^T P(y_{t}|y_{t-1})P(x_{t} | y_{t}) = \\left(\\log P(y_{1}) + \\log P(x_{1}|y_{1}) + \\sum_{t=2}  \\log \\left(P(y_{t}|y_{t-1}) + \\log P(x_{t} | y_{t})\\right) \\right)\n\nwhere t is the index for the time series observations. The parameters of the model \\theta are the ones defining the transition and observation probabilities. These are time-independent since the distributions are stationary. Given an estimation of the parameters \\theta^{(s)} (we use s for the EM iterations in this case), we can infer the latent variables of the model given the observations as:P(y_{1}, ..., y_{T}| x_{1}, ..., x_{T}, \\theta^{(s)})\n\nLet us consider discrete variables and introduce some convenient notation:a_{i,j} = P(y_{t} = Y_j|y_{t-1} = Y_i)\\pi_i = P(y_1 = Y_i)b_{k,i} = P(x_t = X_k|y_t = Y_i)\n\nUsing indicator functions:P(y_{1}) = \\sum_i \\pi_i^{\\delta_{y_{1}, Y_i}}P(y_{t}|y_{t-1}) = \\sum_{i,j} a_{i,j}^{\\delta_{y_{t}, Y_i} \\delta_{y_{t-1}, Y_j}}P(x_{t} | y_{t}) = \\sum_{k,i} b_{k,i}^{\\delta_{x_{t}, X_k}\\delta_{y_{t}, Y_i}}\n\nThe complete log-likelihood can be then written as:\\sum_i \\delta_{y_{1}, Y_i} \\log \\pi_i + \\sum_{k,i} \\delta_{x_{1}, X_k}\\delta_{y_{1}, Y_i} \\log b_{k,i} + \\sum_{t=2} \\left(\\sum_{i,j} \\delta_{y_{t}, Y_i} \\delta_{y_{t-1}, Y_i} \\log a_{i,j} + \\sum_{k,i} \\delta_{x_{t}, X_k}\\delta_{y_{t}, Y_i} \\log b_{k,i}\\right)\n\nWe calculate the expected log-likelihood:Q(\\theta|\\theta^{(s)}) = \\sum_i \\mathbb{E}_s[\\delta_{y_{1}, Y_i}]\\log \\pi_i + \\sum_{k,i} \\delta_{x_{1}, X_k}\\mathbb{E}_s[\\delta_{y_{1}, Y_i}] \\log b_{k,i} + \\sum_{t=2} \\left(\\sum_{i,j} \\mathbb{E}_s[\\delta_{y_{t}, Y_i} \\delta_{y_{t-1}, Y_j}] \\log a_{i,j} + \\sum_{k,i} \\delta_{x_{t}, X_k} \\mathbb{E}_s[\\delta_{y_{t}, Y_i}]\\log b_{k,i}\\right)\n\nwhere E_s is the expectation with respect to the distribution of latent variables at iteration s, shown above. We define:\\gamma_{t}^{s,i} \\equiv  E_s[\\delta_{y_{t}, Y_i}] = P(y_{t} = Y_i| x_{1}, ..., x_{T},\\theta^{(s)})\\chi_{t}^{s,i,j} \\equiv E_s[\\delta_{y_{t}, Y_i} \\delta_{y_{t-1}, Y_j}] = P(y_{t}= Y_i, y_{t-1}= Y_j| x_{1}, ..., x_{T},\\theta^{(s)})\n\nSubstituting it into the expected log-likelihood:Q(\\theta|\\theta^{(s)}) = \\sum_n \\left(\\sum_i \\gamma_{1,n}^{s,i} \\log \\pi_i + \\sum_{k,j} \\delta_{x_{1,n}, X_k}\\gamma_{t,n}^{s,i} \\log b_{k,i} + \\sum_{t=2} \\left(\\sum_{i,j} \\chi_{t,n}^{s,i,j}  \\log a_{i,j} + \\sum_{k,i} \\delta_{x_{t,n}, X_k}\\gamma_{t,n}^{s,i}\\log b_{k,i}\\right)\\right)\n\nNow we can maximize the function with respect to parameters \\pi_i, a_{i,j}, b_{k,i} to find the estimators at iteration s+1:\\pi^{s+1}_i =  \\gamma_{1}^{s,i}a_{i,j}^{s+1} = \\frac{\\sum_{t=1}^{T-1}\\chi_{t}^{s,i,j}}{\\sum_{t=1}^{T-1}\\gamma_{t}^{s,i}}b_{i,k}^{s+1} = \\frac{\\sum_{t=1}^T \\delta_{x_{t}, X_k} \\gamma_{t}^{s,i}}{\\sum_{t=1}^{T-1}\\gamma_{t}^{s,i}}\n\nIt remains to show how to calculate at each iteration the expectations from the E-step. This can be done by introducing a set of two helper probabilities. The forward probability:\\alpha_{i}^s(t) \\equiv P(x_{1} = X_{k_1}, ..., x_{t} = X_{k_t}, y_{t} = Y_i| \\theta^{(s)})\n\nwhich can be calculated recursively:\\alpha_{i}^s(1) = P(x_{1} = X_{k_1}, y_{1} = Y_i| \\theta^{(s)})= P(x_{1} = X_{k_1}| y_{1}=Y_i,  \\theta^{(s)}) P (y_{1} = Y_i| \\theta^{(s)}) = b_{k_1,i}^s \\pi_i^s\n\nand\\alpha_{i}^s(t+1) = P(x_{1} = X_{k_1}, ..., x_{t+1} = X_{k_{t+1}}, y_{t+1} = Y_i| \\theta^{(s)})= \\sum_{i'} P(x_{1} = X_{k_1}, ..., x_{t+1} = X_{k_{t+1}}, y_{t} = Y_{i'}, y_{t+1} = Y_i| \\theta^{(s)})= \\sum_{i'} P(x_{t+1} = X_{k_{t+1}}, y_{t+1} = Y_i| x_{1} = X_{k_1}, ..., x_{t} = X_{k_{t}}, y_{t} = Y_{i'}, \\theta^{(s)}) P(x_{1} = X_{k_1}, ..., x_{t} = X_{k_{t}}, y_{t} = Y_{i'}| \\theta^{(s)})= \\sum_{i'} P(x_{t+1} = X_{k_{t+1}}, y_{t+1} = Y_i| y_{t} = Y_{i'}, \\theta^{(s)}) \\alpha_{i'}^s(t)= \\sum_{i'} P(x_{t+1} = X_{k_{t+1}}| y_{t+1} = Y_i, \\theta^{(s)})P(y_{t+1} = Y_i | y_{t} = Y_{i'}, \\theta^{(s)}) \\alpha_{i'}^s(t)= b_{k_{t+1},i}^s \\sum_{i'} a_{i,i'}^s \\alpha_{i',n}^s(t)\n\nAnd the backward probability:\\beta_{i}^s(t) \\equiv P(x_{t+1} = X_{k_{t+1}}, ..., x_{T} = X_{k_T}| y_{t} = Y_i, \\theta^{(s)})\n\nAs an exercise, we let the student to derive the recursive formulae for its calculation:\\beta_{i}^s(T) = 1\\beta_{i}^s(t) = \\sum_{i'} \\beta_{i'}^s(t+1) a_{i',i}^s b_{k_{t+1},i'}^s\n\nWe can now write the probabilities of the latent variables as a function of these helper probabilities using Bayes’ theorem:\\gamma_{t}^{s,i}  = P(y_{t} = Y_i| x_{1}, ..., x_{T},\\theta^{(s)}) = \\frac{P(y_{t} = Y_i, x_{1}, ..., x_{T}|\\theta^{(s)})}{P(y_{t} = Y_i|\\theta^{(s)})}= \\frac{P(x_{t+1}, ..., x_{T}|y_{t} = Y_i,x_{1}, ..., x_{t}, \\theta^{(s)}) P(y_{t} = Y_i,x_{1}, ..., x_{t}| \\theta^{(s)})}{\\sum_{i'}P(y_{t} = Y_{i'}, x_{1}, ..., x_{T}|\\theta^{(s)})}= \\frac{\\alpha_{i}^s(t) \\beta_{i}^s(t)}{\\sum_{i'}\\alpha_{i'}^s(t) \\beta_{i'}^s(t)}\n\nand\\chi_{t}^{s,i,j} = P(y_{t}= Y_i, y_{t-1}= Y_j| x_{1}, ..., x_{T},\\theta^{(s)}) = \\frac{P(y_{t}= Y_i, y_{t-1}= Y_j, x_{1}, ..., x_{T}|\\theta^{(s)})}{P(x_{1}, ..., x_{T}|\\theta^{(s)})}= \\frac{P(x_{t}, ..., x_{T}| y_{t}= Y_i,  y_{t-1}= Y_j, x_{1}, ..., x_{t-1},\\theta^{(s)}) P(y_{t}= Y_i, y_{t-1}= Y_j, x_{1}, ..., x_{t-1}|\\theta^{(s)}}{P(x_{1}, ..., x_{T}|\\theta^{(s)})}= \\frac{P(x_{t}, ..., x_{T}| y_{t}= Y_i,\\theta^{(s)}) P(y_{t}= Y_i| y_{t-1}= Y_j, x_{1}, ..., x_{t-1},\\theta^{(s)})P(y_{t-1}= Y_j, x_{1}, ..., x_{t-1},\\theta^{(s)})}{P(x_{1}, ..., x_{T}|\\theta^{(s)}) }=\\frac{P(x_{t}|y_{t}= Y_i,\\theta^{(s)}) P(x_{t+1}, ..., x_{T}, y_{t}= Y_i,\\theta^{(s)}) a_{i,j}^s \\alpha_{j}^s(t-1)}{P(x_{1}, ..., x_{T}|\\theta^{(s)}) }= \\frac{b_{k_t, i}^s \\beta_{i}^s(t) a_{i,j}^s \\alpha_{j}^s(t-1)}{\\sum_{i',j'} b_{k_t, i'}^s \\beta_{i'}^s(t) a_{i',j'}^s \\alpha_{j'}^s(t-1)}\n\nThis completes all the ingredients need to perform EM in the Baum-Welch algorithm. Given the set of parameters \\theta^{(s)} we use the forward and backward probabilities to update the estimations of the latent variables (E-step) which then are plugged in the estimators of \\theta^{(s+1)} (M-step) and we iterate until convergence. The only thing needed are initial estimates of the parameters, \\theta^{(0)}, which could be random numbers or based on prior knowledge. A good choice of initialization can help to increase the speed of convergence.","type":"content","url":"/markdown/intro-bayesian#example-2-hidden-markov-model-the-baum-welch-algorithm","position":53},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl6":"Gaussian observation probabilities","lvl5":"Example 2: Hidden Markov Model (the Baum - Welch algorithm)","lvl4":"Expectation Maximization (EM)","lvl3":"Estimation of Latent Variable Models","lvl2":"Latent variable models"},"type":"lvl6","url":"/markdown/intro-bayesian#gaussian-observation-probabilities","position":54},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl6":"Gaussian observation probabilities","lvl5":"Example 2: Hidden Markov Model (the Baum - Welch algorithm)","lvl4":"Expectation Maximization (EM)","lvl3":"Estimation of Latent Variable Models","lvl2":"Latent variable models"},"content":"So far we have considered only discrete transition and observation probabilities. We can consider different probability models within the HMM framework and Baum - Welch EM learning. If the observations are continuous variables, we can model their conditional distributions as Gaussian:b_{i}(x_t) = P(x_t|y_t = Y_i) = {\\mathcal N}(x|\\mu_i, \\sigma_i)\n\nThe extension to multi-variate Gaussian should be easy to derive. The expected log-likelihood now reads:Q(\\theta|\\theta^{(s)}) = \\left(\\sum_i \\gamma_{1}^{s,i} \\log \\pi_i -\\frac{1}{2} \\sum_{j} \\gamma_{t}^{s,i} (\\frac{(x_{1} - \\mu_i)^2}{\\sigma_i^2} + \\log( 2\\pi \\sigma_i^2)) + \\sum_{t=2} \\left(\\sum_{i,j} \\chi_{t}^{s,i,j}  \\log a_{i,j} -\\frac{1}{2} \\sum_{i} \\gamma_{t}^{s,i}(\\frac{(x_{t} - \\mu_i)^2}{\\sigma_i^2} + \\log( 2\\pi \\sigma_i^2)) \\right)\\right)\n\nWe leave as an exercise to work the expressions for the EM estimators of the parameters of the Gaussian (the rest of estimators don’t change):\\mu_i^{(s+1)} = \\frac{ \\sum_{t=1}^T \\gamma_{t}^{s,i} x_{t}}{\\sum_{t=1}^{T}\\gamma_{t}^{s,i}}(\\sigma_i^{(s+1)})^2 = \\frac{\\sum_{t=1}^T \\gamma_{t}^{s,i} (x_{t} - \\mu_i^{(s+1)})^2}{\\sum_{t=1}^{T}\\gamma_{t}^{s,i}}\n\nwhich are similar to the GMM ones --in fact HMM models can be interpreted as time-series Gaussian Mixture Models.","type":"content","url":"/markdown/intro-bayesian#gaussian-observation-probabilities","position":55},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl5":"Example 3: Local Level Model (simple case of a Kalman Filter)","lvl4":"Expectation Maximization (EM)","lvl3":"Estimation of Latent Variable Models","lvl2":"Latent variable models"},"type":"lvl5","url":"/markdown/intro-bayesian#example-3-local-level-model-simple-case-of-a-kalman-filter","position":56},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl5":"Example 3: Local Level Model (simple case of a Kalman Filter)","lvl4":"Expectation Maximization (EM)","lvl3":"Estimation of Latent Variable Models","lvl2":"Latent variable models"},"content":"Recall that the local level model equations are given by:y_{t+1} = y_t + w_t, w_t \\sim {\\mathcal N}(0, \\sigma_w^2)x_t = y_t + v_t, v_t \\sim {\\mathcal N}(0, \\sigma_v^2)\n\nAssume we have observations x_{0:T}. The complete log-likelihood is similar to the one for the HMM, only now we exploit the properties of the Gaussian distributions:l= \\sum_{t=1}^T \\log P(x_t|y_t)P(y_t|y_{t-1}) = -\\sum_{t=1}^T \\left(\\frac{1}{2} \\log 2\\pi \\sigma_v^2 + \\frac{(x_t-y_t)^2 }{2\\sigma_v^2}+ \\frac{1}{2} \\log 2\\pi \\sigma_w^2 + \\frac{(y_{t+1}-y_t)^2}{2\\sigma_w^2} \\right)\n\nAs usual with EM we derive the iterative equations using induction: we assume we have already a set of parameters estimated at iteration s, \\theta^{(s)} =\\{\\sigma_v^{(s)}, \\sigma_w^{(s)}\\} and derive the updates for iteration s+1. To initialize the algorithm we need a parameter seed that can be based on general heuristics or prior information. We start with the E-step, where we compute the expectation of the log-likelihood using the best inference available on the latent variables at the current iteration, P(y_t|x_{0:T}, \\theta^{(s)}):Q(\\theta|\\theta^{(s)}) = -\\sum_{t=1}^T \\left(\\frac{1}{2} \\log 2\\pi \\sigma_v^2 + \\frac{\\mathbb{E}_s[(x_t-y_t)^2] }{2\\sigma_v^2}+ \\frac{1}{2} \\log 2\\pi \\sigma_w^2 + \\frac{\\mathbb{E}_s[(y_{t+1}-y_t)^2]}{2\\sigma_w^2} \\right)\n\nwhere \\mathbb{E}_s[...] \\equiv \\mathbb{E}[... |x_{0:T}, \\theta^{(s)}]. Let us compute the first of the expectations:\\mathbb{E}_s[(x_t-y_t)^2] = \\mathbb{E}_s[x_t^2 + y_t^2 - 2x_t y_t] = x_t^2 + \\mathbb{E}_s[y_t^2] - 2 x_t \\mathbb{E}_s[y_t] = x_t^2 + (\\hat{y}_{t|T}^{(s)})^2 + (\\sigma_{t|T}^{(s)})^2 - 2 x_t \\hat{y}_{t|T}^{(s)}\n\nwhere we have used the smoothing equations introduced previously, only their iterative computation is done using parameters \\theta^{(s)}. The second one can be computed as:\\mathbb{E}_s[(y_{t+1}-y_t)^2] = \\mathbb{E}_s[y_{t+1}^2 + y_t^2 - 2y_{t+1} y_t] = \\mathbb{E}_s[y_{t+1}^2] + \\mathbb{E}_s[y_t^2] - 2 \\mathbb{E}_s[y_{t+1} y_t] \\\\= (\\hat{y}_{t+1|T}^{(s)})^2 + (\\sigma_{t+1|T}^{(s)})^2  + (\\hat{y}_{t|T}^{(s)})^2 + (\\sigma_{t|T}^{(s)})^2 - 2 \\sigma^{(s)}_{t+1,t|T}  -  2 \\hat{y}_{t+1|T}^{(s)} \\hat{y}_{t|T}^{(s)} \\\\ = (\\hat{y}_{t+1|T}^{(s)} - \\hat{y}_{t|T}^{(s)})^2 +  (\\sigma_{t+1|T}^{(s)})^2 + (\\sigma_{t|T}^{(s)})^2 - 2 \\sigma^{(s)}_{t+1,t|T}\n\nwhere we have introduced the smoothed estimation of the one-lag covariance \\sigma_{t+1,t|T}^{(s)} \\equiv \\mathbb{E}_s[y_{t+1} y_t] - \\hat{y}_{t+1|T}^{(s)} \\hat{y}_{t|T}^{(s)}. A derivation of the recursive equation to compute this covariance can be checked in \n\nMurphy, 2013:\\sigma_{t+1,t|T}^{(s)} =  J_t (\\sigma_{t+1|T}^{(s)})^2\n\nwhere we have introduced the smoother gain J_t:J_t \\equiv \\frac{ (\\sigma_{t|t}^{(s)})^2}{ (\\sigma_{t+1|t}^{(s)})^2}\n\nLet us move now into the M-step, where we obtain new estimators for the parameters of the model by maximizing the expected complete likelihood. We compute the extremes and leave the interested reader the evaluation of the second derivatives to verify that they are indeed maxima:\\frac{\\partial}{\\partial \\sigma_v^2} Q(\\theta|\\theta^{(s)}) = \\sum_{t=1}^T \\left(\\frac{1}{2\\sigma_v^2} -\\frac{\\mathbb{E}_s[(x_t-y_t)^2] }{2\\sigma_v^4}\\right) =0 \\rightarrow (\\sigma_v^{(s+1)})^2 =  \\frac{1}{T}\\sum_{t=1}^T \\mathbb{E}_s[(x_t-y_t)^2]\\frac{\\partial}{\\partial \\sigma_w^2} Q(\\theta|\\theta^{(s)}) = \\sum_{t=1}^{T-1} \\left(\\frac{1}{2\\sigma_w^2} -\\frac{\\mathbb{E}_s[(y_{t+1}-y_t)^2] }{2\\sigma_w^4}\\right) =0 \\rightarrow (\\sigma_w^{(s+1)})^2 = \\frac{1}{T-1} \\sum_{t=1}^{T-1} \\mathbb{E}_s[(y_{t+1}-y_t)^2]\n\nThe new estimators depend on the expectations calculated in the E-step, which themselves are calculated doing a forward and smoothing passes over the data.\n\nThis completes the derivation of the EM algorithm for the local level model, since now starting from a seed for the parameters, we can iteratively refine our estimators until we reach a convergence in terms of parameters or log-likelihood.\n\nTo see the algorithm in action we simulate a local level model over 200 time-steps, with \\sigma_v^2 = 1.0 and \\sigma_w^2 = 0.05. Then we use the simulated data to find the parameters using Expectation Maximization. The algorithm reach quickly convergence, as seen in the figure below. As a sanity check of the implementation, we see that the log-likelihood over EM iterations never decreases.\n\n\n\n(a)\n\n\n\n(b)Simulated latent state variable and observations for the local level model with \\sigma_v^2 = 1.0 and \\sigma_w^2 = 0.05. The green line shows the inferred latent state using the smoothing algorithm, with a two sigma confidence interval shaded.```\n\nFigure 10:Log-likelihood of the local level model evaluated with the estimation of the parameters using EM for each iteration. We see that log-likelihood never decreases, which is a sanity check for the correctneess of the implementation.```\n\nThe converged parameters for this simulation are \\hat{\\sigma}_v^2 = 0.872 and \\hat{\\sigma}_w^2 = 0.054, which are reasonable approximations although not exact, since EM does not guarantee to reach the maximum of the likelihood. Finally, we run the smoother using the converged parameters and compare it with the observations and the true latent state variable. The smoother provides a good approximation for the true latent state, which remains within a two sigma band for the full simulation.","type":"content","url":"/markdown/intro-bayesian#example-3-local-level-model-simple-case-of-a-kalman-filter","position":57},{"hierarchy":{"lvl1":"Causal inference"},"type":"lvl1","url":"/markdown/intro-causal","position":0},{"hierarchy":{"lvl1":"Causal inference"},"content":"","type":"content","url":"/markdown/intro-causal","position":1},{"hierarchy":{"lvl1":"Causal inference","lvl2":"Introduction"},"type":"lvl2","url":"/markdown/intro-causal#introduction","position":2},{"hierarchy":{"lvl1":"Causal inference","lvl2":"Introduction"},"content":"","type":"content","url":"/markdown/intro-causal#introduction","position":3},{"hierarchy":{"lvl1":"Causal inference","lvl2":"The three ladders of causation"},"type":"lvl2","url":"/markdown/intro-causal#the-three-ladders-of-causation","position":4},{"hierarchy":{"lvl1":"Causal inference","lvl2":"The three ladders of causation"},"content":"","type":"content","url":"/markdown/intro-causal#the-three-ladders-of-causation","position":5},{"hierarchy":{"lvl1":"Causal inference","lvl2":"Interventions and the Do-calculus"},"type":"lvl2","url":"/markdown/intro-causal#interventions-and-the-do-calculus","position":6},{"hierarchy":{"lvl1":"Causal inference","lvl2":"Interventions and the Do-calculus"},"content":"","type":"content","url":"/markdown/intro-causal#interventions-and-the-do-calculus","position":7},{"hierarchy":{"lvl1":"Causal inference","lvl2":"Counterfactuals"},"type":"lvl2","url":"/markdown/intro-causal#counterfactuals","position":8},{"hierarchy":{"lvl1":"Causal inference","lvl2":"Counterfactuals"},"content":"","type":"content","url":"/markdown/intro-causal#counterfactuals","position":9},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments"},"type":"lvl1","url":"/markdown/intro-financial-instruments","position":0},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments"},"content":"","type":"content","url":"/markdown/intro-financial-instruments","position":1},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl2":"Introduction"},"type":"lvl2","url":"/markdown/intro-financial-instruments#introduction","position":2},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl2":"Introduction"},"content":"As discussed in the previous chapter, a financial instrument is an exchangeable contract that specifies the conditions for the transfer of funds between two parties — including the amounts exchanged, the timing, and any clauses or rights involved. Given the diversity of objectives among issuers and investors, it is not surprising that financial instruments have become a major field of innovation, constantly adapting to new funding, investment, and risk management needs.\n\nClassical financial instruments such as stocks and bonds form the foundation of modern markets. They articulate the basic economic functions of financing corporations and governments — the former in exchange for ownership and a share of profits (whether retained or paid as dividends), and the latter through fixed periodic payments and the promise of capital repayment at maturity. These are often referred to as cash instruments, since they involve direct investment or lending relationships and represent claims on real assets or income streams.\n\nBeyond equities and bonds, the cash universe includes money market instruments — such as repos, treasury bills, and commercial paper — which facilitate short-term financing and liquidity management. Closely related are foreign exchange (FX) instruments, which enable the exchange of funds across currencies. FX spot transactions are also cash instruments, involving the immediate delivery of one currency against another, while FX forwards and swaps allow participants to manage funding or hedging needs across different currencies and maturities. Together, these cash markets provide the foundation for pricing, liquidity, and risk transfer across the global financial system.\n\nFinancial innovation has, however, gone far beyond these classical instruments. Derivative markets extend the set of available contracts by allowing exposures to be tailored — separating, transferring, or amplifying specific sources of risk. Instruments such as futures, swaps, and options are now central to global markets, used both for risk management and speculation. Their design allows participants to trade not the assets themselves, but the conditions under which value changes — such as interest rates, credit spreads, or exchange rates.\n\nFinally, a broad range of hybrid and structured products combine features of both cash and derivative instruments to meet more specialized investment or funding objectives. Products such as structured notes, credit-linked instruments, and securitizations illustrate how financial engineering can reshape risk and return profiles to suit diverse investor preferences.\n\nIn what follows, we organize financial instruments into three broad families:\n\nCash instruments, encompassing equities, bonds, money-market, and foreign exchange products. These instruments have typically liquid prices and conform the main so-called market risk factor, i.e. the fundamental sources of systematic risk arising from movements in observable market variables such as equity prices, interest rates, yield curves, credit spreads, and exchange rates.\n\nDerivative instruments, including forwards, swaps, and options. These instruments derive their value from underlying cash instruments or market variables and are primarily used to transfer, hedge, or leverage market risk factors, often introducing nonlinear and path-dependent payoffs\n\nHybrid and structured products, which combine multiple risk exposures and payoff structures. These instruments embed derivatives within cash products to tailor risk–return profiles, resulting in complex sensitivities to several market risk factors and, in many cases, reduced transparency and liquidity\n\nThis structure reflects both the economic function of instruments and the way modern markets are organized in practice.","type":"content","url":"/markdown/intro-financial-instruments#introduction","position":3},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl2":"Cash instruments and the main market risk factors"},"type":"lvl2","url":"/markdown/intro-financial-instruments#cash-instruments-and-the-main-market-risk-factors","position":4},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl2":"Cash instruments and the main market risk factors"},"content":"Cash instruments represent direct claims on an asset, income stream, or borrower. They are the foundation of capital markets, facilitating investment, funding, and liquidity management. Their valuation depends primarily on the expected cash flows, credit risk, and time value of money.\n\nAs mentioned above, from a risk perspective, cash instruments can be used to define market risk factors, which are latent factors that can be used to largely explain their observable co-movements in prices. In equities, while individual stocks exhibit significant idiosyncratic risk, the dominant systematic risk factor is captured at the portfolio level through broad market indexes, which represent aggregate exposure to economic growth, risk appetite, and discount-rate dynamics. Fixed-income instruments are primarily driven by the dynamics of interest-rate and credit spreads curves; money-market instruments by short-term funding rates and liquidity conditions; and foreign exchange instruments by relative interest rates, macroeconomic fundamentals, and cross-border capital flows.","type":"content","url":"/markdown/intro-financial-instruments#cash-instruments-and-the-main-market-risk-factors","position":5},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl3":"Equities","lvl2":"Cash instruments and the main market risk factors"},"type":"lvl3","url":"/markdown/intro-financial-instruments#equities","position":6},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl3":"Equities","lvl2":"Cash instruments and the main market risk factors"},"content":"Equities are financial instruments through which investors provide capital to a company in exchange for participation in its economic performance and governance. Unlike debt, equities do not promise predefined cash flows. Instead, the payoff to equity holders is directly linked to the evolution of the firm’s assets, profitability, and long-term prospects. This feature makes equity the primary risk-bearing instrument in a company’s capital structure and the main channel through which investors gain exposure to corporate growth.\n\nFrom a balance sheet perspective, equity represents shareholders’ capital and is defined residually through the fundamental accounting identity\\text{Assets} = \\text{Liabilities} + \\text{Equity}\n\nThis identity must always hold. Consequently, any change in the value of a firm’s assets or liabilities is mechanically reflected in equity. In practice, the nominal value of debt is largely fixed by contract, so short-term fluctuations in firm value are absorbed almost entirely by equity. When asset values decline, equity is reduced first; only after equity is exhausted do losses begin to impair debt holders. This asymmetric position explains both the higher volatility of equity prices and the shareholders’ claim on the firm’s upside.\n\nCompanies raise equity by issuing shares, either privately or through public offerings in organized markets. In a public issuance, investors purchase shares in a competitive process, providing cash capital to the firm. A defining feature of equity markets is the existence of a liquid secondary market, where shares can be freely traded among investors. Although trading in the secondary market does not directly affect the firm’s cash position, it plays a crucial economic role: liquidity increases investors’ willingness to supply capital in the primary market and generates a continuously updated market valuation of the firm.\n\nAn equity share typically grants three fundamental rights. First, shareholders participate in the firm’s profits through dividends, when and if these are distributed. Second, they hold a residual claim on the firm’s assets in the event of liquidation, after all creditors have been paid. Third, equity often confers control rights, usually exercised through voting on corporate governance matters. Importantly, equity imposes no obligation on the firm to repay the invested capital or to make regular payments, sharply distinguishing it from debt financing.\n\nThis absence of contractual repayment makes equity a flexible and, from the firm’s perspective, often cheaper source of financing. Dividends are paid only when profits allow, whereas debt requires fixed interest payments regardless of business conditions. As a result, adverse shocks to firm value tend to be reflected first and most strongly in equity prices, while debt valuations remain relatively stable until default risk becomes material.","type":"content","url":"/markdown/intro-financial-instruments#equities","position":7},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"Valuation of Equity","lvl3":"Equities","lvl2":"Cash instruments and the main market risk factors"},"type":"lvl4","url":"/markdown/intro-financial-instruments#valuation-of-equity","position":8},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"Valuation of Equity","lvl3":"Equities","lvl2":"Cash instruments and the main market risk factors"},"content":"Equity valuation can be approached using several complementary frameworks, each emphasizing a different economic dimension of the equity claim.","type":"content","url":"/markdown/intro-financial-instruments#valuation-of-equity","position":9},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl5":"Accounting (book value) approach","lvl4":"Valuation of Equity","lvl3":"Equities","lvl2":"Cash instruments and the main market risk factors"},"type":"lvl5","url":"/markdown/intro-financial-instruments#accounting-book-value-approach","position":10},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl5":"Accounting (book value) approach","lvl4":"Valuation of Equity","lvl3":"Equities","lvl2":"Cash instruments and the main market risk factors"},"content":"From the balance sheet, total equity is defined as the difference between assets and liabilities. On a per-share basis, book value is given by\\text{Book Value per Share} = \\frac{\\text{Assets} - \\text{Liabilities}}{\\text{Number of Shares Outstanding}}\n\nBook value provides an accounting anchor for equity valuation, but it often differs substantially from market prices. This divergence reflects conservative accounting rules, historical cost conventions, and the limited recognition of economically important assets on the balance sheet. Intangible assets such as intellectual property, proprietary technology, data, organizational capital, and workforce skills are typically expensed rather than capitalized, despite being central to value creation in many modern firms. As a result, market prices necessarily incorporate expectations about future assets and liabilities that are only imperfectly captured by accounting statements.","type":"content","url":"/markdown/intro-financial-instruments#accounting-book-value-approach","position":11},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl5":"Market valuation","lvl4":"Valuation of Equity","lvl3":"Equities","lvl2":"Cash instruments and the main market risk factors"},"type":"lvl5","url":"/markdown/intro-financial-instruments#market-valuation","position":12},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl5":"Market valuation","lvl4":"Valuation of Equity","lvl3":"Equities","lvl2":"Cash instruments and the main market risk factors"},"content":"In traded markets, equity is valued through its share price. The market value of equity, or market capitalization, is defined as\\text{Market Capitalization} = P \\times N\n\nwhere P denotes the share price and N the number of outstanding shares. Market prices aggregate investors’ expectations about future profitability, risk, growth, and the evolution of both tangible and intangible assets, making equity valuation inherently forward-looking and partially speculative. Persistent deviations between market value and book value are therefore not anomalies per se, but reflections of beliefs about future economic outcomes.","type":"content","url":"/markdown/intro-financial-instruments#market-valuation","position":13},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl5":"Discounted dividend approach","lvl4":"Valuation of Equity","lvl3":"Equities","lvl2":"Cash instruments and the main market risk factors"},"type":"lvl5","url":"/markdown/intro-financial-instruments#discounted-dividend-approach","position":14},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl5":"Discounted dividend approach","lvl4":"Valuation of Equity","lvl3":"Equities","lvl2":"Cash instruments and the main market risk factors"},"content":"Conceptually, a share can be viewed as a claim on an uncertain stream of future dividends. Under this perspective, and using the ideas of fair value that will be thoroughly discussed in chapter \n\nFair value estimation, a fundamental model for the price of a stock satisfiesP_t = \\mathbb{E}_t\\left[ \\sum_{k=1}^{\\infty} \\frac{d_{t+k}}{(1+r)^k} \\right]\n\nwhere d_{t+k} denotes dividends paid in the future, which are brought into present value using discount factors at a rate r. This rate cannot, in general, be identified with the risk-free rate if the price is to reflect the risk borne by investors. Instead, r should be interpreted as a risk-adjusted discount rate that compensates investors for both the time value of money and the uncertainty associated with future dividends.\n\nThe discounted dividend framework has been subject to important critiques, though. Shiller’s volatility argument \n\nShiller, 1981 considers the implication of the present-value relation under rational expectations. LetP_t^* = \\sum_{k=1}^{\\infty} \\frac{d_{t+k}}{(1+r)^k}\n\ndenote the ex post realized present value of future dividends. Under rational expectations, the observed price satisfies P_t = \\mathbb{E}_t[P_t^*]. A basic variance inequality then implies\\operatorname{Var}(P_t) \\leq \\operatorname{Var}(P_t^*).\n\nEmpirically, however, stock prices exhibit substantially greater volatility than the realized discounted value of dividends, violating this inequality. This excess volatility indicates that a literal, constant-discount-rate rational expectations dividend discount model cannot fully account for observed equity price dynamics. Possible resolutions include time-varying discount rates, changing risk premia, or deviations from fully rational expectations.","type":"content","url":"/markdown/intro-financial-instruments#discounted-dividend-approach","position":15},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"Risk Factors and Expected Returns","lvl3":"Equities","lvl2":"Cash instruments and the main market risk factors"},"type":"lvl4","url":"/markdown/intro-financial-instruments#risk-factors-and-expected-returns","position":16},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"Risk Factors and Expected Returns","lvl3":"Equities","lvl2":"Cash instruments and the main market risk factors"},"content":"While valuation frameworks describe what equity prices represent, asset pricing models focus on why equities earn the returns they do. At the portfolio level, expected equity returns are primarily compensation for exposure to systematic risk factors rather than idiosyncratic firm-level uncertainty. The idea underlying this theoretical framework is that idiosyncratic risk can be removed by providing sufficient diversification to a portfolio, but systematic risks can not.\n\nThe Capital Asset Pricing Model (CAPM) expresses the expected excess return on an asset as proportional to its covariance with the market portfolio:\\mathbb{E}[R_i] - R_f = \\beta_i\\,\\big(\\mathbb{E}[R_m] - R_f\\big), \\quad \\beta_i = \\frac{\\operatorname{Cov}(R_i, R_m)}{\\operatorname{Var}(R_m)}\n\nwhere R_f is the risk-free rate and R_m the market return. In this framework, equity risk is summarized by market beta, and only market-wide risk is priced.\n\nEmpirical evidence suggests that additional systematic factors help explain cross-sectional differences in equity returns. The Fama–French three-factor model extends CAPM by including size and value factors:R_i - R_f = \\alpha_i + \\beta_{m,i}(R_m - R_f) + \\beta_{s,i}\\,\\text{SMB} + \\beta_{v,i}\\,\\text{HML} + \\varepsilon_i\n\nwhere SMB (small minus big) captures size-related risk and HML (high minus low) captures capitalization-related risk. The Carhart four-factor model further augments this specification with a momentum factor (MOM):R_i - R_f = \\alpha_i + \\beta_{m,i}(R_m - R_f) + \\beta_{s,i}\\,\\text{SMB} + \\beta_{v,i}\\,\\text{HML} + \\beta_{mom,i}\\,\\text{MOM} + \\varepsilon_i\n\nIn these models, expected equity returns arise from exposure to multiple sources of systematic risk, each associated with a risk premium. Equity pricing thus reflects not only expectations about future cash flows, but also how those cash flows co-vary with broader economic conditions, completing the link between valuation, risk, and returns.","type":"content","url":"/markdown/intro-financial-instruments#risk-factors-and-expected-returns","position":17},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl3":"Money Market Instruments","lvl2":"Cash instruments and the main market risk factors"},"type":"lvl3","url":"/markdown/intro-financial-instruments#money-market-instruments","position":18},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl3":"Money Market Instruments","lvl2":"Cash instruments and the main market risk factors"},"content":"Money market instruments are short-term financial instruments used to manage liquidity, fund short-term obligations, and transmit monetary policy through the financial system. They are characterized by short maturities, typically ranging from overnight to one year, high credit quality, and low price volatility under normal market conditions. Unlike equities, money market instruments are not designed to provide exposure to long-term growth, but to preserve capital, provide liquidity, and facilitate the efficient functioning of payment and funding markets.\n\nFrom a balance sheet perspective, money market instruments primarily arise from the short-term financing needs of governments, banks, and corporations. For issuers, they represent short-term liabilities used to bridge timing mismatches between cash inflows and outflows. For investors, they appear as near-cash assets—claims with well-defined nominal values and short horizons. Because maturities are short and contractual cash flows are fixed or highly predictable, valuation uncertainty is typically limited relative to longer-dated fixed income or equity instruments.\n\nMoney market instruments play a central role in the architecture of modern financial systems. They underpin interbank lending, collateralized funding, treasury cash management, and the settlement of securities transactions. Instruments such as Treasury bills, commercial paper, certificates of deposit, and repurchase agreements form an interconnected ecosystem in which liquidity is continuously redistributed among market participants. Disruptions in these markets therefore tend to have immediate and systemic consequences, as illustrated by repeated episodes of stress in interbank and repo markets.","type":"content","url":"/markdown/intro-financial-instruments#money-market-instruments","position":19},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"Risk Characteristics","lvl3":"Money Market Instruments","lvl2":"Cash instruments and the main market risk factors"},"type":"lvl4","url":"/markdown/intro-financial-instruments#risk-characteristics","position":20},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"Risk Characteristics","lvl3":"Money Market Instruments","lvl2":"Cash instruments and the main market risk factors"},"content":"Although money market instruments are often perceived as low-risk, they are not risk-free. Their short maturity significantly reduces interest rate risk, but other sources of risk remain relevant. Credit risk arises from the possibility that the issuer may fail to repay at maturity, even over short horizons. Liquidity risk reflects the potential inability to sell or roll over positions without price concessions, particularly during periods of market stress. In collateralized instruments, such as repos, valuation and haircut risk play an additional role, as changes in collateral values can amplify funding pressures.\n\nFrom a portfolio perspective, money market instruments are typically viewed as low-volatility assets with minimal exposure to long-term systematic risk factors. However, they are highly sensitive to funding conditions and confidence effects. What appears as idiosyncratic credit or liquidity risk at the instrument level can rapidly become systemic when many institutions attempt to secure short-term funding simultaneously.","type":"content","url":"/markdown/intro-financial-instruments#risk-characteristics","position":21},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"Link to Monetary Policy","lvl3":"Money Market Instruments","lvl2":"Cash instruments and the main market risk factors"},"type":"lvl4","url":"/markdown/intro-financial-instruments#link-to-monetary-policy","position":22},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"Link to Monetary Policy","lvl3":"Money Market Instruments","lvl2":"Cash instruments and the main market risk factors"},"content":"Money market instruments are the primary channel through which central banks implement and transmit monetary policy. Policy rates are operationally targeted in overnight or very short-term money markets, and central bank actions directly affect the pricing and availability of short-term funding. Open market operations, standing facilities, and asset purchase programs operate by injecting or absorbing reserves, thereby influencing money market rates and spreads.\n\nBecause of their short maturity, yields on money market instruments closely track policy rates and expectations of near-term monetary policy. Changes in central bank target rates, corridor systems, or liquidity provision frameworks are rapidly reflected in money market prices. As a result, money markets provide a real-time signal of monetary conditions and play a crucial role in anchoring the short end of the yield curve.\n\nMore broadly, conditions in money markets influence the transmission of monetary policy to the rest of the financial system. Disruptions in short-term funding can impair banks’ ability to extend credit, weaken the pass-through of policy rate changes, and force central banks to intervene as lenders of last resort. For this reason, the stability and smooth functioning of money markets are not only a technical concern, but a core objective of modern central banking.","type":"content","url":"/markdown/intro-financial-instruments#link-to-monetary-policy","position":23},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"Reference Rates","lvl3":"Money Market Instruments","lvl2":"Cash instruments and the main market risk factors"},"type":"lvl4","url":"/markdown/intro-financial-instruments#reference-rates","position":24},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"Reference Rates","lvl3":"Money Market Instruments","lvl2":"Cash instruments and the main market risk factors"},"content":"Modern financial markets rely on reference interest rates—benchmarks used to determine floating-rate payments in loans, bonds, and derivatives. These rates serve as the foundation for trillions of dollars of financial contracts, ensuring a consistent and transparent mechanism for pricing and settlement. Monetary markets play a key role for the determination of reference rates, typically linked to instruments issued in these markets.\n\nProbably the most famous reference rate is the London Interbank Offered Rate (LIBOR), which became the dominant global benchmark during the 1980s, although its roots date back to the late 1960s. The first recorded use of a LIBOR-like rate appeared in 1969, when a group of London-based banks agreed to price a syndicated loan to the Shah of Iran at a margin over the rate at which they could borrow short-term funds in the interbank market. This convention proved practical for cross-border lending, allowing banks to align loan pricing with their own funding costs. As syndicated lending expanded in the 1970s and derivatives markets emerged in the 1980s, LIBOR was formalized by the British Bankers’ Association (BBA) as a standard daily benchmark, calculated as the average rate at which major banks believed they could borrow from one another on an unsecured basis.\n\nHowever, the credibility of LIBOR was deeply undermined during the LIBOR manipulation scandal uncovered between 2011 and 2012. Investigations revealed that several panel banks had deliberately altered their submissions, either to profit from derivative positions or to disguise funding stress during the global financial crisis. The scandal exposed structural weaknesses: submissions were often based on expert judgment rather than actual transactions, making the benchmark vulnerable to manipulation. Regulatory responses included substantial fines, criminal prosecutions, and the establishment of new oversight frameworks for benchmark administration. Yet, the damage to trust was profound and accelerated global efforts to reform reference rate frameworks.\n\nIn the aftermath, regulators and market participants collaborated to design risk-free rates (RFRs) rooted in observable market transactions. These new benchmarks aim to eliminate reliance on subjective quotes and to reflect nearly risk-free overnight funding costs. Examples include:\n\nthe Secured Overnight Financing Rate (SOFR) in the United States\n\nthe Euro Short-Term Rate (€STR) in the euro area\n\nSONIA in the United Kingdom.\n\nUnlike LIBOR, which was unsecured and term-based, these rates are overnight and transaction-based, typically derived from repo or wholesale funding markets. Over time, derivatives markets have developed conventions for compounded-in-arrears calculations to build synthetic term structures compatible with existing financial products.\n\nThe transition from LIBOR to alternative rates has relevant contractual implications. Because LIBOR was embedded in an enormous range of financial instruments—from corporate loans to floating-rate notes and interest-rate swaps—its cessation required robust fallback provisions. Industry bodies such as ISDA introduced standardized fallback methodologies and spread adjustments to account for the credit and term premia embedded in LIBOR. Market participants had to amend legacy contracts, update systems, and reprice instruments to maintain consistency and avoid legal uncertainty.","type":"content","url":"/markdown/intro-financial-instruments#reference-rates","position":25},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"Main monetary market instruments","lvl3":"Money Market Instruments","lvl2":"Cash instruments and the main market risk factors"},"type":"lvl4","url":"/markdown/intro-financial-instruments#main-monetary-market-instruments","position":26},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"Main monetary market instruments","lvl3":"Money Market Instruments","lvl2":"Cash instruments and the main market risk factors"},"content":"","type":"content","url":"/markdown/intro-financial-instruments#main-monetary-market-instruments","position":27},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl5":"Treasury Bills","lvl4":"Main monetary market instruments","lvl3":"Money Market Instruments","lvl2":"Cash instruments and the main market risk factors"},"type":"lvl5","url":"/markdown/intro-financial-instruments#treasury-bills","position":28},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl5":"Treasury Bills","lvl4":"Main monetary market instruments","lvl3":"Money Market Instruments","lvl2":"Cash instruments and the main market risk factors"},"content":"","type":"content","url":"/markdown/intro-financial-instruments#treasury-bills","position":29},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl5":"Commercial Paper","lvl4":"Main monetary market instruments","lvl3":"Money Market Instruments","lvl2":"Cash instruments and the main market risk factors"},"type":"lvl5","url":"/markdown/intro-financial-instruments#commercial-paper","position":30},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl5":"Commercial Paper","lvl4":"Main monetary market instruments","lvl3":"Money Market Instruments","lvl2":"Cash instruments and the main market risk factors"},"content":"","type":"content","url":"/markdown/intro-financial-instruments#commercial-paper","position":31},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl5":"Certificates of Deposit","lvl4":"Main monetary market instruments","lvl3":"Money Market Instruments","lvl2":"Cash instruments and the main market risk factors"},"type":"lvl5","url":"/markdown/intro-financial-instruments#certificates-of-deposit","position":32},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl5":"Certificates of Deposit","lvl4":"Main monetary market instruments","lvl3":"Money Market Instruments","lvl2":"Cash instruments and the main market risk factors"},"content":"","type":"content","url":"/markdown/intro-financial-instruments#certificates-of-deposit","position":33},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl5":"Repurchase Agreements (Repos)","lvl4":"Main monetary market instruments","lvl3":"Money Market Instruments","lvl2":"Cash instruments and the main market risk factors"},"type":"lvl5","url":"/markdown/intro-financial-instruments#repurchase-agreements-repos","position":34},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl5":"Repurchase Agreements (Repos)","lvl4":"Main monetary market instruments","lvl3":"Money Market Instruments","lvl2":"Cash instruments and the main market risk factors"},"content":"Repurchase agreements, or repos, are among the most important instruments in modern money markets. A repo is a short-term secured loan: one party sells a security—typically a government bond—with the commitment to repurchase it at a later date and a slightly higher price. The difference between the sale and repurchase price reflects the repo rate, analogous to an interest rate on a collateralized borrowing. From the counterparty’s point of view, the transaction is a reverse repo, meaning it lends cash against the security as collateral.\n\nRepos are fundamental to liquidity management and collateral circulation. They enable financial institutions to fund positions efficiently, manage short-term liquidity, and facilitate price discovery in fixed income markets. Central banks also rely on repos as a primary instrument of monetary policy, using them to inject or withdraw liquidity and to influence short-term interest rates. The global repo market is vast: outstanding balances are estimated in the tens of trillions of dollars, with the U.S. segment alone exceeding $5 trillion in daily transactions \n\nOffice of Financial Research, 2023.\n\nMechanically, two key parameters define the economics of a repo: the repo rate and the haircut—the percentage discount applied to the market value of the collateral. A higher haircut protects the lender against a fall in collateral value but increases the borrower’s funding cost. Haircuts vary with the perceived credit quality and liquidity of the collateral: Treasury securities typically carry haircuts close to zero, while corporate bonds or structured products may require 5–20%. In stressed conditions, haircuts often rise sharply, forcing deleveraging and amplifying market instability, as seen during the 2008 financial crisis.\n\nFor example, consider a repo in which a bank borrows \\$98 million in cash for one week and pledges \\$100 million in Treasury bonds as collateral. The haircut is therefore 2%. If the agreed repo rate is 3% per annum, the repurchase price after seven days will be:98,000,000×(1+0.03×\\frac{7}{360})≈98,057,166\\$\n\nAt maturity, the bank repays \\$98.06 million and receives back its \\$100 million in bonds. For the cash lender, the repo rate represents the secured yield on the transaction, while for the borrower, it represents the funding cost of holding the securities.\n\nRepos are also integral to the market-making business. Suppose a dealer sells a bond to an asset manager such as Amundi, but does not hold the bond in inventory. The dealer can borrow the bond in the repo market, for instance from another asset manager like BlackRock, who lends it against cash collateral. The dealer delivers the bond to the client and must then either wait for another client to take the opposite position, renew the repo agreement upon maturity, or buy the bond in the interbank market to close the position. This continuous use of repos allows dealers to provide liquidity and make markets without holding large inventories of bonds.\n\nRepos also underpin short selling, as they provide the mechanism by which short sellers borrow securities to sell them in anticipation of price declines. However, short positions carry significant risk if prices rise. Public data on open short positions—such as the short‐interest reports published by FINRA, NYSE, and Nasdaq—allow regulators and market participants to monitor short activity and its potential influence on price pressure and funding conditions. For example, the GameStop short squeeze in early 2021 \n\nU.S. Securities and Exchange Commission, 2021 demonstrated how short sellers can face extreme pressure when prices move unexpectedly, forcing them to cover positions at large losses. Earlier examples, such as those portrayed in The Big Short \n\nLewis, 2010, show that even when short sellers are correct in their analysis, they can face severe liquidity and timing risks.","type":"content","url":"/markdown/intro-financial-instruments#repurchase-agreements-repos","position":35},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl3":"Fixed Income","lvl2":"Cash instruments and the main market risk factors"},"type":"lvl3","url":"/markdown/intro-financial-instruments#fixed-income","position":36},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl3":"Fixed Income","lvl2":"Cash instruments and the main market risk factors"},"content":"Fixed income instruments are debt securities that promise a predefined set of cash flows over time, either in fixed or variable form. By purchasing a fixed income instrument, investors lend capital to an issuer in exchange for contractual payments and the repayment of principal at maturity. Unlike equity, which represents a residual claim on a firm’s assets, fixed income securities confer senior, legally binding claims.\n\nIssuers of fixed income instruments span the full spectrum of the economy. Sovereign issuers finance fiscal activity and provide benchmark risk-free curves. Corporate issuers use bonds to fund investment and manage leverage. Municipal issuers finance regional and local infrastructure, while supranational institutions issue debt to support international development and policy objectives.\n\nAcross all cases, the dominant fixed income instrument is the bond, characterized by periodic coupon payments and a final repayment of principal at a specified maturity. The presence of a maturity date anchors valuation in a way that sharply contrasts with equities, whose cash flows are discretionary and potentially infinite.","type":"content","url":"/markdown/intro-financial-instruments#fixed-income","position":37},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"Valuation of fixed income instruments","lvl3":"Fixed Income","lvl2":"Cash instruments and the main market risk factors"},"type":"lvl4","url":"/markdown/intro-financial-instruments#valuation-of-fixed-income-instruments","position":38},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"Valuation of fixed income instruments","lvl3":"Fixed Income","lvl2":"Cash instruments and the main market risk factors"},"content":"As we will discuss in chapter \n\nFair value estimation,  The theoretical foundation of fixed income valuation is the time value of money.\n\nThis logic extends naturally to bonds, which deliver multiple future cash flows. For a bond with maturity T, coupon payments C_t, and face value N, the price P is given by the sum of discounted promised payments:P = \\sum_{t=1}^{T} \\frac{C_t}{(1+r_t)^t} + \\frac{N}{(1+r_T)^T}\n\nIn practice, bonds traded in the market have observable prices that do not necessarily match with these theoretical prices, since there are additional risks and liquidity constraints for which investors typically demand an extra yield in order to invest in these instruments as alternatives to risk-free deposits. We define the yield to maturity  as the constant rate y that equates the discounted value of promised cash flows to the observed market price:P = \\sum_{t=1}^{T} \\frac{C_t}{(1+y)^t} + \\frac{N}{(1+y)^T}\n\nThe yield is therefore an implied quantity rather than a fundamental primitive: it is the discount rate that reconciles price and cash flows and allows bonds with different coupons and maturities to be compared on a common basis. If we compute yields for a set of bonds from the same issuer with different maturities, the sorted collection of yields is called the yield curve.","type":"content","url":"/markdown/intro-financial-instruments#valuation-of-fixed-income-instruments","position":39},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"Fixed income risk factors","lvl3":"Fixed Income","lvl2":"Cash instruments and the main market risk factors"},"type":"lvl4","url":"/markdown/intro-financial-instruments#fixed-income-risk-factors","position":40},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"Fixed income risk factors","lvl3":"Fixed Income","lvl2":"Cash instruments and the main market risk factors"},"content":"Risk in fixed income markets is commonly organized around two broad dimensions. The first is interest rate risk, driven by movements in the level and shape of the yield curve for relatively risk-free issuers, typically sovereign bonds issued by governments with solid finances like the USA, Germany or Japan, or supranational entities like the European Union. Alternatively, interest rate risk factors can also be inferred from short-term monetary instruments and interest rate derivatives like standard interest rate swaps (IRS).  The collection of inferred rates across maturities constitutes the term structure of interest rates. Changes in this term structure are typically highly correlated and can be mostly described in term of latent risk factors driving co-movements like parallel shifts, steepening, or flattening of the interest rate curve. Such risk factors are then linked to movements in prices government bond portfolios and other rates products. Notice that short-term rates movements are typically tied to monetary policy decisions, while longer-term rates embed expectations about future policy paths, inflation, and macroeconomic conditions.\n\nThe second major risk dimension is credit risk. Bonds issued by corporations, institutions or governments that are exposed to default risk trade at yields above those of comparable risk-free sovereign benchmarks. The difference between the yield on a risky bond and the yield on a benchmark bond of the same maturity is the credit spread s:s = y_{\\text{risky}} - y_{\\text{risk-free}}\n\nCredit spreads compensate investors for expected default losses, recovery uncertainty, liquidity risk, and risk premia. In sovereign markets, this differential is often referred to as a risk premium relative to a benchmark issuer; in corporate markets, it is the central pricing variable for credit as an asset class.\n\nThis distinction between rates risk and credit risk provides the organizing principle for fixed income markets. It also motivates the common separation between rates products and credit products in trading, portfolio management, and risk analysis. For a deeper introduction to this topic, a classical reading is \n\nFabozzi, 2007.","type":"content","url":"/markdown/intro-financial-instruments#fixed-income-risk-factors","position":41},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"Main fixed income instrument types","lvl3":"Fixed Income","lvl2":"Cash instruments and the main market risk factors"},"type":"lvl4","url":"/markdown/intro-financial-instruments#main-fixed-income-instrument-types","position":42},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"Main fixed income instrument types","lvl3":"Fixed Income","lvl2":"Cash instruments and the main market risk factors"},"content":"Bonds with different coupon types: fixed, floating, zero-coupon.\n\nPrice conventions: clean/dirty price.\n\nRisk factors from the term structured: pca, factor models\n\nInterest rate sensitivity: duration, convexity.\n\nCredit bond valuation: recovery rates, default risk. Anecdote recovery FTX","type":"content","url":"/markdown/intro-financial-instruments#main-fixed-income-instrument-types","position":43},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl3":"Foreign Exchange (FX)","lvl2":"Cash instruments and the main market risk factors"},"type":"lvl3","url":"/markdown/intro-financial-instruments#foreign-exchange-fx","position":44},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl3":"Foreign Exchange (FX)","lvl2":"Cash instruments and the main market risk factors"},"content":"The foreign exchange (FX) market enables the transfer of funds and the management of exposures between currencies. It is the mechanism through which international trade, investment, and financing are denominated, settled, and hedged. Participants include corporations managing cross-border payments, investors reallocating portfolios internationally, banks optimizing liquidity, and central banks implementing monetary policy or intervention.\n\nFX instruments therefore serve two fundamental purposes:\n\nTransaction and funding needs: facilitating payments and transfers between currencies.\n\nRisk management: hedging the exposure that arises when assets, liabilities, or revenues are denominated in foreign currencies.\n\nThe FX market operates continuously, 24 hours a day, across major financial centers, and is the largest and most liquid market in the world.","type":"content","url":"/markdown/intro-financial-instruments#foreign-exchange-fx","position":45},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"FX Spot Transactions","lvl3":"Foreign Exchange (FX)","lvl2":"Cash instruments and the main market risk factors"},"type":"lvl4","url":"/markdown/intro-financial-instruments#fx-spot-transactions","position":46},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"FX Spot Transactions","lvl3":"Foreign Exchange (FX)","lvl2":"Cash instruments and the main market risk factors"},"content":"An FX spot transaction is the most basic form of foreign exchange. It represents an agreement to exchange two currencies for near-immediate delivery — conventionally T+2 business days after the trade date (T+1 for certain pairs like USD/CAD).\n\nSpot prices are quoted as base/quote currency pairs, for example:\n\nEUR/USD = 1.0750\n\nwhich means that one euro (the base currency) costs 1.0750 U.S. dollars (the quote currency).\n\nPrices are typically quoted with bid–ask spreads, reflecting market liquidity and transaction costs:\n\nEUR/USD 1.0749–1.0751\n\nindicating the dealer is willing to buy euros at 1.0749 (bid) and sell them at 1.0751 (ask).\n\nSpot FX serves as the reference price for a wide range of currency-linked instruments. While we will briefly describe forwards and swaps in this section, their mechanics and broader applications will be examined in more detail in the following chapter on derivatives. Spot FX also underpins cross-border settlement systems like CLS (Continuous Linked Settlement), which reduce counterparty risk by ensuring the simultaneous settlement of both legs of the transaction.","type":"content","url":"/markdown/intro-financial-instruments#fx-spot-transactions","position":47},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"FX Forward Contracts","lvl3":"Foreign Exchange (FX)","lvl2":"Cash instruments and the main market risk factors"},"type":"lvl4","url":"/markdown/intro-financial-instruments#fx-forward-contracts","position":48},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"FX Forward Contracts","lvl3":"Foreign Exchange (FX)","lvl2":"Cash instruments and the main market risk factors"},"content":"An FX forward is a contract to exchange two currencies at a future date and a predetermined rate. The forward rate is determined by covered interest parity (CIP), which ensures the absence of arbitrage between spot and forward markets.\n\nFormally, for currencies A and B:F_{A/B} = S_{A/B} \\times \\frac{(1 + i_A T)}{(1 + i_B T)}\n\nwhere S_{A/B} is the spot rate (price of one unit of A in units of B), i_A and i_B are the interest rates in each currency, and T is the maturity in years.\n\nIntuitively, the forward rate reflects the interest rate differential between the two currencies: the currency with the higher interest rate will typically trade at a forward discount, and the one with the lower rate at a forward premium.\n\nFX forwards are extensively used by corporations and investors to hedge currency exposures associated with foreign-denominated assets, liabilities, or anticipated cash flows.","type":"content","url":"/markdown/intro-financial-instruments#fx-forward-contracts","position":49},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"FX Swaps","lvl3":"Foreign Exchange (FX)","lvl2":"Cash instruments and the main market risk factors"},"type":"lvl4","url":"/markdown/intro-financial-instruments#fx-swaps","position":50},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"FX Swaps","lvl3":"Foreign Exchange (FX)","lvl2":"Cash instruments and the main market risk factors"},"content":"An FX swap combines two FX transactions — a spot exchange of currencies and a simultaneous forward transaction that reverses the exchange at a later date.\n\nIt allows participants to borrow or lend one currency while using another as collateral, without taking directional exposure to the exchange rate. In practice, FX swaps function as short-term funding instruments and are widely used by banks, asset managers, and central banks for liquidity management across currencies.\n\nThe swap points (difference between forward and spot rates) are determined by the same interest rate differential that drives forward pricing.","type":"content","url":"/markdown/intro-financial-instruments#fx-swaps","position":51},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"Determinants of Spot FX Rates","lvl3":"Foreign Exchange (FX)","lvl2":"Cash instruments and the main market risk factors"},"type":"lvl4","url":"/markdown/intro-financial-instruments#determinants-of-spot-fx-rates","position":52},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"Determinants of Spot FX Rates","lvl3":"Foreign Exchange (FX)","lvl2":"Cash instruments and the main market risk factors"},"content":"While short-term FX movements can be driven by order flow, risk sentiment, or central bank interventions, medium- and long-term exchange rate trends are influenced by macroeconomic fundamentals. Some of the most relevant drivers include:\n\nReal Interest Rate Differentials:Real interest rates (nominal rates adjusted for inflation) between two economies can significantly influence currency valuation. When one economy offers higher real yields, global investors may shift capital to take advantage of the better return, increasing demand for that currency and driving its spot FX price higher.\n\nLocal Stock Market Performance:Strong performance in a country’s stock market can attract foreign investment, as investors seek higher returns. To purchase local equities, they must first acquire the domestic currency, boosting its demand and potentially appreciating its spot value.\n\nTrade Balance (Exports vs. Imports):The balance of trade between countries also impacts currency valuation. If a country exports more than it imports, foreign buyers must convert their currency into the exporter’s currency to pay for goods. Over time, sustained trade surpluses can increase demand for the exporter’s currency, appreciating its value. Conversely, persistent deficits may weaken it.","type":"content","url":"/markdown/intro-financial-instruments#determinants-of-spot-fx-rates","position":53},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl2":"Derivative Instruments"},"type":"lvl2","url":"/markdown/intro-financial-instruments#derivative-instruments","position":54},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl2":"Derivative Instruments"},"content":"Derivatives are contracts whose value depends on the price or level of an underlying asset, rate, or index. They allow market participants to transfer, hedge, or create specific risk exposures. Their valuation rests on no-arbitrage principles, linking them to the prices of cash instruments.","type":"content","url":"/markdown/intro-financial-instruments#derivative-instruments","position":55},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl3":"Forwards and Futures","lvl2":"Derivative Instruments"},"type":"lvl3","url":"/markdown/intro-financial-instruments#forwards-and-futures","position":56},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl3":"Forwards and Futures","lvl2":"Derivative Instruments"},"content":"Definition: agreements to buy/sell an asset at a future date and price.\n\nPricing intuition: cost-of-carry model; relationship to spot price.\n\nMargining and daily settlement (for futures).\n\nExamples: FX forwards, bond futures, equity index futures.\n\nUses: hedging, speculation, arbitrage.","type":"content","url":"/markdown/intro-financial-instruments#forwards-and-futures","position":57},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl3":"Swaps","lvl2":"Derivative Instruments"},"type":"lvl3","url":"/markdown/intro-financial-instruments#swaps","position":58},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl3":"Swaps","lvl2":"Derivative Instruments"},"content":"Interest rate swaps (IRS): fixed-for-floating exchange, notionals, netting.\n\nPricing and valuation: discounting, forward rates, and par swap rates.\n\nCross-currency swaps: exchanging cash flows in different currencies.\n\nCredit default swaps (CDS): protection leg, premium leg, credit events.\n\nApplications: managing funding costs, transforming exposures.\n\nMarket conventions: day count, accrual, ISDA documentation.","type":"content","url":"/markdown/intro-financial-instruments#swaps","position":59},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl3":"Options","lvl2":"Derivative Instruments"},"type":"lvl3","url":"/markdown/intro-financial-instruments#options","position":60},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl3":"Options","lvl2":"Derivative Instruments"},"content":"Options are derivatives contracts that grant the holder the  the option (hence the name) to buy or sell (depending on the option) a given financial instrument (the underlying) at a price at time contingent to the clauses of the contract. The most simple option, the European option, pre-specifies a given time T, the expire, and price K, to exercise this option. But there are many other variations in the market. An option to buy a financial instrument is referred as a call option, and an option to sell is a put option.\n\nLet us consider european options of stocks. If the market price of the stock at the expiry is S_T, a call option will be exercised by a rational investor only if the price is higher than the strike K, making a profit of S_T-K, which in some cases is directly paid in cash, in others the actual stock is received, but of course it could be directly sold in the market at a favorable price. Therefore, the payoff of a call option can be written as:C_T = (S_T-K)^+\n\nwhere (.)^+ denotes the positive part of the argument. On the contrary, a put option will only be exercised if the market price is below the strike, hence the payoff is:P_T = (K-S_T)^+\n\nOptions can be traded in regulated markets or be quoted by bank dealers. Since the investor that holds the option cannot lose money from it, such option does not come for free, and the question is how much is worth such option, which is called the premium of the option. At expiry, the price is naturally the payoff function. What at at time t < T there is still  uncertainty about the price S_T which will determine the final profit (if any), so the premium will be different. But how much? That is the subject of the theory of option pricing.","type":"content","url":"/markdown/intro-financial-instruments#options","position":61},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"Basic strategies: long/short positions, spreads, combinations.","lvl3":"Options","lvl2":"Derivative Instruments"},"type":"lvl4","url":"/markdown/intro-financial-instruments#basic-strategies-long-short-positions-spreads-combinations","position":62},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"Basic strategies: long/short positions, spreads, combinations.","lvl3":"Options","lvl2":"Derivative Instruments"},"content":"","type":"content","url":"/markdown/intro-financial-instruments#basic-strategies-long-short-positions-spreads-combinations","position":63},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"Valuation intuition: time value, volatility, and Greeks.","lvl3":"Options","lvl2":"Derivative Instruments"},"type":"lvl4","url":"/markdown/intro-financial-instruments#valuation-intuition-time-value-volatility-and-greeks","position":64},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"Valuation intuition: time value, volatility, and Greeks.","lvl3":"Options","lvl2":"Derivative Instruments"},"content":"","type":"content","url":"/markdown/intro-financial-instruments#valuation-intuition-time-value-volatility-and-greeks","position":65},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"Implied volatility and surface.","lvl3":"Options","lvl2":"Derivative Instruments"},"type":"lvl4","url":"/markdown/intro-financial-instruments#implied-volatility-and-surface","position":66},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"Implied volatility and surface.","lvl3":"Options","lvl2":"Derivative Instruments"},"content":"","type":"content","url":"/markdown/intro-financial-instruments#implied-volatility-and-surface","position":67},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"Market conventions: OTC vs. exchange-traded, delta quoting.","lvl3":"Options","lvl2":"Derivative Instruments"},"type":"lvl4","url":"/markdown/intro-financial-instruments#market-conventions-otc-vs-exchange-traded-delta-quoting","position":68},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl4":"Market conventions: OTC vs. exchange-traded, delta quoting.","lvl3":"Options","lvl2":"Derivative Instruments"},"content":"","type":"content","url":"/markdown/intro-financial-instruments#market-conventions-otc-vs-exchange-traded-delta-quoting","position":69},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl3":"Other Derivative Types (optional)","lvl2":"Derivative Instruments"},"type":"lvl3","url":"/markdown/intro-financial-instruments#other-derivative-types-optional","position":70},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl3":"Other Derivative Types (optional)","lvl2":"Derivative Instruments"},"content":"CDO tranches (intro).\n\nCommodity and volatility derivatives.\n\nExotic options: barriers, Asians, digitals (brief mention).","type":"content","url":"/markdown/intro-financial-instruments#other-derivative-types-optional","position":71},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl2":"Hybrid and Structured Products"},"type":"lvl2","url":"/markdown/intro-financial-instruments#hybrid-and-structured-products","position":72},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl2":"Hybrid and Structured Products"},"content":"Hybrid and structured products combine features of multiple instruments to create customized payoff profiles. They often embed derivative components within a funding or investment vehicle, enabling issuers and investors to fine-tune exposure to market variables, credit risk, or volatility.","type":"content","url":"/markdown/intro-financial-instruments#hybrid-and-structured-products","position":73},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl3":"Structured Notes and Deposits","lvl2":"Hybrid and Structured Products"},"type":"lvl3","url":"/markdown/intro-financial-instruments#structured-notes-and-deposits","position":74},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl3":"Structured Notes and Deposits","lvl2":"Hybrid and Structured Products"},"content":"Concept: debt instrument plus embedded option.\n\nExamples: equity-linked notes, reverse convertibles, range accruals.\n\nPayoff design: principal protection, participation rates, caps/floors.\n\nValuation and risks: credit of issuer, liquidity, complexity.","type":"content","url":"/markdown/intro-financial-instruments#structured-notes-and-deposits","position":75},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl3":"Credit-Linked Instruments","lvl2":"Hybrid and Structured Products"},"type":"lvl3","url":"/markdown/intro-financial-instruments#credit-linked-instruments","position":76},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl3":"Credit-Linked Instruments","lvl2":"Hybrid and Structured Products"},"content":"Credit-linked notes (CLN).\n\nSecuritized credit exposures (ABS, MBS).\n\nTranching and credit enhancement mechanisms.\n\nRelation to CDS markets.","type":"content","url":"/markdown/intro-financial-instruments#credit-linked-instruments","position":77},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl3":"Securitization and Structured Finance","lvl2":"Hybrid and Structured Products"},"type":"lvl3","url":"/markdown/intro-financial-instruments#securitization-and-structured-finance","position":78},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl3":"Securitization and Structured Finance","lvl2":"Hybrid and Structured Products"},"content":"Economic rationale: transferring credit risk and freeing balance sheet capacity.\n\nStructure: SPV, collateral pool, tranches, waterfalls.\n\nMarket evolution and crises context (e.g., 2008).","type":"content","url":"/markdown/intro-financial-instruments#securitization-and-structured-finance","position":79},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl3":"Hybrid Securities","lvl2":"Hybrid and Structured Products"},"type":"lvl3","url":"/markdown/intro-financial-instruments#hybrid-securities","position":80},{"hierarchy":{"lvl1":"Mechanics of Financial Instruments","lvl3":"Hybrid Securities","lvl2":"Hybrid and Structured Products"},"content":"Convertible and exchangeable bonds.\n\nPerpetual bonds and contingent convertibles (CoCos).\n\nPreferred shares and hybrids between debt and equity.","type":"content","url":"/markdown/intro-financial-instruments#hybrid-securities","position":81},{"hierarchy":{"lvl1":"Financial Markets"},"type":"lvl1","url":"/markdown/intro-financial-markets","position":0},{"hierarchy":{"lvl1":"Financial Markets"},"content":"Financial markets address a perennial challenge faced by societies: the efficient allocation of savings toward productive uses. Individuals and institutions with excess funds aim to achieve the highest returns (investment) while maintaining maximum liquidity. Consequently, they often prefer to lend money on a short-term basis. Conversely, those requiring capital—whether to launch new ventures, expand existing businesses, acquire assets such as homes through mortgages, or finance social programs—seek to secure funds at minimal cost and for extended periods. Without a mechanism to reconcile these opposing objectives, surplus funds may remain idle, hindering their potential to contribute to economic productivity.\n\nModern capitalist societies solve this problem in two ways, which form the backbone of the financial system: the banking system and financial markets.","type":"content","url":"/markdown/intro-financial-markets","position":1},{"hierarchy":{"lvl1":"Financial Markets","lvl2":"The Banking System"},"type":"lvl2","url":"/markdown/intro-financial-markets#the-banking-system","position":2},{"hierarchy":{"lvl1":"Financial Markets","lvl2":"The Banking System"},"content":"In the banking system the center is the bank, an institution or private company that offers short-term deposits with high availability to those\nwith excess funds, and long-term loans to those who need funds. A capital cushion based on short-term loans or directly cash is maintained in order to fulfills withdrawals. Their activity is not exempt from\nrisks:\n\nliquidity risk, which can happens if the demand to withdraw deposits exceeds the capital cushion, and depositors cannot be satisfied(bank run)\n\ncredit risk, which happens when borrowers of funds default on their obligations and don’t give back those funds.\n\nBanks compensate those risks by charging a spread between interest of\nloans and interest of deposits, on top of a margin to pay for their\noperations. The banking system has traditionally being the central core\nof the financial system in Europe.","type":"content","url":"/markdown/intro-financial-markets#the-banking-system","position":3},{"hierarchy":{"lvl1":"Financial Markets","lvl2":"Financial markets"},"type":"lvl2","url":"/markdown/intro-financial-markets#financial-markets","position":4},{"hierarchy":{"lvl1":"Financial Markets","lvl2":"Financial markets"},"content":"In financial markets, those who need funds issue financial instruments,legally binding contracts that articulate the terms in which those funds will be returned to the investor, as well as the compensation for the service. So far that is not that different from traditional banking loans. The key for financial instruments is the possibility of\ntransferring the property of the contract, i.e. the right to receive back those future cash-flows and compensations. This provides a mechanism for the lender to potentially recover the funds before the contract ends, solving the problem of liquidity even if contracts are issued with long-term horizons.\n\nOf course, for that the lender needs to find a counterpart that is willing to purchase the financial instrument, and agree to the price.\nPricing financial instruments is however not a simple task, since their value depend on assumptions about the certainty of those future cash-flows (for instance, what if the borrower cannot commit in the future to pay back?). There are also considerations of opportunity costs, since the potential investor will necessarily compare the return\non its investment when acquiring the financial instrument with other potential productive uses of her money. Financial markets developed precisely to solve these two problems: 1) finding counterparts, 2) setting a price.\n\nFinancial markets are the place where interested parties meet and negotiate the prices of financial instruments. In primary markets, financial instruments are issued by the borrowers and acquire initially by investors, sometimes via intermediaries like banks. In secondary markets, already issued financial instruments are negotiated between investors, with the original borrower not playing any part any more in the process. Most of trading nowadays occurs in secondary markets.\n\nIn this process, financial markets not only provide of a mechanism for investors to obtain liquidity from their investments, but also serve as a mechanism for price discovery. In the end, in financial markets (as in\nother markets), the price of a financial instrument is what two willing parties agree to transact, independently of the subjective value that each of them place on the instrument. Of course, by pulling multiple investors in the same place to negotiate, financial markets channel multiple speculative ways about the value of a financial instrument into\nan actual price. These prices can guide investors about future\ndecisions.","type":"content","url":"/markdown/intro-financial-markets#financial-markets","position":5},{"hierarchy":{"lvl1":"Financial Markets","lvl2":"Who Participates in Financial Markets?"},"type":"lvl2","url":"/markdown/intro-financial-markets#who-participates-in-financial-markets","position":6},{"hierarchy":{"lvl1":"Financial Markets","lvl2":"Who Participates in Financial Markets?"},"content":"Financial markets primarily involve legal entities, although natural persons —individuals—may also participate, typically through intermediaries such as banks or brokers. The core participants issuing financial instruments are varied: corporations issue securities to fund their business activities, while governments at all levels—local, regional, national, and supranational—issue bonds to finance social programs or public investments. Additionally, banks design and issue customized products aimed at meeting the investment needs of institutional and individual investors.\n\nOn the demand side, institutional investors such as hedge funds, mutual funds, pension funds, and insurance companies play a crucial role in acquiring financial instruments. Their goal is either to generate returns for their clients or manage specific risks. Corporations also buy financial instruments, both to make their excess cash productive and to hedge against business risks. Similarly, banks acquire these instruments as part of their liquidity management strategy, to generate returns on customer deposits, and to mitigate various financial risks. Central banks are key participants as well, utilizing financial instruments to execute their monetary policies, influencing money supply and interest rates.\n\nIn terms of intermediation, banks and brokers facilitate market transactions. Banks, particularly in their role as market-makers, are key liquidity providers. They stand ready to buy or sell financial instruments, profiting from the spread—the difference between the buying and selling price—which compensates them for holding the instrument until an opposing counterparty emerges. Brokers, on the other hand, connect buyers and sellers through their networks or proprietary platforms, charging a fee for their matchmaking services without holding the financial instruments themselves.\n\nAn increasingly important role in financial markets today is played by new liquidity providers —technology-driven financial firms. Many of these firms originated from high-frequency trading and now use advanced algorithms to provide liquidity in electronic markets. This new breed of liquidity providers competes with traditional market-makers, leveraging automation and sophisticated trading strategies to ensure tighter spreads and more efficient markets.","type":"content","url":"/markdown/intro-financial-markets#who-participates-in-financial-markets","position":7},{"hierarchy":{"lvl1":"Financial Markets","lvl2":"Types of Financial Instruments and Asset Classes"},"type":"lvl2","url":"/markdown/intro-financial-markets#types-of-financial-instruments-and-asset-classes","position":8},{"hierarchy":{"lvl1":"Financial Markets","lvl2":"Types of Financial Instruments and Asset Classes"},"content":"Financial instruments are the building blocks of financial markets, representing the contracts or securities through which money flows. These instruments are typically grouped into distinct asset classes based on their characteristics, the types of returns they provide, and the markets they operate within. Understanding the different types of financial instruments and asset classes is essential for comprehending how financial markets function and how participants make investment decisions.\n\nThe major asset classes include equity, fixed income, money markets, derivatives, foreign exchange (FX), commodities, and cryptocurrencies. Each asset class plays a distinct role in the financial ecosystem, catering to different investor needs, risk appetites, and financial goals.","type":"content","url":"/markdown/intro-financial-markets#types-of-financial-instruments-and-asset-classes","position":9},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Equity and Fixed Income","lvl2":"Types of Financial Instruments and Asset Classes"},"type":"lvl3","url":"/markdown/intro-financial-markets#equity-and-fixed-income","position":10},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Equity and Fixed Income","lvl2":"Types of Financial Instruments and Asset Classes"},"content":"The most well-known financial instruments are stocks and bonds, which form the foundation of two major asset classes: equity and fixed income.\n\nStocks represent ownership in a corporation and give investors a claim on a portion of the company’s assets and earnings. Stocks are part of the equity asset class, providing potential for capital appreciation and dividends. However, they also expose investors to market volatility and business risk, making them a relatively high-risk, high-reward investment.\n\nBonds, on the other hand, are debt securities issued by governments, corporations, or other entities to raise capital. Investors in bonds lend money to the issuer in exchange for periodic interest payments and the return of the bond’s face value at maturity. Bonds belong to the fixed income asset class, which generally offers stable and predictable returns, although they are still subject to risks such as interest rate fluctuations and credit defaults. Actually, within the fixed income asset class we typically distinguish between rates and credit instruments:\n\nRates instruments are instruments whose value is primarily driven by changes in interest rates (e.g. government bonds yields, central bank policy rates). In the case of bonds, government bonds are classified as rates instruments.\n\nCredit instruments are primarily driven by the creditworthiness of the issuer. In the case of bonds, corporate bonds. As we will discuss in chapter \n\nMechanics of Financial Instruments, in the case of corporate bonds, their value is also influenced by interest rates, although in general the effect is more marginal.\n\nBoth stocks and bonds are often referred to as cash instruments, meaning their value derives directly from the underlying market dynamics, without the need for an intermediary asset.","type":"content","url":"/markdown/intro-financial-markets#equity-and-fixed-income","position":11},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Money Markets","lvl2":"Types of Financial Instruments and Asset Classes"},"type":"lvl3","url":"/markdown/intro-financial-markets#money-markets","position":12},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Money Markets","lvl2":"Types of Financial Instruments and Asset Classes"},"content":"Within the fixed income category, there is a sub-sector known as the money market, which some consider an asset class of its own due to its distinct features. Money market instruments are short-term debt securities that typically mature in less than a year, such as Treasury bills, commercial paper, and certificates of deposit. These instruments are highly liquid and relatively low-risk, making them an attractive option for institutions and governments looking to manage short-term liquidity needs, as well as for investors seeking a safe place to park cash temporarily.","type":"content","url":"/markdown/intro-financial-markets#money-markets","position":13},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Derivatives","lvl2":"Types of Financial Instruments and Asset Classes"},"type":"lvl3","url":"/markdown/intro-financial-markets#derivatives","position":14},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Derivatives","lvl2":"Types of Financial Instruments and Asset Classes"},"content":"Derivatives are financial contracts whose value is derived from the performance of an underlying asset, index, or rate. They can be based on a wide range of underlying assets, including stocks, bonds, commodities, currencies, and interest rates. This is why some categorize derivatives within the asset class of the underlying instrument, though they are often considered an asset class of their own due to their unique characteristics.\n\nDerivatives are versatile and serve several purposes, such as hedging risks, speculating on price movements, or leveraging positions. Common types of derivatives include futures, options, swaps, and forwards, each offering different structures and risk profiles. For example, a company might use derivatives to hedge against fluctuations in interest rates or commodity prices, while a trader might use options to speculate on the price of a stock.","type":"content","url":"/markdown/intro-financial-markets#derivatives","position":15},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Foreign Exchange (FX)","lvl2":"Types of Financial Instruments and Asset Classes"},"type":"lvl3","url":"/markdown/intro-financial-markets#foreign-exchange-fx","position":16},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Foreign Exchange (FX)","lvl2":"Types of Financial Instruments and Asset Classes"},"content":"The foreign exchange (FX) market is a critical component of the global financial system, where participants trade currencies. Transactions in the FX market are generally categorized as either spot** or derivative transactions.\n\nSpot transactions involve the immediate exchange of currencies, typically settling within two business days. While FX spot transactions are essential for international trade and finance, they are not classified as financial instruments under MiFID II, as they do not involve contractual obligations extending beyond the settlement period.\n\nFX derivatives, such as forwards, options, and swaps, are used to hedge currency risk or speculate on currency movements. Unlike spot transactions, these contracts involve specific obligations between parties and are therefore considered financial instruments.\n\nFX markets are used by a wide range of participants, including central banks (to manage currency reserves), corporations (to hedge currency risk in international operations), and investors looking to profit from currency fluctuations.","type":"content","url":"/markdown/intro-financial-markets#foreign-exchange-fx","position":17},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Commodities","lvl2":"Types of Financial Instruments and Asset Classes"},"type":"lvl3","url":"/markdown/intro-financial-markets#commodities","position":18},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Commodities","lvl2":"Types of Financial Instruments and Asset Classes"},"content":"Commodities are physical goods such as oil, gold, and agricultural products, traded primarily in spot markets. These goods themselves are not considered financial instruments, as they represent tangible assets rather than financial claims. However, the derivatives based on commodities, such as futures and options on commodities, are classified as financial instruments. These contracts enable investors to gain exposure to commodity price movements without needing to take physical delivery of the underlying goods, providing opportunities for hedging and speculation.","type":"content","url":"/markdown/intro-financial-markets#commodities","position":19},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Cryptocurrencies","lvl2":"Types of Financial Instruments and Asset Classes"},"type":"lvl3","url":"/markdown/intro-financial-markets#cryptocurrencies","position":20},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Cryptocurrencies","lvl2":"Types of Financial Instruments and Asset Classes"},"content":"Cryptocurrencies, such as Bitcoin and Ethereum, are a relatively new and rapidly evolving asset class. Unlike traditional financial instruments, cryptocurrencies do not represent contractual obligations or financial claims. Instead, they function as digital assets, leveraging blockchain technology to provide decentralized and transparent transactions. Their value is driven by supply and demand dynamics, making them highly volatile compared to other asset classes.\n\nAlthough cryptocurrencies themselves are not classified as financial instruments under existing regulatory frameworks, derivatives on cryptocurrencies (such as Bitcoin futures) are considered financial contracts. These derivatives allow market participants to speculate on or hedge against the price movements of cryptocurrencies, much like they would with other asset classes.","type":"content","url":"/markdown/intro-financial-markets#cryptocurrencies","position":21},{"hierarchy":{"lvl1":"Financial Markets","lvl2":"Financial Market Structures"},"type":"lvl2","url":"/markdown/intro-financial-markets#financial-market-structures","position":22},{"hierarchy":{"lvl1":"Financial Markets","lvl2":"Financial Market Structures"},"content":"Market structures play a critical role in the functioning of financial markets, influencing how participants interact and how trades are executed. Traditionally, markets have been organized to accommodate the needs of intermediaries, such as dealers, who facilitate trading by providing liquidity. However, market structures have evolved significantly, shaped by technology, competition, and regulatory changes.\n\nFinancial markets can broadly be categorized into three main structures based on the role of intermediaries and the nature of participation:\n\nInter-Dealer Markets: Inter-dealer markets are venues where intermediaries, often referred to as dealers or market makers, trade exclusively with each other. The primary purpose of these markets is to allow dealers to manage their inventories effectively, enabling them to better serve their clients in dealer-to-client markets. The main types of inter-dealer markets are exchanges and inter-dealer broker networks. Notice that, while traditionally exclusive to dealers, some inter-dealer markets now allow institutional investors to participate directly through membership or other access arrangements, called Direct Market Access (DMA)\n\nDealer-to-Client Markets: Dealer-to-client markets are the most common venues where intermediaries interact with investors, including institutional and retail clients. Here, dealers act as liquidity providers, offering quotes to clients who wish to buy or sell financial instruments. These markets are typically either quote-driven, where prices are determined by dealers, or order-driven, where buyers and sellers are matched directly. Increasingly, electronic platforms have replaced traditional voice-based trading, improving efficiency and transparency.\n\nAlternative Markets: Alternative markets represent a significant departure from traditional market structures by eliminating the segmentation between dealers and clients. These markets are often referred to as “all-to-all” trading platforms because they allow any participant—dealer or client—to interact and trade under the same conditions. Alternative markets have gained prominence due to technological advancements and regulatory pressures aimed at increasing competition and reducing trading costs. These markets often provide a mix of trading mechanisms, such as order books or dark pools, that facilitate liquidity without relying exclusively on dealers.","type":"content","url":"/markdown/intro-financial-markets#financial-market-structures","position":23},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"The Evolution of Market Structures","lvl2":"Financial Market Structures"},"type":"lvl3","url":"/markdown/intro-financial-markets#the-evolution-of-market-structures","position":24},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"The Evolution of Market Structures","lvl2":"Financial Market Structures"},"content":"Market structures have evolved through several stages, driven by the interplay of standardization, technological progress, competition, and regulation:\n\nSingle-Dealer to Client: Initially, trading was dominated by single-dealer platforms, where clients could only trade directly with one intermediary. These structures were common in the early stages of market development or for niche and complex products.\n\nMulti-Dealer to Client: As markets matured, competition among dealers increased, leading to platforms that allowed clients to interact with multiple dealers simultaneously. This structure enhanced price transparency and liquidity for investors.\n\nDealer-to-Dealer Markets: The rise of inter-dealer markets enabled dealers to manage their inventories more effectively, creating a robust secondary layer of liquidity. Over time, some of these markets allowed direct access to clients through mechanisms like direct market access (DMA).\n\nAll-to-All Markets: Recent regulatory reforms, such as MiFID II in Europe and Reg NMS in the USA, have encouraged the development of all-to-all markets. These structures aim to lower trading costs by eliminating intermediary fees and creating a more level playing field for all participants.","type":"content","url":"/markdown/intro-financial-markets#the-evolution-of-market-structures","position":25},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Key Trends Shaping Modern Market Structures","lvl2":"Financial Market Structures"},"type":"lvl3","url":"/markdown/intro-financial-markets#key-trends-shaping-modern-market-structures","position":26},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Key Trends Shaping Modern Market Structures","lvl2":"Financial Market Structures"},"content":"Modern market structures are influenced by three key trends:\n\nAccessibility: The shift toward all-to-all markets has made it easier for participants to access liquidity directly, bypassing traditional intermediaries.\n\nTransparency: Regulatory frameworks, particularly in Europe and the USA, have emphasized greater transparency in trading, requiring platforms to disclose pricing and trade information.\n\nFragmentation: Regulatory reforms seeking to increase competition in trading venues have encouraged the emergence of alternative markets where financial instruments can be traded. This has produced a fragmentation of liquidity, which used to be concentrated in a single exchange.\n\nTechnological Advancements: The move from voice-based trading to electronic platforms has transformed the efficiency and scalability of markets. Electronic trading systems now dominate most asset classes, enabling faster execution and broader market participation.","type":"content","url":"/markdown/intro-financial-markets#key-trends-shaping-modern-market-structures","position":27},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Market structure according to specific regulations","lvl2":"Financial Market Structures"},"type":"lvl3","url":"/markdown/intro-financial-markets#market-structure-according-to-specific-regulations","position":28},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Market structure according to specific regulations","lvl2":"Financial Market Structures"},"content":"Different regulations have introduced specific terminology to describe market structures, providing their own classification of market structures.\n\nIn the US, regulation distinguishes between Exchange trading venues and Alternative Trading Systems (ATS), the latter grouping all non-exchange trading venues like ECNs, IDBs and Dark Pools. Dodd-Frank also introduces the concept of Swap Execution Facilities (SEF) as a trading platform to execute standardized derivatives, particularly swaps, ensuring pre-trade and post-trade transparency, efficiency and regulatory compliance.\n\nIn Europe, MiFID II four types of market structure are defined:\n\nRegulated Markets: which can be mapped to exchanges\n\nMultilateral Trading Facilities (MTFs): similar to ATS in the USA regulation, e.g. covering non-exchange trading venues like ECNs and dark pools. European regulation also introduces the concept of Organized Trading Facilities (OTFs), restricted to non-equities instruments and with more lax requirements, but in terms of structure is similar to MTFs.\n\nSystematic Internalizers (SIs): not strictly a market structure, when a broker or dealer executes orders on own account outside of RMs, MTFs or OTFs.","type":"content","url":"/markdown/intro-financial-markets#market-structure-according-to-specific-regulations","position":29},{"hierarchy":{"lvl1":"Financial Markets","lvl2":"The main building blocks of financial market structure"},"type":"lvl2","url":"/markdown/intro-financial-markets#the-main-building-blocks-of-financial-market-structure","position":30},{"hierarchy":{"lvl1":"Financial Markets","lvl2":"The main building blocks of financial market structure"},"content":"","type":"content","url":"/markdown/intro-financial-markets#the-main-building-blocks-of-financial-market-structure","position":31},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Exchanges","lvl2":"The main building blocks of financial market structure"},"type":"lvl3","url":"/markdown/intro-financial-markets#exchanges","position":32},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Exchanges","lvl2":"The main building blocks of financial market structure"},"content":"Exchanges (also known are regulated markets) are among the oldest institutions in financial markets, serving as centralized venues where buyers and sellers meet to trade financial instruments. They provide a structured environment for trading, ensuring transparency, fairness, and efficiency in the execution of transactions. Historically, exchanges were physical locations where traders gathered to negotiate prices through verbal communication and hand signals. Over time, these systems evolved into highly sophisticated electronic platforms that now dominate the financial landscape.\n\nThe primary role of an exchange is to facilitate the matching of buy and sell orders, ensuring that trades are executed efficiently and at market-driven prices. This process of price discovery is one of the most critical functions of an exchange. By aggregating supply and demand, exchanges reflect the collective valuation of financial instruments, providing a transparent benchmark for investors. This transparency extends to the dissemination of information, as exchanges publicly display orders and executed trades, enhancing market confidence and reducing information asymmetry among participants.\n\nAnother key function of exchanges is their contribution to liquidity. By concentrating a large number of buyers and sellers in a single venue, exchanges enable market participants to trade efficiently, minimizing the costs and delays associated with finding counterparties. For many financial instruments, the presence of a liquid exchange can mean the difference between a vibrant market and one that is illiquid and inaccessible.\n\nAccess to exchanges has traditionally been restricted to members, such as brokers and dealers, who execute trades on behalf of their clients or for their own accounts. However, with the advent of direct market access (DMA), institutional investors can now connect to exchanges directly using broker-provided infrastructure. This shift has allowed greater participation and has reduced the reliance on intermediaries, further increasing efficiency.\n\nOver the years, exchanges have adapted to changing technologies and regulatory environments. The open-outcry systems of the past, where trading occurred on physical floors, have been largely replaced by electronic order books. These systems enable the seamless matching of orders, often in milliseconds, and ensure that market activity is highly transparent. Despite these advancements, the core purpose of exchanges—to facilitate fair and efficient trading—has remained unchanged.\n\nExchanges cater to a wide range of asset classes, including equities, fixed income, and derivatives. In the equities market, prominent exchanges such as the New York Stock Exchange (NYSE) and NASDAQ in the United States, and the London Stock Exchange (LSE) and Deutsche Börse in Europe, dominate. In the derivatives market, institutions like the Chicago Mercantile Exchange (CME) and Eurex play a similar role, providing venues for trading standardized contracts. The evolution of exchanges has also seen them diversify their offerings, with many now facilitating the trading of newer instruments like exchange-traded funds (ETFs) and cryptocurrencies.\n\nWhile exchanges offer many advantages, including transparency, reduced counterparty risk, and high efficiency, they face increasing competition from alternative trading systems. These include dark pools and electronic communication networks, which operate outside traditional exchange frameworks. Regulatory reforms, such as MiFID II in Europe and Reg NMS in the United States, have intensified this competition by encouraging the creation of private trading venues that rival exchanges in functionality.\n\nDespite these challenges, exchanges continue to be a cornerstone of financial markets. Their ability to adapt to technological advancements and regulatory shifts ensures their relevance in an ever-changing landscape. By providing a centralized and regulated platform for trading, exchanges remain indispensable for fostering market confidence and maintaining the integrity of financial systems.","type":"content","url":"/markdown/intro-financial-markets#exchanges","position":33},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Inter-Dealer Broker Networks (IDBs)","lvl2":"The main building blocks of financial market structure"},"type":"lvl3","url":"/markdown/intro-financial-markets#inter-dealer-broker-networks-idbs","position":34},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Inter-Dealer Broker Networks (IDBs)","lvl2":"The main building blocks of financial market structure"},"content":"Inter-dealer broker (IDB) networks play a crucial role in financial markets by acting as intermediaries between dealers, facilitating transactions that would otherwise be difficult to execute directly. These networks are particularly important in over-the-counter (OTC) markets, where trading is less centralized and often involves complex, less standardized financial instruments.\n\nUnlike exchanges, which are open to a wide range of participants, inter-dealer broker networks operate exclusively within the realm of dealers, such as banks and other financial institutions that act as market makers. These networks exist to bridge gaps in supply and demand among dealers, ensuring that liquidity remains available even in the most challenging market conditions. They provide a venue for dealers to manage inventory imbalances, hedge positions, and execute large trades without disrupting the broader market.\n\nHistorically, inter-dealer broker networks relied heavily on voice-based trading, where brokers would connect dealers over the phone to negotiate trades. This method was particularly suited to illiquid or bespoke instruments, where the nuances of each deal required direct human interaction. However, the rise of electronic trading has transformed these networks, with many now offering electronic platforms that support faster and more efficient trade execution. These platforms often aggregate quotes from multiple dealers or, in some cases, provide fully functional order books similar to those found on exchanges. For highly liquid instruments, such as government bonds or certain derivatives, electronic trading has become the dominant mechanism within inter-dealer networks.\n\nThe services provided by inter-dealer brokers extend beyond mere trade execution. They act as neutral intermediaries, ensuring that transactions remain anonymous to prevent sensitive information about trading strategies or inventory levels from leaking to competitors. This anonymity is a key feature of inter-dealer broker networks, particularly in markets where large trades can significantly impact prices.\n\nInter-dealer broker networks are especially vital in markets that are predominantly OTC, such as fixed income, foreign exchange, and derivatives. For example, platforms like BrokerTec and MTS Cash are central to the trading of government and corporate bonds, while ICAP and CME SwapStream dominate in the trading of swaps. In the foreign exchange market, inter-dealer platforms like EBS aggregate liquidity and facilitate price discovery across major currency pairs.\n\nIn recent years, regulatory changes and technological advancements have reshaped the inter-dealer broker landscape. Regulations such as MiFID II and Dodd-Frank have introduced new transparency and reporting requirements, pushing brokers to modernize their operations and adopt electronic trading systems. These changes have enhanced efficiency but also increased competition among brokers and other trading venues. At the same time, some inter-dealer networks have begun offering limited access to institutional investors through mechanisms like direct market access (DMA), further blurring the lines between traditional inter-dealer and dealer-to-client markets.\n\nDespite these shifts, inter-dealer broker networks remain a cornerstone of financial markets, providing critical infrastructure for OTC trading. They ensure that dealers can manage risk, adjust positions, and execute large trades without destabilizing markets. As technology continues to advance, these networks are likely to evolve further, integrating new tools to enhance their efficiency and adaptability while continuing to fulfill their essential role in the financial system.","type":"content","url":"/markdown/intro-financial-markets#inter-dealer-broker-networks-idbs","position":35},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Electronic Communication Networks (ECNs)","lvl2":"The main building blocks of financial market structure"},"type":"lvl3","url":"/markdown/intro-financial-markets#electronic-communication-networks-ecns","position":36},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Electronic Communication Networks (ECNs)","lvl2":"The main building blocks of financial market structure"},"content":"Electronic Communication Networks (ECNs) are digital platforms that connect buyers and sellers directly, providing a decentralized and efficient mechanism for trading financial instruments. As traditional exchanges have become electronic trading venues, the differences between them and ECNs is blurring. Both of them typically operate as “lit” platforms, based on order books that are fully transparent, offering real-time insights into market depth and pricing. The main differences are essentially:\n\nthe role of intermediaries like brokers or dealers, who have a monopoly of access and a dominant role as market-makers in exchanges\n\nthat exchanges are still considered the official pricing references for  financial instruments, used for end of day valuations of inventories and settlement of contracts.\n\nFor those asset classes, like FX, where traditional exchanges don’t exist, ECNs have a prominent role. Examples are ECNs like FXAll, Hotspot FX and 360T. A similar situation happens for Crypto, which as a relatively new asset class, does not trade in traditional exchanges and has also adopted ECN-like models. Platforms such as Binance and Coinbase Pro operate as order-driven trading venues, offering functionality similar to standard ECNs but tailored to the unique dynamics of digital assets.\n\nThanks to innovative trading functionalities and competitive fees, ECNs are also becoming more relevant for financial instruments mostly traded in exchanges. In the equity markets, Turquoise, a pan-european ECN, provides efficient trading across multiple equity markets, while CBOE (formerly BATS) has grown to become one of the largest equity trading platforms in the world.\n\nIn fixed income, ECNs such as BrokerTec and MarketAxess have introduced new efficiencies. BrokerTec specializes in government bond trading and repos, while MarketAxess focuses on corporate bonds, integrating both ECN functionality and Request-for-Quote (RfQ) systems. These platforms enable dealers and institutional investors to trade with greater speed and transparency in traditionally opaque markets.\n\nDespite their many benefits, ECNs face challenges, including market fragmentation and regulatory scrutiny. As multiple ECNs compete for liquidity, market participants often need to navigate between venues to find the best prices. Regulations like MiFID II in Europe and Reg NMS in the United States have introduced transparency and operational requirements that ECNs must comply with, adding complexity to their operations. Additionally, the technological demands of maintaining ultra-low latency systems make ECNs resource-intensive to operate.\n\nNevertheless, ECNs have fundamentally reshaped financial markets by increasing accessibility, reducing transaction costs, and fostering competition. They have become a cornerstone of modern trading, offering a blend of transparency, efficiency, and innovation. As markets evolve, ECNs will likely continue to expand their reach, further blurring the lines between traditional and alternative trading venues.","type":"content","url":"/markdown/intro-financial-markets#electronic-communication-networks-ecns","position":37},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Dark Pools","lvl2":"The main building blocks of financial market structure"},"type":"lvl3","url":"/markdown/intro-financial-markets#dark-pools","position":38},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Dark Pools","lvl2":"The main building blocks of financial market structure"},"content":"Dark pools are private trading venues where buy and sell orders are matched without being publicly displayed. These platforms operate outside traditional exchanges and are designed to provide anonymity and minimize market impact, particularly for large trades. Dark pools have become an essential part of the financial ecosystem, catering primarily to institutional investors who seek to execute sizable orders without revealing their intentions to the broader market.\n\nThe key feature of dark pools is their opacity. Unlike lit markets, where order books are visible to all participants, dark pools conceal order details until after a trade is executed. This anonymity helps mitigate the risk of adverse price movements, which can occur when other market participants detect large orders and trade against them. For institutional investors, this reduces execution costs and enables more efficient handling of large positions.\n\nDark pools emerged as a response to the limitations of traditional exchanges, particularly for block trading. As markets became more fragmented and lit venues introduced greater transparency requirements, the demand for private trading mechanisms grew. Today, dark pools are operated by a variety of entities, including investment banks, brokerage firms, and even exchanges themselves. Prominent examples include Credit Suisse Crossfinder, ITG POSIT, and Liquidnet, which cater to equity markets, and Hotspot QT, which offers dark liquidity for foreign exchange trading.\n\nIn terms of functionality, dark pools typically use one of three matching mechanisms:\n\nDark Order Books: These pools match hidden limit orders based on price and time priority.\n\nMidpoint Matching: Some dark pools use the midpoint of the bid and ask prices from a lit market as a reference price for matching trades.\n\nPass-Through Mechanisms: Unmatched orders in dark pools may be routed to lit markets for execution.\n\nDark pools are most prevalent in equity markets, where they account for a significant portion of trading volume in major financial centers. However, their influence has also expanded into other asset classes, including foreign exchange and fixed income. For example, platforms like Liquidnet and Crossfinder are frequently used by institutional investors to execute large equity trades, while Hotspot QT caters to FX transactions.\n\nDespite their advantages, dark pools have attracted scrutiny from regulators and market participants. Critics argue that the lack of transparency can lead to unfair trading practices and reduce the quality of price discovery in lit markets. In response, regulatory initiatives like MiFID II in Europe have introduced stricter oversight and reporting requirements for dark pool activity. In the United States, similar efforts under the SEC aim to ensure that dark pools operate fairly and do not disadvantage other market participants.\n\nTechnological advancements have also shaped the evolution of dark pools. Many now integrate advanced algorithms and smart order routing systems to optimize trade execution. These tools analyze market conditions across lit and dark venues to determine the best execution strategy for a given order.\n\nIn summary, dark pools play a critical role in modern financial markets, providing a valuable tool for institutional investors to execute large trades with minimal market impact. While they continue to face challenges related to transparency and regulation, their importance in facilitating efficient and discreet trading ensures they remain a key component of the trading landscape.","type":"content","url":"/markdown/intro-financial-markets#dark-pools","position":39},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Systematic Internalizers","lvl2":"The main building blocks of financial market structure"},"type":"lvl3","url":"/markdown/intro-financial-markets#systematic-internalizers","position":40},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Systematic Internalizers","lvl2":"The main building blocks of financial market structure"},"content":"A Systematic Internalizer is an investment firm that executes client orders internally, using its own capital, rather than routing those orders to external trading venues like exchanges or Multilateral Trading Facilities (MTFs). Unlike traditional trading venues, SIs do not operate a central order book where multiple participants interact; instead, they provide quotes bilaterally to their clients.\n\nThe traditional role of an SI has been handling smaller, less liquid trades or the provision of tailored liquidity for institutional clients. By internalizing trades, they reduce reliance on external venues and offer greater control over execution. This mechanism ensures tighter spreads, faster execution, and potentially better pricing, making them attractive to sophisticated market participants.\n\nIn recent years a new breed of systematic internalizers have emerged as dominant players in the financial markets. They are the so-called new liquidity providers, non-bank firms with a strong focus on technology and automation, most of them having started as High-Frequency-Trading (HFT) firms. Examples of these firms are Citadel Securities and Virtu Financial. These firms operate as traditional market-makers in lit trading venues, but as SIs they have been instrumental in the execution of retail orders routed by online brokers like Robinhood. For them, retail orders provide a relatively noisy flow that can be matched internally without routing it to trading venues, providing relevant savings in fees and spreads that can be passed partially to the online brokers. Such trading model is known as payment for order flow (PFOF) Those, in turn, can offer commission-free trading to retail investors, cannibalizing traditional brokers that route orders to trading venues.\n\nThe dominance of firms like Citadel and their role in platforms such as Robinhood has drawn regulatory attention. Critics argue that PFOF and internalization may create potential conflicts of interest and reduce transparency compared to lit markets.","type":"content","url":"/markdown/intro-financial-markets#systematic-internalizers","position":41},{"hierarchy":{"lvl1":"Financial Markets","lvl2":"Overview of market structures per asset class"},"type":"lvl2","url":"/markdown/intro-financial-markets#overview-of-market-structures-per-asset-class","position":42},{"hierarchy":{"lvl1":"Financial Markets","lvl2":"Overview of market structures per asset class"},"content":"","type":"content","url":"/markdown/intro-financial-markets#overview-of-market-structures-per-asset-class","position":43},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Equity market structure","lvl2":"Overview of market structures per asset class"},"type":"lvl3","url":"/markdown/intro-financial-markets#equity-market-structure","position":44},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Equity market structure","lvl2":"Overview of market structures per asset class"},"content":"The equity market structure represents the intricate framework through which stocks and other equity instruments are traded. It has undergone a profound transformation over the years, influenced by technological advancements, regulatory reforms, and the diversification of trading venues. Modern equity markets are characterized by a blend of traditional exchanges, alternative trading systems, and off-exchange mechanisms, all working together to facilitate liquidity, price discovery, and trade execution.\n\nTraditional Exchanges\n\nAt the heart of the equity market structure are traditional exchanges such as the New York Stock Exchange (NYSE), NASDAQ, London Stock Exchange (LSE), and Deutsche Börse. These venues provide centralized platforms for trading shares of publicly listed companies. Historically, exchanges were physical locations where traders met to negotiate prices through open-outcry systems. Today, most trading occurs electronically, with Central Limit Order Books (CLOBs) being the dominant mechanism. CLOBs match buy and sell orders based on price and time priority, ensuring transparent and efficient execution. Exchanges play a critical role in price discovery by reflecting the collective views of market participants and aggregating supply and demand. They also provide regulatory oversight to ensure fairness and investor protection.\n\nThe Rise of Alternative Trading Systems\n\nIn recent years, the dominance of traditional exchanges has been challenged by the rise of Alternative Trading Systems (ATSs). These include Electronic Communication Networks (ECNs) and dark pools, which offer innovative alternatives for trade execution. As we have discussed above, ECNs are fully electronic platforms that match orders directly between participants, bypassing intermediaries. They are especially effective for high-frequency and algorithmic trading, where speed and tight spreads are essential. Dark pools, on the other hand, cater to institutional investors seeking anonymity when executing large trades. By concealing order details, dark pools minimize the market impact of block trades, helping reduce transaction costs. Prominent examples of dark pools operating in the Equity markets include Liquidnet and Credit Suisse Crossfinder, while ECNs like Instinet and BATS (now part of CBOE) have become critical to equity trading infrastructure.\n\nThe Growth of Off-Exchange Trading: Systematic internalizers\n\nAs we discussed previously, another significant development particularly relevant for the equity market structure is the growth of off-exchange trading. Internalization and wholesale market making have become dominant forces, driven by firms like Citadel Securities and Virtu Financial. These firms match buy and sell orders internally or execute them against their own inventories. This approach offers several advantages, including cost efficiency, faster execution, and price improvements for retail investors. Retail trading platforms like Robinhood rely heavily on these wholesale market makers, routing customer orders to them for execution. While this model democratizes access to equity markets, it has also sparked regulatory debates about transparency and potential conflicts of interest, particularly regarding payment for order flow (PFOF).\n\nThe Role of Regulation\n\nThe regulatory landscape has played a pivotal role in shaping equity market structures. In the United States, Regulation National Market System (Reg NMS), introduced in 2005, prioritizes best execution by requiring trades to be executed at the best available price across all venues. This regulation has fostered competition and innovation, but it has also contributed to market fragmentation. In Europe, the Markets in Financial Instruments Directive II (MiFID II) has brought stricter transparency requirements, mandating both pre-trade and post-trade reporting for equities. MiFID II also introduced volume caps on dark pool trading to preserve price discovery in lit markets while encouraging competition through Multilateral Trading Facilities (MTFs).\n\nOpportunities and Challenges\n\nThe evolution of equity markets has brought both opportunities and challenges. Market fragmentation, driven by the proliferation of trading venues, has increased the complexity of accessing consolidated liquidity. The rise of algorithmic and high-frequency trading has revolutionized how equities are traded, emphasizing speed and efficiency, but it has also raised concerns about market stability during volatile periods. Additionally, the surge in retail investor participation, facilitated by platforms like Robinhood, has changed market dynamics, increasing reliance on internalizers and raising questions about fairness.\n\nDespite these challenges, equity markets remain a cornerstone of the global financial system, adapting to technological advancements and regulatory changes while continuing to serve the needs of a diverse range of participants. Balancing competition, transparency, and efficiency will be essential as market structures evolve further in response to new innovations and global economic trends.","type":"content","url":"/markdown/intro-financial-markets#equity-market-structure","position":45},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Fixed income market structure","lvl2":"Overview of market structures per asset class"},"type":"lvl3","url":"/markdown/intro-financial-markets#fixed-income-market-structure","position":46},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Fixed income market structure","lvl2":"Overview of market structures per asset class"},"content":"The fixed income market, which encompasses government bonds, corporate bonds, and other debt instruments, plays a central role in the global financial system. It provides a mechanism for governments, corporations, and institutions to raise capital through borrowing while offering investors a source of steady income and portfolio diversification. Unlike equity markets, the fixed income market is primarily over-the-counter (OTC) in nature, meaning that trades are negotiated bilaterally between participants rather than through centralized exchanges. This characteristic defines its structure, trading mechanisms, and the way liquidity is provided.\n\nMarket Segments\n\nThe segmentation between inter-dealer markets and dealer-to-client markets is particularly prominent in the fixed income market structure.\n\nThe inter-dealer market facilitates trading among dealers or market makers, allowing them to adjust their inventories and manage risk. These transactions often take place on specialized platforms such as BrokerTec and MTS, which cater to highly liquid instruments like government bonds. These platforms operate using Central Limit Order Books (CLOBs), where prices and order depths are visible to participants, ensuring transparency and efficient price discovery.\n\nThe dealer-to-client market, on the other hand, is where dealers interact directly with institutional investors such as asset managers, pension funds, and insurance companies. This segment is typically less transparent than the inter-dealer market, as prices and order flows are not widely disseminated. Instead, trading often occurs through Request-for-Quote (RfQ) systems, where clients request prices from multiple dealers before executing a trade. Platforms like Tradeweb, Bloomberg Terminal, and MarketAxess dominate this space, providing electronic solutions that streamline the negotiation process. In this market, liquidity is heavily dependent on the willingness of dealers to provide quotes, making it less consistent than in more standardized markets like equities or foreign exchange.\n\nLiquidity and Instrument Types\n\nGovernment bonds represent one of the most liquid segments of the fixed income market, with electronic trading accounting for more than 60-70% of activity at the beginning of the 2020s. These bonds are often traded in large volumes and benefit from a broad base of market participants, including central banks, hedge funds, and proprietary trading firms. Platforms like BrokerTec and Eurex Repo are central to the trading of government debt, offering robust infrastructure and high levels of transparency. Corporate bonds, by contrast, are less liquid, with electronic trading comprising only 30-40% of volume, primarily for smaller trades. Larger corporate bond trades, often referred to as block trades, are typically negotiated directly between counterparties or through specialized trading desks.\n\nThe Impact of Technology and Innovation\n\nThe evolution of technology and the rise of electronic trading have significantly influenced the fixed income market structure. While voice trading continues to play a role, particularly for bespoke or illiquid instruments, electronic platforms have gained prominence. These platforms not only improve efficiency and reduce transaction costs but also provide pre-trade and post-trade transparency. MarketAxess, for example, has pioneered innovations like Open Trading, a model that allows all-to-all trading, enabling any participant to act as a liquidity provider. This is a notable departure from the traditional dealer-centric model.\n\nNon-bank liquidity providers, such as Citadel Securities and Virtu Financial, have also emerged as influential players in the fixed income market. These firms leverage advanced algorithms and high-frequency trading strategies to provide continuous two-way pricing, challenging the traditional dominance of bank dealers. Their presence has narrowed bid-ask spreads and improved execution quality for many instruments, though their focus often remains on the most liquid securities.\n\nThe Role of Regulation\n\nRegulation has played a significant role in shaping the fixed income market structure, particularly in the wake of the 2008 financial crisis. In Europe, MiFID II introduced stringent transparency requirements, mandating pre-trade and post-trade reporting for bond transactions. As we discussed previously, this has increased the availability of market data via APAs, improving price discovery, but on the other hand it has imposed relevant compliance costs on market participants. In the United States, the Dodd-Frank Act emphasized centralized clearing and execution for fixed income derivatives, reducing counterparty risk and increasing oversight.\n\nChallenges and Future Directions\n\nDespite these advancements, the fixed income market continues to face challenges. Liquidity remains fragmented, particularly for less liquid instruments like high-yield bonds or municipal debt. The reliance on dealer-provided liquidity means that market conditions can deteriorate rapidly during periods of stress, as seen during the COVID-19 market dislocation in 2020. Moreover, the adoption of electronic trading, while growing, still lags behind other asset classes, reflecting the bespoke nature of many fixed income instruments.","type":"content","url":"/markdown/intro-financial-markets#fixed-income-market-structure","position":47},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"FX market structure","lvl2":"Overview of market structures per asset class"},"type":"lvl3","url":"/markdown/intro-financial-markets#fx-market-structure","position":48},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"FX market structure","lvl2":"Overview of market structures per asset class"},"content":"The foreign exchange (FX) market is the largest and most liquid financial market in the world, where currencies are traded across a decentralized global network. Unlike equity markets, the FX market lacks a central exchange and instead operates through a vast web of interconnected trading platforms, banks, and financial institutions. This decentralized structure allows the market to function continuously, 24 hours a day, across different time zones.\n\nMarket Segments\n\nThe FX market also exhibits a prominent segmentation between inter-dealer and dealer-to-client markets. In the inter-dealer market, major banks and liquidity providers trade currencies among themselves to balance their inventories and manage risk. This segment is characterized by high volumes and tight spreads, as participants consist of sophisticated institutions. Dealer-to-client markets, on the other hand, involve transactions between banks or liquidity providers and their clients, such as corporations, asset managers, or retail traders. Here, pricing is often less competitive, as it reflects the service costs of providing tailored liquidity to end users.\n\nTrading in the FX market is facilitated by a mix of single-dealer platforms, multi-dealer platforms, and electronic communication networks (ECNs). Single-dealer platforms are proprietary systems operated by large banks, such as Citi’s Velocity and Deutsche Bank’s Autobahn. These platforms provide clients with direct access to streaming prices and customized liquidity. Multi-dealer platforms like Refinitiv FXall and 360T aggregate prices from multiple liquidity providers, enabling clients to compare quotes and execute trades more efficiently. ECNs, such as EBS (Electronic Brokerage System) and CBOE FX, serve as centralized hubs for inter-dealer trading. ECNs operate using an order-driven model, similar to a stock exchange, where participants can place and match orders anonymously, ensuring tight spreads and deep liquidity for major currency pairs like EUR/USD and USD/JPY.\n\nAnother important component of FX market structure is the over-the-counter (OTC) nature of most transactions. Unlike exchange-traded assets, FX trades are conducted bilaterally between counterparties, with terms negotiated directly. While this flexibility allows for customization, it also introduces risks such as counterparty default. To address these risks, central counterparties (CCPs) and clearinghouses have become increasingly important, particularly for FX derivatives, providing a level of security through post-trade clearing.\n\nThe Impact of Technology and Innovation\n\nThe rise of algorithmic trading has profoundly impacted the FX market. Algorithms now execute a significant portion of FX trades, particularly in the spot and forward markets. These systems are designed to optimize execution strategies, minimize market impact, and reduce transaction costs. High-frequency trading (HFT) firms, including non-bank liquidity providers like Citadel Securities and XTX Markets, have further transformed the landscape by injecting liquidity into the market and narrowing bid-ask spreads. These firms compete directly with traditional bank liquidity providers, challenging the conventional dominance of large financial institutions.\n\nThe Role of Regulation\n\nRegulation has also played a key role in shaping FX market structure. In the United States, the Dodd-Frank Act introduced Swap Execution Facilities (SEFs) for certain FX derivatives, such as non-deliverable forwards (NDFs) and options. SEFs mandate pre-trade transparency and centralized execution, bringing these instruments closer to the regulatory framework of listed markets. In Europe, MiFID II has extended reporting requirements to FX transactions, increasing transparency and ensuring compliance across borders.\n\nChallenges and Future Directions\n\nDespite its decentralized nature, the FX market faces challenges, including market fragmentation and unequal access to liquidity. Retail traders, for instance, often face wider spreads and less favorable execution compared to institutional players. Platforms like eToro and retail aggregators provide retail access to FX trading but rely on intermediaries to bridge the gap between retail and institutional markets. Furthermore, regulatory scrutiny of practices like last look, where liquidity providers can reject trades even after quoting prices, has raised questions about fairness and execution quality.","type":"content","url":"/markdown/intro-financial-markets#fx-market-structure","position":49},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Derivatives markets structure","lvl2":"Overview of market structures per asset class"},"type":"lvl3","url":"/markdown/intro-financial-markets#derivatives-markets-structure","position":50},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Derivatives markets structure","lvl2":"Overview of market structures per asset class"},"content":"The derivatives market structure encompasses a wide range of trading venues and mechanisms that facilitate the buying and selling of financial instruments whose value is derived from underlying assets. These instruments, including futures, options, swaps, and credit derivatives, are critical tools for hedging risk, speculating on price movements, and enhancing portfolio efficiency. The structure of this market is complex, reflecting the diversity of its participants, instruments, and trading mechanisms.\n\nCentralized and Decentralized Markets\n\nDerivatives markets are divided into two primary categories: centralized markets, where standardized contracts are traded on regulated exchanges, and decentralized over-the-counter (OTC) markets, where bespoke contracts are negotiated directly between counterparties.\n\nCentralized markets operate through exchanges like the Chicago Mercantile Exchange (CME), Eurex, and the Intercontinental Exchange (ICE). These venues provide a transparent and regulated environment for trading standardized derivatives, such as futures and options. Exchanges use Central Limit Order Books (CLOBs) to match buy and sell orders, ensuring efficient price discovery and liquidity. Additionally, centralized clearinghouses guarantee the performance of contracts, mitigating counterparty risk and enhancing market stability.\n\nIn contrast, OTC markets are decentralized networks where parties negotiate derivative contracts directly. OTC derivatives, including swaps and bespoke options, offer flexibility in terms of contract size, duration, and underlying asset. This customization makes OTC markets essential for addressing specific hedging or investment needs, especially for large institutional players. However, the bilateral nature of OTC trading introduces counterparty risk and limits transparency.\n\nProduct Segmentation\n\nThe derivatives market is further categorized by the instruments traded and the venues where they are exchanged:\n\nFutures and Options Markets: Futures and options are predominantly traded on exchanges due to their standardized nature. Venues like CME, Eurex, and the London International Financial Futures Exchange (LIFFE) dominate this segment.\nThese contracts provide liquidity and leverage for speculating on asset price movements or managing exposure to commodities, interest rates, or currencies.\n\nSwap Markets: Swaps, such as interest rate swaps (IRS) and cross-currency swaps, are primarily traded in OTC markets. Platforms like ICAP i-Swap, Tradeweb, and CME SwapStream have modernized this segment, introducing electronic trading and central clearing.\nIn the United States, the Dodd-Frank Act established Swap Execution Facilities (SEFs) to increase transparency and centralize trading for standardized swap contracts.\n\nCredit Derivatives: Credit derivatives, including CDS, allow market participants to hedge or speculate on credit risk. These instruments are traded OTC but are increasingly cleared through centralized clearinghouses like ICE Clear Credit to reduce systemic risk.\n\nRecent developments\n\nIn recent years, non-bank liquidity providers, such as Citadel Securities and Virtu Financial, have become critical players in derivatives markets. These firms use advanced algorithms and high-frequency trading systems to provide continuous liquidity, particularly in standardized instruments like futures and options. Their presence has narrowed bid-ask spreads, enhanced price discovery, and reduced transaction costs for participants. However, their influence remains limited in bespoke OTC markets, where traditional dealers and institutional clients dominate.\n\nTechnological advancements have significantly transformed derivatives markets, particularly in electronic trading and post-trade processes. Exchanges and OTC platforms alike have adopted advanced algorithms and smart order routing systems to optimize execution. Central clearinghouses now integrate with trading platforms to streamline settlement processes, reducing operational risk.\n\nThe rise of algorithmic trading has been especially pronounced in liquid derivatives markets like futures. High-frequency traders (HFTs) dominate these segments, leveraging speed and computational power to capture small price inefficiencies. This shift has improved market efficiency but has also introduced new challenges, including concerns about flash crashes and the impact of HFT on market stability.\n\nRegulation and Market Evolution\n\nThe derivatives market structure has been heavily influenced by regulatory reforms, particularly following the 2008 financial crisis. In the United States, the Dodd-Frank Act introduced sweeping changes, requiring standardized OTC derivatives to be traded on SEFs and cleared through central counterparties. In Europe, MiFID II extended pre-trade and post-trade transparency requirements to derivatives markets and introduced Organized Trading Facilities (OTFs) to regulate non-equity instruments.\n\nThese regulations aim to reduce systemic risk by increasing transparency and centralizing trading activity. However, they have also introduced challenges, including higher compliance costs and reduced flexibility for bespoke trading.\n\nChallenges and Trends\n\nAs it happens to other asset classes, the derivatives market faces ongoing challenges. Liquidity fragmentation between exchanges and OTC venues can complicate execution, particularly for large or complex trades. The reliance on traditional dealers in OTC markets also means that liquidity may become constrained during periods of market stress.\n\nEmerging trends, such as the tokenization of derivatives and the integration of blockchain technology, are expected to reshape market structures further. These innovations promise greater efficiency, transparency, and accessibility, potentially bridging the gap between centralized and decentralized trading models.","type":"content","url":"/markdown/intro-financial-markets#derivatives-markets-structure","position":51},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Crypto market structure","lvl2":"Overview of market structures per asset class"},"type":"lvl3","url":"/markdown/intro-financial-markets#crypto-market-structure","position":52},{"hierarchy":{"lvl1":"Financial Markets","lvl3":"Crypto market structure","lvl2":"Overview of market structures per asset class"},"content":"The cryptocurrency market structure represents a unique and rapidly evolving framework for trading digital assets such as Bitcoin, Ethereum, and numerous other cryptocurrencies. Unlike traditional financial markets, the crypto market operates on a decentralized foundation, with trading and transactions facilitated by blockchain technology. This distinct characteristic has led to the development of innovative trading venues and mechanisms, catering to a wide range of participants, from retail investors to institutional traders.\n\nCentralized Exchanges (CEXs)\n\nCentralized exchanges are the cornerstone of the crypto market, providing a familiar and accessible platform for buying, selling, and trading digital assets. These exchanges operate similarly to traditional stock exchanges but with a focus on cryptocurrencies. They serve as intermediaries, matching buyers and sellers while maintaining custody of users’ assets in their wallets.\n\nProminent examples of centralized exchanges include Binance, Coinbase, Kraken, and Bitfinex. These platforms offer a wide range of trading services, including spot trading, futures and options trading, and margin trading. They typically use order books to match buy and sell orders, providing transparent pricing and liquidity. However, centralized exchanges are also custodial, meaning they control users’ private keys, raising concerns about security and regulatory compliance.\n\nCEXs dominate the crypto trading landscape due to their ease of use, high liquidity, and comprehensive trading tools. Institutional investors often prefer these platforms because they offer robust infrastructure, regulatory oversight in some jurisdictions, and access to advanced trading features like API connectivity for algorithmic trading.\n\nDecentralized Exchanges (DEXs)\n\nDecentralized exchanges represent an innovative alternative to centralized platforms, leveraging blockchain technology to enable peer-to-peer trading without intermediaries. Unlike CEXs, DEXs operate non-custodially, meaning users retain full control of their private keys and assets throughout the trading process. Transactions on DEXs are executed via smart contracts, ensuring transparency and automation.\n\nExamples of popular DEXs include Uniswap, SushiSwap, and PancakeSwap, which primarily operate on Ethereum and Binance Smart Chain (BSC). These exchanges use automated market-making (AMM) models, where liquidity is provided by users who deposit assets into liquidity pools. Prices are determined algorithmically based on the ratio of assets in the pool. While DEXs offer unparalleled decentralization and security, they often suffer from lower liquidity, slower execution times, and higher transaction fees during network congestion.\n\nHybrid Models\n\nSome platforms, like dYdX and Binance DEX, combine features of both centralized and decentralized exchanges. These hybrid models aim to balance the transparency and user control of DEXs with the speed and efficiency of CEXs. For instance, dYdX offers non-custodial trading for derivatives while maintaining centralized order matching for performance.\n\nOver-the-Counter (OTC) Desks and Institutional Services\n\nFor large-scale transactions, over-the-counter (OTC) desks have become a critical component of the crypto market structure. These venues facilitate private trades between buyers and sellers, bypassing public order books to avoid slippage and market impact. OTC services are particularly popular among institutional investors, hedge funds, and high-net-worth individuals.\n\nMajor players in the OTC space include Genesis Trading, Cumberland DRW, and Kraken OTC. These desks provide tailored services, including deep liquidity, anonymity, and support for large transactions in less liquid altcoins.\n\nCrypto Derivatives\n\nThe derivatives market for cryptocurrencies has grown rapidly, providing traders with tools to hedge risk, speculate on price movements, and manage exposure. Platforms like Binance Futures, CME Group, FTX (before its collapse), and Deribit offer a wide range of products, including futures, options, and perpetual contracts. These platforms cater to sophisticated traders and institutions by offering leverage, advanced analytics, and liquidity for a variety of instruments.\n\nThe integration of traditional financial institutions, such as CME Group offering Bitcoin and Ethereum futures, highlights the increasing institutionalization of the crypto derivatives market.\n\nRegulation and Challenges\n\nThe regulatory environment for cryptocurrencies remains fragmented and uncertain. While some jurisdictions, like the United States and Europe, have introduced frameworks to govern crypto trading, many regions still lack comprehensive regulation. Key challenges include:\n\nCompliance: Centralized exchanges face increasing scrutiny regarding anti-money laundering (AML) and know-your-customer (KYC) requirements. Compliance standards that are taken for granted in traditional financial markets are not ensured in crypto markets, giving raise to recent scandals like the unauthorized use of customer funds by the now bankrupted FTX crypto exchange to provide liquidity to Alameda, a crypto investment firm that was also owned by the CEO of FTX.\n\nSecurity: Both centralized and decentralized platforms have been targets of hacks and exploits, raising concerns about asset protection.\n\nMarket Manipulation: The lack of consistent oversight has led to concerns about wash trading, pump-and-dump schemes, and other forms of market manipulation.\n\nEmerging Trends\n\nThe crypto market structure continues to evolve, driven by technological advancements and shifting participant needs. Key trends include:\n\nDecentralized Finance (DeFi): Beyond trading, DeFi protocols enable lending, borrowing, and yield farming, expanding the role of decentralized exchanges.\n\nInstitutional Adoption: The entry of major financial institutions, such as BlackRock and Fidelity, into the crypto space is increasing the demand for regulated trading venues and custody solutions.\n\nTokenized Assets: The tokenization of traditional assets, such as equities and real estate, is creating new opportunities for trading on blockchain-based platforms.\n\nLayer 2 Solutions: To address scalability issues, Layer 2 technologies like Optimism and Arbitrum are reducing transaction costs and improving execution times for DEXs.","type":"content","url":"/markdown/intro-financial-markets#crypto-market-structure","position":53},{"hierarchy":{"lvl1":"Financial Markets","lvl2":"Regulation of financial markets"},"type":"lvl2","url":"/markdown/intro-financial-markets#regulation-of-financial-markets","position":54},{"hierarchy":{"lvl1":"Financial Markets","lvl2":"Regulation of financial markets"},"content":"The regulation of financial markets is a cornerstone of global economic stability, designed to ensure transparency, fairness, and the protection of market participants. Financial market regulation governs the activities of trading venues, intermediaries, and participants, shaping how capital flows and risks are managed. It evolves constantly in response to technological advancements, market innovation, and crises that reveal vulnerabilities in the system.\n\nObjectives of Financial Market Regulation\n\nThe primary objectives of financial market regulation are:\n\nEnsuring Market Integrity: Regulations are designed to prevent fraud, manipulation, and insider trading. By safeguarding fair practices, regulators maintain investor confidence.\n\nProtecting Investors: Retail and institutional investors are protected through rules governing disclosure, risk assessment, and the fiduciary responsibilities of financial intermediaries.\n\nPromoting Transparency: Pre-trade and post-trade transparency requirements ensure that all participants have access to accurate and timely information about market activity.\n\nMitigating Systemic Risk: Regulations aim to prevent systemic crises by requiring risk management practices, such as capital adequacy, margin requirements, and stress testing.\nFostering Competition: Regulatory frameworks often promote competition by allowing new entrants and innovations, such as alternative trading systems, while curbing anti-competitive behavior.\n\nKey Regulatory Frameworks\n\nFinancial markets operate under a wide variety of regulatory regimes, with significant differences across jurisdictions. The following are key frameworks that define modern market regulation:\n\nIn the United States, two major regulatory frameworks define the structure of financial markets:\n\nThe Regulation National Market System (Reg NMS) focuses on equities, aiming to ensure best execution and fair access to trading venues. By encouraging competition between exchanges and alternative trading systems (ATSs), Reg NMS has fostered innovation but also contributed to market fragmentation as liquidity is spread across numerous venues.\n\nThe Dodd-Frank Wall Street Reform and Consumer Protection Act, enacted after the 2008 financial crisis, seeks to reduce systemic risk and enhance transparency in derivatives markets. Among its key contributions is the introduction of Swap Execution Facilities (SEFs), which standardize derivatives trading through centralized clearing and reporting. Additionally, Dodd-Frank strengthens consumer protections by imposing stricter oversight on complex financial products.\n\nIn Europe, the regulatory landscape is shaped by the Markets in Financial Instruments Directive II (MiFID II) and the European Market Infrastructure Regulation (EMIR):\n\nMiFID II provides a comprehensive framework governing equities, fixed income, derivatives, and other instruments. It imposes stringent pre-trade and post-trade transparency requirements across asset classes and defines distinct market structures, including Multilateral Trading Facilities (MTFs), Organized Trading Facilities (OTFs), and Systematic Internalizers (SIs). To preserve price discovery, MiFID II also limits dark pool trading with volume caps, while allowing exemptions for large-in-scale orders.\n\nMeanwhile, EMIR addresses derivatives markets by mandating central clearing for standardized contracts and enhancing reporting requirements. These measures aim to reduce counterparty risk and improve oversight of derivative transactions.\n\nAt a global level, several initiatives play a critical role in harmonizing financial market regulations:\n\nThe Basel III Framework focuses on international banking standards, enhancing capital adequacy, leverage ratios, and liquidity coverage to bolster financial stability.\n\nThe International Organization of Securities Commissions (IOSCO) establishes global benchmarks for securities market regulation, fostering international cooperation to tackle cross-border challenges effectively.\n\nChallenges in Financial Market Regulation\n\nDespite its critical importance, regulating financial markets is fraught with challenges. The rapid pace of technological innovation often outpaces regulatory frameworks, as seen with the rise of high-frequency trading (HFT) and cryptocurrencies. Ensuring that rules remain effective without stifling innovation requires a delicate balance.\n\nMarket globalization introduces additional complexity. Cross-border trading, global derivatives markets, and international capital flows necessitate coordination between regulators. However, jurisdictional differences often lead to regulatory arbitrage, where firms exploit inconsistencies to minimize compliance costs.\n\nRecent Developments and Trends\n\nRegulating Emerging Technologies: Electronic and Algorithmic Trading were among the first major technological advances disrupting the workings of financial markets. Their regulation has been addressed by sections of the USA and European frameworks mentioned above. In particular, the most comprehensive regulation of Algorithmic Trading has been tackled by a specific section of MiFID 2, RTS-6, as we will discuss in the specific chapter introducing this technology. More recently, the rise of cryptocurrencies and decentralized finance (DeFi) has prompted new regulatory efforts. Jurisdictions like the European Union are leading with frameworks such as the Markets in Crypto-Assets Regulation (MiCA), while others, like the United States, are still defining comprehensive policies.\nArtificial Intelligence, particularly the use of Large Language Models (LLMs) and agent-based frameworks for trading, is expected to be the next major chapter of regulatory oversight.\n\nEnvironmental, Social, and Governance (ESG) Regulation: As sustainable investing gains traction, regulators are focusing on mandating ESG disclosures and preventing green-washing. Europe’s Sustainable Finance Disclosure Regulation (SFDR) is a leading example.\n\nData and Cybersecurity: The reliance on digital platforms and cloud-based systems has increased regulatory focus on data protection, cybersecurity, and operational resilience. Here, specific regulatory frameworks like the European Union’s General Data Protection Regulation (GDPR) also have an impact in the financial markets.\n\nRetail Investor Protection: The surge in retail trading, driven by platforms like Robinhood, is prompting new rules on transparency, execution quality, and the specific regulation of practices like payment for order flow (PFOF).","type":"content","url":"/markdown/intro-financial-markets#regulation-of-financial-markets","position":55},{"hierarchy":{"lvl1":"Liquidity modelling"},"type":"lvl1","url":"/markdown/liquidity-modelling","position":0},{"hierarchy":{"lvl1":"Liquidity modelling"},"content":"","type":"content","url":"/markdown/liquidity-modelling","position":1},{"hierarchy":{"lvl1":"Liquidity modelling","lvl2":"Introduction"},"type":"lvl2","url":"/markdown/liquidity-modelling#introduction","position":2},{"hierarchy":{"lvl1":"Liquidity modelling","lvl2":"Introduction"},"content":"","type":"content","url":"/markdown/liquidity-modelling#introduction","position":3},{"hierarchy":{"lvl1":"Liquidity modelling","lvl2":"Exercises"},"type":"lvl2","url":"/markdown/liquidity-modelling#exercises","position":4},{"hierarchy":{"lvl1":"Liquidity modelling","lvl2":"Exercises"},"content":"","type":"content","url":"/markdown/liquidity-modelling#exercises","position":5},{"hierarchy":{"lvl1":"Modelling the Limit Order Book"},"type":"lvl1","url":"/markdown/lob-models","position":0},{"hierarchy":{"lvl1":"Modelling the Limit Order Book"},"content":"","type":"content","url":"/markdown/lob-models","position":1},{"hierarchy":{"lvl1":"Modelling the Limit Order Book","lvl2":"Introduction"},"type":"lvl2","url":"/markdown/lob-models#introduction","position":2},{"hierarchy":{"lvl1":"Modelling the Limit Order Book","lvl2":"Introduction"},"content":"","type":"content","url":"/markdown/lob-models#introduction","position":3},{"hierarchy":{"lvl1":"Modelling the Limit Order Book","lvl2":"Models for order arrival"},"type":"lvl2","url":"/markdown/lob-models#models-for-order-arrival","position":4},{"hierarchy":{"lvl1":"Modelling the Limit Order Book","lvl2":"Models for order arrival"},"content":"","type":"content","url":"/markdown/lob-models#models-for-order-arrival","position":5},{"hierarchy":{"lvl1":"Modelling the Limit Order Book","lvl3":"Visible and hidden orders","lvl2":"Models for order arrival"},"type":"lvl3","url":"/markdown/lob-models#visible-and-hidden-orders","position":6},{"hierarchy":{"lvl1":"Modelling the Limit Order Book","lvl3":"Visible and hidden orders","lvl2":"Models for order arrival"},"content":"","type":"content","url":"/markdown/lob-models#visible-and-hidden-orders","position":7},{"hierarchy":{"lvl1":"Modelling the Limit Order Book","lvl2":"Market impact models"},"type":"lvl2","url":"/markdown/lob-models#market-impact-models","position":8},{"hierarchy":{"lvl1":"Modelling the Limit Order Book","lvl2":"Market impact models"},"content":"","type":"content","url":"/markdown/lob-models#market-impact-models","position":9},{"hierarchy":{"lvl1":"Modelling the Limit Order Book","lvl2":"Volume prediction models"},"type":"lvl2","url":"/markdown/lob-models#volume-prediction-models","position":10},{"hierarchy":{"lvl1":"Modelling the Limit Order Book","lvl2":"Volume prediction models"},"content":"","type":"content","url":"/markdown/lob-models#volume-prediction-models","position":11},{"hierarchy":{"lvl1":"Modelling the Limit Order Book","lvl2":"Probability of filling a limit order"},"type":"lvl2","url":"/markdown/lob-models#probability-of-filling-a-limit-order","position":12},{"hierarchy":{"lvl1":"Modelling the Limit Order Book","lvl2":"Probability of filling a limit order"},"content":"","type":"content","url":"/markdown/lob-models#probability-of-filling-a-limit-order","position":13},{"hierarchy":{"lvl1":"Modelling the Limit Order Book","lvl2":"Short term price prediction"},"type":"lvl2","url":"/markdown/lob-models#short-term-price-prediction","position":14},{"hierarchy":{"lvl1":"Modelling the Limit Order Book","lvl2":"Short term price prediction"},"content":"","type":"content","url":"/markdown/lob-models#short-term-price-prediction","position":15},{"hierarchy":{"lvl1":"Modelling the Limit Order Book","lvl2":"Information Asymmetry"},"type":"lvl2","url":"/markdown/lob-models#information-asymmetry","position":16},{"hierarchy":{"lvl1":"Modelling the Limit Order Book","lvl2":"Information Asymmetry"},"content":"","type":"content","url":"/markdown/lob-models#information-asymmetry","position":17},{"hierarchy":{"lvl1":"Modelling the Limit Order Book","lvl2":"Simulation of LOBs"},"type":"lvl2","url":"/markdown/lob-models#simulation-of-lobs","position":18},{"hierarchy":{"lvl1":"Modelling the Limit Order Book","lvl2":"Simulation of LOBs"},"content":"","type":"content","url":"/markdown/lob-models#simulation-of-lobs","position":19},{"hierarchy":{"lvl1":"Modelling the Limit Order Book","lvl3":"Probabilistic generative models","lvl2":"Simulation of LOBs"},"type":"lvl3","url":"/markdown/lob-models#probabilistic-generative-models","position":20},{"hierarchy":{"lvl1":"Modelling the Limit Order Book","lvl3":"Probabilistic generative models","lvl2":"Simulation of LOBs"},"content":"","type":"content","url":"/markdown/lob-models#probabilistic-generative-models","position":21},{"hierarchy":{"lvl1":"Modelling the Limit Order Book","lvl3":"Agent based models","lvl2":"Simulation of LOBs"},"type":"lvl3","url":"/markdown/lob-models#agent-based-models","position":22},{"hierarchy":{"lvl1":"Modelling the Limit Order Book","lvl3":"Agent based models","lvl2":"Simulation of LOBs"},"content":"","type":"content","url":"/markdown/lob-models#agent-based-models","position":23},{"hierarchy":{"lvl1":"Market Making fundamentals"},"type":"lvl1","url":"/markdown/market-making-fundamentals","position":0},{"hierarchy":{"lvl1":"Market Making fundamentals"},"content":"","type":"content","url":"/markdown/market-making-fundamentals","position":1},{"hierarchy":{"lvl1":"Market Making fundamentals","lvl2":"Introduction"},"type":"lvl2","url":"/markdown/market-making-fundamentals#introduction","position":2},{"hierarchy":{"lvl1":"Market Making fundamentals","lvl2":"Introduction"},"content":"","type":"content","url":"/markdown/market-making-fundamentals#introduction","position":3},{"hierarchy":{"lvl1":"Market Making fundamentals","lvl2":"The market-making business objective"},"type":"lvl2","url":"/markdown/market-making-fundamentals#the-market-making-business-objective","position":4},{"hierarchy":{"lvl1":"Market Making fundamentals","lvl2":"The market-making business objective"},"content":"","type":"content","url":"/markdown/market-making-fundamentals#the-market-making-business-objective","position":5},{"hierarchy":{"lvl1":"Market Making fundamentals","lvl2":"Market-Making risks and trade-offs"},"type":"lvl2","url":"/markdown/market-making-fundamentals#market-making-risks-and-trade-offs","position":6},{"hierarchy":{"lvl1":"Market Making fundamentals","lvl2":"Market-Making risks and trade-offs"},"content":"","type":"content","url":"/markdown/market-making-fundamentals#market-making-risks-and-trade-offs","position":7},{"hierarchy":{"lvl1":"Market Making fundamentals","lvl2":"Deconstructing a market-making strategy"},"type":"lvl2","url":"/markdown/market-making-fundamentals#deconstructing-a-market-making-strategy","position":8},{"hierarchy":{"lvl1":"Market Making fundamentals","lvl2":"Deconstructing a market-making strategy"},"content":"","type":"content","url":"/markdown/market-making-fundamentals#deconstructing-a-market-making-strategy","position":9},{"hierarchy":{"lvl1":"Market Making fundamentals","lvl3":"Fair price estimation","lvl2":"Deconstructing a market-making strategy"},"type":"lvl3","url":"/markdown/market-making-fundamentals#fair-price-estimation","position":10},{"hierarchy":{"lvl1":"Market Making fundamentals","lvl3":"Fair price estimation","lvl2":"Deconstructing a market-making strategy"},"content":"","type":"content","url":"/markdown/market-making-fundamentals#fair-price-estimation","position":11},{"hierarchy":{"lvl1":"Market Making fundamentals","lvl3":"Spread determination","lvl2":"Deconstructing a market-making strategy"},"type":"lvl3","url":"/markdown/market-making-fundamentals#spread-determination","position":12},{"hierarchy":{"lvl1":"Market Making fundamentals","lvl3":"Spread determination","lvl2":"Deconstructing a market-making strategy"},"content":"","type":"content","url":"/markdown/market-making-fundamentals#spread-determination","position":13},{"hierarchy":{"lvl1":"Market Making fundamentals","lvl3":"Hedging the inventory","lvl2":"Deconstructing a market-making strategy"},"type":"lvl3","url":"/markdown/market-making-fundamentals#hedging-the-inventory","position":14},{"hierarchy":{"lvl1":"Market Making fundamentals","lvl3":"Hedging the inventory","lvl2":"Deconstructing a market-making strategy"},"content":"","type":"content","url":"/markdown/market-making-fundamentals#hedging-the-inventory","position":15},{"hierarchy":{"lvl1":"Market Making fundamentals","lvl2":"Market - making and market microstructure"},"type":"lvl2","url":"/markdown/market-making-fundamentals#market-making-and-market-microstructure","position":16},{"hierarchy":{"lvl1":"Market Making fundamentals","lvl2":"Market - making and market microstructure"},"content":"","type":"content","url":"/markdown/market-making-fundamentals#market-making-and-market-microstructure","position":17},{"hierarchy":{"lvl1":"Market Making fundamentals","lvl3":"Dealer to Client Platforms","lvl2":"Market - making and market microstructure"},"type":"lvl3","url":"/markdown/market-making-fundamentals#dealer-to-client-platforms","position":18},{"hierarchy":{"lvl1":"Market Making fundamentals","lvl3":"Dealer to Client Platforms","lvl2":"Market - making and market microstructure"},"content":"","type":"content","url":"/markdown/market-making-fundamentals#dealer-to-client-platforms","position":19},{"hierarchy":{"lvl1":"Market Making fundamentals","lvl3":"Limit Order Books","lvl2":"Market - making and market microstructure"},"type":"lvl3","url":"/markdown/market-making-fundamentals#limit-order-books","position":20},{"hierarchy":{"lvl1":"Market Making fundamentals","lvl3":"Limit Order Books","lvl2":"Market - making and market microstructure"},"content":"","type":"content","url":"/markdown/market-making-fundamentals#limit-order-books","position":21},{"hierarchy":{"lvl1":"Market Making fundamentals","lvl2":"Exercises"},"type":"lvl2","url":"/markdown/market-making-fundamentals#exercises","position":22},{"hierarchy":{"lvl1":"Market Making fundamentals","lvl2":"Exercises"},"content":"","type":"content","url":"/markdown/market-making-fundamentals#exercises","position":23},{"hierarchy":{"lvl1":"Reinforcement Learning and Market Making"},"type":"lvl1","url":"/markdown/market-making-rl","position":0},{"hierarchy":{"lvl1":"Reinforcement Learning and Market Making"},"content":"","type":"content","url":"/markdown/market-making-rl","position":1},{"hierarchy":{"lvl1":"Reinforcement Learning and Market Making","lvl2":"Introduction"},"type":"lvl2","url":"/markdown/market-making-rl#introduction","position":2},{"hierarchy":{"lvl1":"Reinforcement Learning and Market Making","lvl2":"Introduction"},"content":"","type":"content","url":"/markdown/market-making-rl#introduction","position":3},{"hierarchy":{"lvl1":"Reinforcement Learning and Market Making","lvl2":"Exercises"},"type":"lvl2","url":"/markdown/market-making-rl#exercises","position":4},{"hierarchy":{"lvl1":"Reinforcement Learning and Market Making","lvl2":"Exercises"},"content":"","type":"content","url":"/markdown/market-making-rl#exercises","position":5},{"hierarchy":{"lvl1":"Market microstructure"},"type":"lvl1","url":"/markdown/market-microstructure","position":0},{"hierarchy":{"lvl1":"Market microstructure"},"content":"Market microstructure is the study of the processes and mechanisms through which financial instruments are traded in financial markets. It focuses on how different participants —such as investors, intermediaries, and liquidity providers—interact, and how their actions affect the price formation, liquidity, and efficiency of markets. Whereas many financial models often assume that prices reflect all available information, market microstructure digs deeper into the realities of trading: the role of transaction costs, bid-ask spreads, order types, and how information asymmetry influences trading strategies and outcomes.\n\nAt the core of market microstructure is the understanding that prices are not simply the result of supply and demand but are shaped by specific choices in the architecture of the market. These include:\n\nTrading mechanisms: how in practice investors interact with each other to trade (buy or sell) instruments\n\nTrading times and frequency: when and how often can a financial instrument be traded\n\nOrder types: how investors communicate their intention of trading\n\nTrading protocols: what are the technical rules of the market\n\nTransparency: which information can be given to investors to\nensure they can trade fairly\n\nBy studying these elements, market microstructure helps explain phenomena like price discovery, short-term price fluctuations, and the impact of large trades.","type":"content","url":"/markdown/market-microstructure","position":1},{"hierarchy":{"lvl1":"Market microstructure","lvl2":"Types of trading mechanisms"},"type":"lvl2","url":"/markdown/market-microstructure#types-of-trading-mechanisms","position":2},{"hierarchy":{"lvl1":"Market microstructure","lvl2":"Types of trading mechanisms"},"content":"The way financial markets are structured has a profound effect on how trades are executed, how prices are formed, and how liquidity is provided. At the heart of this structure lies the mechanism that governs how buyers and sellers interact. Broadly speaking, we can categorize trading mechanisms into three types: quote-driven, order-driven, and hybrid. Each has its unique characteristics and implications for market participants.\n\nQuote-Driven Markets\n\nIn a quote-driven market, transactions revolve around a specialized intermediary known as a dealer or market maker. The dealer plays a central role by continuously quoting prices at which they are willing to buy and sell a given financial instrument. These quotes typically consist of two parts: the bid price, which is the price the dealer is willing to pay to buy the instrument, and the ask price, which is the price at which they are willing to sell it. The dealer profits from the spread, the difference between the bid and ask prices, and adjusts these quotes dynamically based on supply, demand, and market conditions.\n\nIn this system, investors cannot trade directly with each other. Instead, they must transact exclusively through the dealer, who provides liquidity by maintaining an inventory of the instruments and adjusting quotes to reflect market activity. This structure is particularly common in over-the-counter (OTC) markets, such as foreign exchange or bond markets, where liquidity might not be as readily available as in centralized exchanges.\n\nAn important feature of quote-driven markets is that prices are often indicative—they provide an approximation of the market but may not be immediately executable, especially for larger quantities or illiquid instruments. In many cases, a negotiation process is necessary for larger trades. Investors and dealers engage in a dialogue to finalize the trade, allowing the dealer to reassess risk and adjust prices accordingly.\n\nOrder-Driven Markets\n\nOrder-driven markets operate very differently from their quote-driven counterparts. Instead of relying on dealers to provide prices and liquidity, these markets use a central order book where buyers and sellers submit their orders. Investors place orders that specify the price at which they are willing to buy or sell a security, and the market matches these orders according to a predefined set of rules.\n\nIn an order-driven system, participants interact directly with each other through their orders. There is no negotiation process; instead, a matching engine automatically pairs buy and sell orders based on their price and the time they were entered into the system. This ensures a fair and transparent market, where prices are determined by the collective actions of all market participants.\n\nThe most common types of orders are limit orders and market orders. A limit order specifies a price at which the trader is willing to buy or sell, while a market order is an instruction to execute the trade immediately at the best available price. The order book is public, meaning that participants can see all current bids and asks, providing a high level of transparency in terms of available liquidity and market sentiment.\n\nOrder-driven markets are typical of centralized exchanges, such as stock exchanges like the New York Stock Exchange (NYSE) or futures markets. These markets tend to have high volumes of orders, allowing for continuous matching and a high degree of liquidity, particularly in actively traded instruments.\n\nHybrid Markets\n\nMany modern markets blend the features of both quote-driven and order-driven systems to offer the best of both worlds. These hybrid markets are designed to provide liquidity through dealers while maintaining the transparency and fairness of an order-driven system.\n\nOne version of a hybrid market includes quote-driven mechanisms where dealers post executable prices. Unlike indicative quotes, these prices are firm and represent actual tradeable prices for specified quantities. This arrangement provides certainty for investors, especially when trading in large quantities or illiquid instruments, where the guarantee of liquidity from a dealer can be critical.\n\nIn another version of a hybrid market, dealers are obligated to participate in an order-driven system. They act as liquidity providers, continually placing passive two-way orders (both buy and sell orders) on the public order book. These dealers, often referred to as market makers, must post prices even when the market is quiet, ensuring that there is always liquidity available. This contractual obligation to provide liquidity helps maintain market stability and reduces the risk of sudden price jumps due to a lack of counter-party interest.\n\nHybrid markets strike a balance between dealer-driven liquidity provision and the transparency and efficiency of an order book system. They are commonly used in highly liquid markets, such as foreign exchange platforms and some modern equity exchanges, where large institutional players need the certainty of liquidity alongside the openness of an order-driven market.\n\nWhy two different market microstructures?\n\nEach of these trading mechanisms serves different types of markets and participants. Quote-driven markets are particularly useful for illiquid or over-the-counter products, where large trades and custom negotiations are common. Order-driven markets thrive in environments with high trading volumes and transparency, offering fast execution and price discovery through collective action. Hybrid markets bring together the strengths of both systems, ensuring liquidity and fairness even in challenging trading conditions. Understanding these mechanisms is crucial for anyone looking to navigate today’s financial markets.","type":"content","url":"/markdown/market-microstructure#types-of-trading-mechanisms","position":3},{"hierarchy":{"lvl1":"Market microstructure","lvl2":"Trading Times and Frequencies"},"type":"lvl2","url":"/markdown/market-microstructure#trading-times-and-frequencies","position":4},{"hierarchy":{"lvl1":"Market microstructure","lvl2":"Trading Times and Frequencies"},"content":"The timing of trades and the frequency with which they occur are essential aspects of market design, significantly impacting how liquidity, price discovery, and volatility are managed. Different trading mechanisms operate under varying time structures, and these structures influence the behavior of market participants, the efficiency of order execution, and the stability of markets. Broadly, trading times and frequencies can be categorized into continuous trading, periodic auctions, and request-driven trading. Each of these methods plays a role in shaping market dynamics, especially under different market conditions such as periods of high volatility or when liquidity is scarce.\n\nContinuous Trading\n\nContinuous trading is the most common method used in modern financial markets, where trading happens continuously within a defined window of time. During this window, market participants can submit and execute orders at any moment, allowing for a constant flow of trades and real-time price updates. This method is particularly effective in markets with high trading volumes, where liquidity is readily available throughout the trading day.\n\nOrder-Driven Continuous Trading: In order-driven markets, continuous trading typically takes place using a continuous double auction mechanism. In this system, all buy and sell orders are collected in a limit order book, where they are matched based on price and time priority. The order book allows for ongoing price discovery as orders are continuously executed whenever a matching counterparty is found. Traders have full visibility of the order book, seeing both the available bids and asks at various price levels, which ensures transparency and competitive pricing.\n\nQuote-Driven Continuous Trading: In quote-driven markets, dealers play a pivotal role in continuous trading by constantly updating their bid and offer prices for the instruments they manage. In purely quote-driven markets, these prices are often indicative, meaning they serve as guidelines and may require negotiation before a trade can be executed. However, in hybrid markets, dealers may quote firm prices, which are executable immediately for specific quantities. This ensures that liquidity is available continuously, even when direct order matching between investors is not feasible.\n\nContinuous trading is particularly advantageous for markets with high liquidity, as it enables fast execution and efficient price formation. However, in periods of high volatility, continuous trading can exacerbate price fluctuations, as large orders or sudden imbalances between supply and demand can cause significant price swings.\n\nContinuous Trading with Scheduled Call Auctions\n\nTo address the challenges of volatility in continuous markets, some systems introduce scheduled call auctions at key points during the trading day. A call auction is a method where all orders are temporarily pooled and then executed simultaneously at a single price, which balances supply and demand.\n\nIn a continuous trading with call auctions model, the market operates continuously but pauses at predetermined times, such as the market open or close, or during periods of heightened volatility, to conduct a call auction. These auctions help stabilize prices by concentrating liquidity into a single moment, allowing for a more orderly transition between trading sessions or a more controlled response to volatility spikes.\n\nAt the open, for example, call auctions help manage the influx of orders that accumulate overnight, ensuring that the market starts trading at a fair price that reflects the balance of supply and demand. Similarly, at the close, call auctions help establish a final price for the trading session, which is often used as a reference for pricing derivatives and other financial products.\n\nScheduled Call Auctions\n\nIn markets that rely on scheduled call auctions, trades only happen at specific times, rather than continuously. Investors submit their buy and sell orders in advance, and these orders are aggregated until a scheduled auction takes place. At the auction time, the market clears at a single price that maximizes the volume of trades, ensuring that the highest number of buy and sell orders are matched.\n\nScheduled call auctions are often used in less liquid markets, where continuous trading might lead to significant price instability due to the low number of orders. By pooling orders into a single event, scheduled auctions create concentrated liquidity and help prevent large price jumps between trades.\n\nThis method is commonly employed in initial public offerings (IPOs), where the initial price discovery process can be difficult due to uncertainty about demand. Scheduled call auctions provide a more controlled environment for determining the opening price of the security.\n\nRequest-Driven Trading\n\nIn some markets, trading does not happen on a continuous or scheduled basis but instead occurs at the discretion of the client. This model, known as request-driven trading, is often used in over-the-counter (OTC) markets for instruments that are not frequently traded or have specialized liquidity needs.\n\nIn request-driven trading, a client requests a quote or a trade from a dealer at a specific time, and the dealer responds with a price. The trade is executed based on this interaction, often involving some negotiation over the size or price of the transaction. This model is common in markets for customized or illiquid products, such as large blocks of bonds or structured financial instruments, where finding a counter-party may not be straightforward.\n\nRequest-driven trading provides flexibility for both clients and dealers, allowing them to negotiate trades that meet specific needs. However, it lacks the transparency and immediacy of continuous or auction-based systems, as each trade is typically a private transaction between the client and the dealer.\n\nTrade-offs\n\nThe choice of trading frequency and timing mechanism reflects the unique characteristics of each market and the needs of its participants. Continuous trading offers fast execution and real-time price discovery, while scheduled auctions and request-driven trading provide alternatives that manage liquidity and volatility more effectively under specific conditions.","type":"content","url":"/markdown/market-microstructure#trading-times-and-frequencies","position":5},{"hierarchy":{"lvl1":"Market microstructure","lvl2":"Order-Driven Markets and the Central Limit Order Book (CLOB)"},"type":"lvl2","url":"/markdown/market-microstructure#order-driven-markets-and-the-central-limit-order-book-clob","position":6},{"hierarchy":{"lvl1":"Market microstructure","lvl2":"Order-Driven Markets and the Central Limit Order Book (CLOB)"},"content":"Order-driven markets are a fundamental type of financial market structure where prices and liquidity are determined directly by the orders placed by market participants, without the need for intermediaries such as dealers. In these markets, investors interact through a central limit order book (CLOB), a system that aggregates and displays buy and sell orders in real time. The CLOB organizes these orders based on price, ensuring that the best available prices are always visible to participants, making the system transparent and efficient.\n\nCLOBs don’t allow orders at arbitrary prices, instead enforcing minimum price increments known as tick sizes, which define the smallest allowable difference between order prices. The tick size choice traditionally falls to the market owner, who aims to balance liquidity by setting smaller tick sizes, encouraging tighter spreads and more precise pricing. However, overly small tick sizes can also penalize liquidity providers who maintain constant bid and ask orders in the CLOB, as they may face more frequent price competition and increased order cancellations.\n\nIn recent years, regulatory bodies in some markets have implemented restrictions on tick sizes to stabilize order book structures and reduce excessive market noise. For instance, in the European Union, MiFID II introduced a tick size regime based on price levels and average daily number of trades for specific instruments, limiting the ability of markets to set overly fine increments. Similarly, in the U.S., tick size pilot programs have explored tick adjustments for smaller-cap stocks to assess impacts on liquidity and execution quality. Such regulatory measures aim to ensure fairer conditions for both liquidity providers and market participants, while supporting orderly and efficient markets.\n\nAdditionally to tick sizes, markets typically impose restrictions on the the quantities that can be traded, known as lot sizes.","type":"content","url":"/markdown/market-microstructure#order-driven-markets-and-the-central-limit-order-book-clob","position":7},{"hierarchy":{"lvl1":"Market microstructure","lvl3":"The Structure of the Central Limit Order Book (CLOB)","lvl2":"Order-Driven Markets and the Central Limit Order Book (CLOB)"},"type":"lvl3","url":"/markdown/market-microstructure#the-structure-of-the-central-limit-order-book-clob","position":8},{"hierarchy":{"lvl1":"Market microstructure","lvl3":"The Structure of the Central Limit Order Book (CLOB)","lvl2":"Order-Driven Markets and the Central Limit Order Book (CLOB)"},"content":"At any given moment, the CLOB provides a snapshot of the market by showing all active buy and sell orders. These orders are aggregated anonymously and displayed in two columns: one for buy orders (bids) and another for sell orders (asks or offers). Each column is ordered by price, with the best bid (the highest price a buyer is willing to pay) and the best ask (the lowest price a seller is willing to accept) at the top.\n\nBest Bid/Ask: The best bid and ask, P^b_{best} and P^a_{best}, represent the highest and lowest prices, respectively, that are currently available in the market. The difference between them is the bid-ask spread, which can serve as a measure of liquidity. A narrow spread generally indicates higher liquidity.\n\nMarket Depth: The CLOB also shows the market depth, which refers to the number of orders available at various price levels. Market depth gives traders insight into the potential for price movements based on the volume of buy and sell interest at different prices.\n\nMid-Price: The mid-price is calculated as the average of the best bid and best ask prices. It serves as an estimate of the fair market price at a given moment:P_{mid} = \\frac{1}{2}(P^b_{best} + P^a_{best})\n\nSpread: The difference between the best bid and ask prices. It is a metric of liquidity in the order book, since the larger it is the less likely is that investors can trade at prices close to the mid-price:S = P^a_{best} + P^b_{best}\n\nVolume Imbalance: it refers to the difference in the volume of buy orders, typically at the best bid and ask prices, although a larger number of levels might be taken for a more robust estimation. A significant imbalance can indicate potential price movement; for example, if the volume at the best bid is substantially higher than the volume at the best ask, it may suggest upward pressure on the price. This feature helps traders gauge short-term supply and demand dynamics and assess potential price changes. It is typically reported as a percentage over total volume in the best bid and ask:I = \\frac{V^b_{best} - V^a_{best}}{V^b_{best} + V^a_{best}}\n\nThe following figure shows a visualization of the order book called market depth, since it shows the current orders sitting in the book ordered by price. Each level might have orders from different participants, but we show them aggregated which is how real markets report them in their feeds.\n\n\n\nFigure 1:Market Depth visualization of a Central Limit Order Book, with orders sorted by price. Volume is aggregated for the different orders in the same price level. In this example, the tick size is 1, the mid price is P_{mid} = 100, the spread is S = 4 and the imbalance I = 0.53","type":"content","url":"/markdown/market-microstructure#the-structure-of-the-central-limit-order-book-clob","position":9},{"hierarchy":{"lvl1":"Market microstructure","lvl3":"Types of Orders","lvl2":"Order-Driven Markets and the Central Limit Order Book (CLOB)"},"type":"lvl3","url":"/markdown/market-microstructure#types-of-orders","position":10},{"hierarchy":{"lvl1":"Market microstructure","lvl3":"Types of Orders","lvl2":"Order-Driven Markets and the Central Limit Order Book (CLOB)"},"content":"Market participants in order-driven markets can use a variety of order types, each serving different trading objectives:\n\nMarket Orders: These orders instruct the system to buy or sell immediately at the best available price, consuming always the liquidity available at best prices (price priority). Market orders are executed quickly, but they carry the risk of paying a higher price or receiving a lower price than expected if there is insufficient liquidity at the best bid or ask. In this case, the order may “walk the book,” executing across multiple price levels and resulting in a higher average price (for buys) or lower average price (for sells). The execution price of an order that consumes liquidity across various levels is computed using the volume weighted average price (VWAP):P_{exec} = \\frac{\\sum_i v_i P_i}{\\sum_i v_i}\n\nExample: Taking the previous CLOB example, suppose an investor wants to buy Suppose an investor wants to buy 500 shares with a market order. As shown in the figure below, the order consumes liquidity up to three levels. The resulting execution price is 103.51. After the market order, the mid-price is recalculated given the new liquidity, to P_{mid} = 101, and the spread broadens to S = 6\n\n\n\nFigure 2:Effect of a buy market order of size 500 in the Central Limit Order Book.\n\nLimit Orders: These specify a price at which the trader is willing to buy or sell. If the limit price is not immediately available to trade, the order remains in the order book until a matching counter-party arrives or the order is canceled. Limit orders allow traders to control the price at which they execute but come with the risk that the order may not be executed if the market price moves away from the limit.\n\nWhen an order is added to a level with existing liquidity, there is the question of which order has the priority when a matching order arrives at the CLOB. Most markets use a time priority or first in first out (FIFO) rule, in which orders that arrived earlier are consumed first. It is not the only rule, though. For instance some markets use a pro-rata rule, in which all existing orders are consumed proportionally to their size.\n\nExample 1: Using again the previous order book, suppose and investor wants to buy the 500 shares but instead of sending a market order, it uses a limit order at price 98. Since there are order existing at that price level, the order sits at the bottom of the level. If this is a market with a time priority rule, this means this order will be the last to be consumed at the level. After placing the order, the mid-price and the spread does not change, but the imbalance is affected, see figure below.\n\n\n\nFigure 3:Effect of a buy limit order of size 500 and price 98 in the Central Limit Order Book.\n\nExample 2: If the investor instead places the order at price 102, there is partial matching liquidity at this level. A partial execution of the limit order happens at price 102, with the remaining order sitting now at this price level. The new mid-price is P_mid = 102.5 and the spread S = 1.\n\n\n\nFigure 4:Effect of a buy limit order of size 500 and price 102 in the Central Limit Order Book. A hatch pattern is used to show the consumed liquidity by the limit order.\n\nHidden Orders: Hidden orders are limit orders that are not visible to other market participants. They allow traders to conceal their trading intentions, which can be advantageous in avoiding market impact or revealing their strategy to others. However, hidden orders generally have lower priority compared to visible orders at the same price level, meaning they will only be executed after all visible orders at that price have been matched.\n\nPrevalence: Estimates suggest that hidden orders can account for approximately 10% to 20% of total liquidity on major exchanges like the NASDAQ or NYSE. The prevalence of hidden orders tends to increase during periods of high volatility or when large institutional players are active.\n\nOther Order Types:\n\nMarket-to-Limit: Begins as a market order but converts into a limit order if only part of the order is filled.\n\nImmediate-Or-Cancel: Any unexecuted portion of the order is canceled immediately after partial execution.\n\nFill-Or-Kill: The order is either executed in full immediately or not at all.\n\nAll-Or-None: Similar to Fill-Or-Kill, but the order may remain active if it cannot be immediately executed.\n\nStop Orders: Triggered when a specified price is reached, turning into a market or limit order based on conditions. For example, a price based condition that will sent a market order if the mid-price of a financial instrument goes below a certain price level.","type":"content","url":"/markdown/market-microstructure#types-of-orders","position":11},{"hierarchy":{"lvl1":"Market microstructure","lvl3":"Trading Fees","lvl2":"Order-Driven Markets and the Central Limit Order Book (CLOB)"},"type":"lvl3","url":"/markdown/market-microstructure#trading-fees","position":12},{"hierarchy":{"lvl1":"Market microstructure","lvl3":"Trading Fees","lvl2":"Order-Driven Markets and the Central Limit Order Book (CLOB)"},"content":"Trading fees are an important consideration in order-driven markets, as they can impact the overall cost of executing trades. Exchanges generally apply fees based on the type of order and the participant’s role in providing or taking liquidity. The common fee structures include:\n\nMaker-Taker Model: This model differentiates between liquidity providers (makers) and liquidity takers. Makers, who place limit orders that add liquidity to the order book, are typically rewarded with a rebate, while takers, who execute against existing orders and remove liquidity, are charged a fee. This model incentivizes the addition of liquidity to the market.\n\nInverted Maker-Taker Model: In some cases, the exchange may use an inverted model, where takers receive a rebate, and makers are charged a fee. This is used to attract more aggressive trading and increase market activity.\n\nFlat Fee Model: Some exchanges charge a flat fee per transaction, regardless of whether the participant is adding or removing liquidity. This approach provides simplicity and is often used in less active markets.\n\nThe specific fee structure and the level of fees depend on various factors, including the type of asset being traded, the exchange, and the participant’s trading volume. In some markets, high-frequency traders may receive favorable fee arrangements due to the large volumes they generate, which contribute significantly to market liquidity.","type":"content","url":"/markdown/market-microstructure#trading-fees","position":13},{"hierarchy":{"lvl1":"Market microstructure","lvl3":"Types of Order Book Data","lvl2":"Order-Driven Markets and the Central Limit Order Book (CLOB)"},"type":"lvl3","url":"/markdown/market-microstructure#types-of-order-book-data","position":14},{"hierarchy":{"lvl1":"Market microstructure","lvl3":"Types of Order Book Data","lvl2":"Order-Driven Markets and the Central Limit Order Book (CLOB)"},"content":"Order-driven markets often provide different levels of data to market participants, depending on their needs:\n\nLevel I: This basic level shows only the best bid and ask prices, giving traders a minimal snapshot of the market.\n\nLevel II: More advanced, this level displays multiple bid and ask levels beyond the best prices, giving insight into the market depth.\n\nLevel III: The most detailed level of data, showing individual orders, their sizes, prices, and time stamps. This information is typically used by algorithmic traders and market makers to track the behavior of specific market participants.\n\nHistorical data from order books is called tick data, and captures every change that occurs in the order book. It is  provided by the exchanges or via financial data companies like LSEG (ex Refinitiv) and Bloomberg, which aggregate and normalize this data.  There are also free resources with samples of data like Lobster.\n\nTick data generally comes in two forms:\n\nOrder Book Snapshots: also known as market-depth data, this form of tick data provides detailed information about the order book at specific time intervals, showing the number of buy and sell orders at various price levels. Order book snapshots allow traders to assess liquidity and understand the potential for price movements based on supply and demand. For example, a trader using order book snapshots can identify a large concentration of sell orders at a specific price level, indicating potential resistance.\n\nMessage Data: this data records all order book events, such as order submissions, cancellations, and executions, with timestamps. Message data allows traders to reconstruct the order book and analyze the flow of activity in real time. For instance, a trader reviewing message data might notice a series of large buy orders submitted successively, indicating strong buying interest and potentially signaling an upward trend.","type":"content","url":"/markdown/market-microstructure#types-of-order-book-data","position":15},{"hierarchy":{"lvl1":"Market microstructure","lvl3":"Advantages and Challenges of Order-Driven Markets","lvl2":"Order-Driven Markets and the Central Limit Order Book (CLOB)"},"type":"lvl3","url":"/markdown/market-microstructure#advantages-and-challenges-of-order-driven-markets","position":16},{"hierarchy":{"lvl1":"Market microstructure","lvl3":"Advantages and Challenges of Order-Driven Markets","lvl2":"Order-Driven Markets and the Central Limit Order Book (CLOB)"},"content":"The CLOB system provides significant pre-trade transparency, as market participants can view the full order book and adjust their strategies based on available prices and liquidity. This transparency promotes price discovery, helping ensure that prices reflect the collective expectations of all market participants.\n\nHowever, the system is not without challenges. In less liquid markets, price volatility can be higher due to the lack of continuous buy and sell interest. Large market orders can “sweep” through multiple price levels, causing significant short-term price fluctuations. Moreover, the transparency of the system can also lead to front-running, where certain traders take advantage of visible large orders by placing their own orders ahead of them.\n\nTo mitigate these issues, many markets incorporate mechanisms like circuit breakers and trading halts during periods of extreme volatility, as well as opening and closing auctions to establish stable prices at the beginning and end of trading sessions.","type":"content","url":"/markdown/market-microstructure#advantages-and-challenges-of-order-driven-markets","position":17},{"hierarchy":{"lvl1":"Market microstructure","lvl2":"Quote-driven markets: the Request for Quote protocol"},"type":"lvl2","url":"/markdown/market-microstructure#quote-driven-markets-the-request-for-quote-protocol","position":18},{"hierarchy":{"lvl1":"Market microstructure","lvl2":"Quote-driven markets: the Request for Quote protocol"},"content":"Central Limit Order Books have become the cornerstone of market microstructure given their efficiency in matching continuously buying and selling interest, while at the same time offering valuable information for price and liquidity discovery to other investors and market participants. However, for their well functioning they rely on having enough liquidity in the form of visible bid and ask limit orders. A relatively empty order book discourages other investors to post their orders, becoming a self-fulfilling prophecy. It also becomes less useful as an engine for price and liquidity discovery. Market operators and even Governments try to incentive this market structure, for instance by offering attractive fees to liquidity providers like banks in exchange of keeping bid and ask limit orders most of the time in a trading session.\n\nHowever, for some financial instruments this is insufficient. Take corporate bonds. Companies usually only issue a single stock, but might have hundreds of different bonds in circulation with different maturities, coupon types, amortization types and contractual clauses. They do so to attract different type of lenders and adapt to specific economic conditions and / or forecasts. As a result, many of those bonds trade very infrequently, making central limit order book relatively useless for market participants to find counter-parties to trade with. The same happens, for example, with most but the less standard derivatives.\n\nThe market solution to trade those instruments has been traditionally to rely on intermediaries:\n\nBrokers, who have an extensive network of market participants that use to find a potential match to an order, charging a commission for the service\n\nDealers, typically banks, who are willing to buy at a discount or sell at a premium, keeping the instruments in their inventory until a counter-party is found. Therefore, in contrast with brokers, they take on inventory risk since they don’t typically can liquidate the position taken quickly, sometimes keeping inventory holdings for extensive periods of time.\n\nInvestors who want to trade illiquid financial instruments would contact brokers or dealers by “voice” (which nowadays means usually by email or chat, sometimes by phone), requesting them quotes and in case of matching interests, trading over the counter, i.e. bilaterally out of market structures.\n\nWhereas this is still the case for some financial instruments, with the advent of electronification in financial markets, broker-dealers started building electronic platforms to allow their clients to request them quotes. Such single-dealer-to-client (SD2C) platforms have the advantage of simplifying and automatic partly both price discovery, trading and settling later the trade, but being proprietary of the broker-dealer, clients were potentially missing the benefit of competition, unless they kept multiple accounts with different broker-dealers.\n\nThis opened up a business opportunity: electronic multi-dealer-to-client (MD2C) platforms, where clients can request quotes simultaneously to multiple dealers in competition. To make clients select them as dealers, broker-dealers typically also provide indicative streams of prices for standard volumes of the contract, which are helpful for price discovery. Therefore, those platforms operated by providers like Bloomberg, MarketAxess or Tradeweb, have become the standard alternative to centralized exchanges based on the Central Limit Order Book for illiquid instruments. At the heart of the platform is a standard request for quote (RfQ) protocol to interact between client and broker-dealers.","type":"content","url":"/markdown/market-microstructure#quote-driven-markets-the-request-for-quote-protocol","position":19},{"hierarchy":{"lvl1":"Market microstructure","lvl3":"Mechanics of the RfQ protocol in MD2C platforms","lvl2":"Quote-driven markets: the Request for Quote protocol"},"type":"lvl3","url":"/markdown/market-microstructure#mechanics-of-the-rfq-protocol-in-md2c-platforms","position":20},{"hierarchy":{"lvl1":"Market microstructure","lvl3":"Mechanics of the RfQ protocol in MD2C platforms","lvl2":"Quote-driven markets: the Request for Quote protocol"},"content":"A client request a quote to a number of dealers N preselected in the electronic platform. The platform sends the RfQ to the selected dealers, who have a maximum time specified by the platform to answer with a quote. As dealers respond, the client receives the quotes immediately, and can decide to trade at any time, or walk away in case she does not want to trade with any of the dealers. When they are willing to trade, though, they are forced to trade with the dealer that offers the most competitive quote. A sketch of this process is shown in the figure below.\n\n\n\nFigure 5:Sketch of the RfQ protocol in a MD2C platform with four dealers in competition\n\nDealers have the following information pre-trade:\n\nThe client’s identity\n\nThe instrument, size and side for which the client request a quote. There is a special type of RfQs called Request for Market (RfM) where dealers don’t know the side and are requested to quote simultaneously bid and ask quotes.\n\nThe number of dealers in competition, although they don’t know their identities\n\nSome platforms offer a composite of indicative dealer’s quotes for specific sizes, using proprietary methodologies. For instance Bloomberg’s CBBT for bonds or MarketAxess CP Plus. However, they don’t know the actual quotes that dealers in competition are sending to the client.\n\nOnce the client decides to trade or walk away, dealer’s have typically access to the following information post-trade:\n\nIf they are chosen to trade, they know the second best price that was quoted by dealers in competition, which is called the cover price. This information is useful to understand how competitive are their prices with respect to other dealers and adjust them if they see consistent biases across many RfQs.\n\nIf they quoted the second best price (the cover price), they are informed that they were cover, although they don’t get informed of the price at which the client traded.\n\nIn any other case, they know if the client traded or just walked away, and implicitly that they were not cover since they did not get informed of being cover.\n\nDepending on the instrument and the regulation, dealers are requested to publish later information about trades in public repositories, which typically uphold the information for a time until it is released, which can be as short as 15-20 minutes and as long as multiple days. This is the case for instance for bonds in Europe, where MiFID 2 regulation requests that trades are published in public APAs (Approved Publication Arrangements). In the USA, FINRA has a similar requirement to publish trades in TRACE (Trade Reporting and Compliance Engine). There are also private information sharing schemes like Trax, where dealers willingly share information about trades to get access to information from other dealers, typically with some limitations in instrument universe, aggregation of volumes and prices over time periods, etc.","type":"content","url":"/markdown/market-microstructure#mechanics-of-the-rfq-protocol-in-md2c-platforms","position":21},{"hierarchy":{"lvl1":"Market microstructure","lvl3":"Request for Streaming (RfS)","lvl2":"Quote-driven markets: the Request for Quote protocol"},"type":"lvl3","url":"/markdown/market-microstructure#request-for-streaming-rfs","position":22},{"hierarchy":{"lvl1":"Market microstructure","lvl3":"Request for Streaming (RfS)","lvl2":"Quote-driven markets: the Request for Quote protocol"},"content":"A related protocol to RfQ is Request for Streaming, in which  dealers are asked by the client to provide executable prices for a period of time for particular sizes. The client can accept the price at any time.","type":"content","url":"/markdown/market-microstructure#request-for-streaming-rfs","position":23},{"hierarchy":{"lvl1":"Market microstructure","lvl3":"Strategic considerations for dealers and clients","lvl2":"Quote-driven markets: the Request for Quote protocol"},"type":"lvl3","url":"/markdown/market-microstructure#strategic-considerations-for-dealers-and-clients","position":24},{"hierarchy":{"lvl1":"Market microstructure","lvl3":"Strategic considerations for dealers and clients","lvl2":"Quote-driven markets: the Request for Quote protocol"},"content":"The existence of MD2C platforms for clients has the advantage of providing competition to dealers who are then forced to provide better prices to clients than in proprietary SD2C platforms. However, in practice, clients don’t usually select the maximum number of dealers allowed by the platforms (which can be around 20 or 30). The reason is that doing so has the trade-off of informing multiple dealers of the client’s intention to trade. Even if a dealer does not end up trading with the client, this information might be relevant to consider potential short-term movements in the market, and might be exploited by them although in some jurisdictions this is considering an illegal practice of front running the client.\n\nDealers might also be discouraged to quote competitively to win the RfQ when they see a lot of dealers in competition for similar reasons: dealers typically hedge their positions after trading using similar instruments and liquid markets. Therefore, other dealers might take advantage of potential short-term movements in the hedging instruments after trading, and take advantage of them, front running the winning dealer. They will also have a better idea of inventory positions in the market, and therefore potential needs to liquidate them aggressively in adverse scenarios.\n\nThis means that in practice clients tend to carefully balance the number of dealers in competition to get the benefit of competition without providing too many of them with valuable trade information.\n\nFor dealers, the quoting decision has also multiple strategic considerations. Apart from the fact that deciding to trade with multiple dealers in competition might be risky due to potential front-running effects in the hedging instruments, choosing the optimal quote is a complex problem that is actually prone to the use of mathematical and statistical analysis, as we will see in other chapters. For the moment, consider the following trade-off:\n\nquote too aggressively (i.e. with prices that are more likely to be accepted by the client) and they might trade with the client at the expense of profits.\n\nquote too conservatively and they might be consistently out-competed by other dealers.\n\nTo find the right balance, dealers tend to monitor metrics like:\n\nhit&miss, i.e. the ratio of trades (hits) with them over the ratio of total trades with other dealers (hits plus misses). A hit&miss that is consistently too high signals a quoting policy that is too aggressive, whereas if it is too low the policy might be too conservative.\n\nthe distance to the cover price, to analyze how far are they quoting with respect to the next dealer. Small distances to cover are an indication of a healthy pricing policy.\n\nDealers need to have other considerations in mind when quoting. For instance, some clients typically request quotes without the intention to trade, just to get information about price levels in illiquid instruments (price discovery). Unless dealers are using trading algorithms, it is convenient to flag those clients and don’t spend time providing them quotes.\n\nOther clients might have hidden information to the dealer when requesting quotes, that will later adversely affect the dealer. For instance, if they are sending a large order to multiple dealers split in small sizes, so each set of dealers only see a part of the full order. As mentioned, when hedging those orders the market for the hedging instruments might have a large price impact from the multiple dealers, costing them profits from the hedging size. In other cases, clients might have consistently better information about short-term price movements due to better market intelligence or insider information. Those flows are therefore adverse or toxic to the dealers, and they need to identify those clients before-hand in order to avoid potential losses in their trading books when trading with them.","type":"content","url":"/markdown/market-microstructure#strategic-considerations-for-dealers-and-clients","position":25},{"hierarchy":{"lvl1":"Market microstructure","lvl3":"RfQ Data","lvl2":"Quote-driven markets: the Request for Quote protocol"},"type":"lvl3","url":"/markdown/market-microstructure#rfq-data","position":26},{"hierarchy":{"lvl1":"Market microstructure","lvl3":"RfQ Data","lvl2":"Quote-driven markets: the Request for Quote protocol"},"content":"RfQ data is more difficult to come across than order book data since it is proprietary to the dealers quoting. Some agreements have been done in the past between dealers and researchers to share data, but public data is not widely available. RfQ datasets are usually offered in two levels of aggregation:\n\nFull negotiation history: those typically contain the full history of the negotiation, including intermediate states in which the dealer might update quotes before the client decides to trade.\n\nFinal negotiation status: in this case, the dataset only offers the final status of the negotiation, i.e. if the client traded with the dealer, traded away or walked away.\n\nIn both cases, the dataset typically includes the type or RfQ (RfQ, RfS, RfM), the timestamp of the status update, the client name and/or ID, the instrument name and/or ID, the size, the side, the price quoted by the dealer and the status. Additional information like the composite mid-price provided by the platform and additional market data might also be included.","type":"content","url":"/markdown/market-microstructure#rfq-data","position":27},{"hierarchy":{"lvl1":"Market microstructure","lvl2":"Financial Security Identification Codes"},"type":"lvl2","url":"/markdown/market-microstructure#financial-security-identification-codes","position":28},{"hierarchy":{"lvl1":"Market microstructure","lvl2":"Financial Security Identification Codes"},"content":"To conclude the chapter, let us review various unique identification codes that are used to uniquely identify financial instruments. These codes ensure clarity and reduce errors in transactions:\n\nTicker Symbol: A ticker symbol is a unique identifier assigned to a specific security listed on an exchange or traded publicly. All listed securities have a unique ticker symbol, which facilitates their identification during trading. Example: IBM, MSFT\n\nISIN (International Securities Identification Number):\nThe International Securities Identification Number (ISIN) is a globally recognized identifier based on ISO standards. It uniquely identifies a security regardless of its currency or the exchange on which it is traded. However, to specify the exchange, another identifier, such as the Market Identifier Code (MIC), is often required. ISINs are widely used for bonds, equities, warrants, commercial paper, and most listed derivatives. Example: LT0000610040\n\nCUSIP (Committee on Uniform Securities Identification Procedures): The CUSIP is an identifier used primarily in the United States to represent stocks and bonds. It consists of nine characters: the first six denote the issuer, the next two specify the type of instrument, and the last is a control digit. Example: 008000AA7\n\nSEDOL (Stock Exchange Daily Official List): The SEDOL code is used for securities trading on the London Stock Exchange and other exchanges in the United Kingdom. It is particularly useful for securities not traded in the United States. While SEDOL does not depend on the exchange, the MIC is required for specifying the exchange. Example: B0WNLY7\n\nRIC (Reuters Instrument Code): The Reuters Instrument Code (RIC) is a proprietary code used by Reuters to identify financial instruments and indices. It consists of two parts separated by a period (“.”). The first part represents the instrument, while the second, which is optional, denotes the exchange. Example: MSFT.OQ","type":"content","url":"/markdown/market-microstructure#financial-security-identification-codes","position":29},{"hierarchy":{"lvl1":"Mean reversion strategies"},"type":"lvl1","url":"/markdown/mean-reversion-strategies","position":0},{"hierarchy":{"lvl1":"Mean reversion strategies"},"content":"","type":"content","url":"/markdown/mean-reversion-strategies","position":1},{"hierarchy":{"lvl1":"Mean reversion strategies","lvl2":"Introduction"},"type":"lvl2","url":"/markdown/mean-reversion-strategies#introduction","position":2},{"hierarchy":{"lvl1":"Mean reversion strategies","lvl2":"Introduction"},"content":"","type":"content","url":"/markdown/mean-reversion-strategies#introduction","position":3},{"hierarchy":{"lvl1":"Mean reversion strategies","lvl2":"Exercises"},"type":"lvl2","url":"/markdown/mean-reversion-strategies#exercises","position":4},{"hierarchy":{"lvl1":"Mean reversion strategies","lvl2":"Exercises"},"content":"","type":"content","url":"/markdown/mean-reversion-strategies#exercises","position":5},{"hierarchy":{"lvl1":"Optimal portfolios"},"type":"lvl1","url":"/markdown/optimal-portfolios","position":0},{"hierarchy":{"lvl1":"Optimal portfolios"},"content":"","type":"content","url":"/markdown/optimal-portfolios","position":1},{"hierarchy":{"lvl1":"Optimal portfolios","lvl2":"Introduction"},"type":"lvl2","url":"/markdown/optimal-portfolios#introduction","position":2},{"hierarchy":{"lvl1":"Optimal portfolios","lvl2":"Introduction"},"content":"","type":"content","url":"/markdown/optimal-portfolios#introduction","position":3},{"hierarchy":{"lvl1":"Optimal portfolios","lvl2":"Exercises"},"type":"lvl2","url":"/markdown/optimal-portfolios#exercises","position":4},{"hierarchy":{"lvl1":"Optimal portfolios","lvl2":"Exercises"},"content":"","type":"content","url":"/markdown/optimal-portfolios#exercises","position":5},{"hierarchy":{"lvl1":"Preface"},"type":"lvl1","url":"/markdown/preface","position":0},{"hierarchy":{"lvl1":"Preface"},"content":"WIP","type":"content","url":"/markdown/preface","position":1},{"hierarchy":{"lvl1":"Preface","lvl2":"Prior knowledge"},"type":"lvl2","url":"/markdown/preface#prior-knowledge","position":2},{"hierarchy":{"lvl1":"Preface","lvl2":"Prior knowledge"},"content":"WIP","type":"content","url":"/markdown/preface#prior-knowledge","position":3},{"hierarchy":{"lvl1":"Preface","lvl2":"How to read this book"},"type":"lvl2","url":"/markdown/preface#how-to-read-this-book","position":4},{"hierarchy":{"lvl1":"Preface","lvl2":"How to read this book"},"content":"WIP","type":"content","url":"/markdown/preface#how-to-read-this-book","position":5},{"hierarchy":{"lvl1":"Preface","lvl2":"Acknowledgments"},"type":"lvl2","url":"/markdown/preface#acknowledgments","position":6},{"hierarchy":{"lvl1":"Preface","lvl2":"Acknowledgments"},"content":"WIP","type":"content","url":"/markdown/preface#acknowledgments","position":7},{"hierarchy":{"lvl1":"Quantitative investment fundamentals"},"type":"lvl1","url":"/markdown/quant-investment-fundamentals","position":0},{"hierarchy":{"lvl1":"Quantitative investment fundamentals"},"content":"","type":"content","url":"/markdown/quant-investment-fundamentals","position":1},{"hierarchy":{"lvl1":"Quantitative investment fundamentals","lvl2":"Introduction"},"type":"lvl2","url":"/markdown/quant-investment-fundamentals#introduction","position":2},{"hierarchy":{"lvl1":"Quantitative investment fundamentals","lvl2":"Introduction"},"content":"","type":"content","url":"/markdown/quant-investment-fundamentals#introduction","position":3},{"hierarchy":{"lvl1":"Quantitative investment fundamentals","lvl2":"Exercises"},"type":"lvl2","url":"/markdown/quant-investment-fundamentals#exercises","position":4},{"hierarchy":{"lvl1":"Quantitative investment fundamentals","lvl2":"Exercises"},"content":"","type":"content","url":"/markdown/quant-investment-fundamentals#exercises","position":5},{"hierarchy":{"lvl1":"References"},"type":"lvl1","url":"/markdown/references","position":0},{"hierarchy":{"lvl1":"References"},"content":"","type":"content","url":"/markdown/references","position":1},{"hierarchy":{"lvl1":"Release Notes"},"type":"lvl1","url":"/markdown/releases","position":0},{"hierarchy":{"lvl1":"Release Notes"},"content":"","type":"content","url":"/markdown/releases","position":1},{"hierarchy":{"lvl1":"Release Notes","lvl2":"02.26"},"type":"lvl2","url":"/markdown/releases#id-02-26","position":2},{"hierarchy":{"lvl1":"Release Notes","lvl2":"02.26"},"content":"Added new sections to the Algorithmic Trading chapters.","type":"content","url":"/markdown/releases#id-02-26","position":3},{"hierarchy":{"lvl1":"Release Notes","lvl2":"12.25"},"type":"lvl2","url":"/markdown/releases#id-12-25","position":4},{"hierarchy":{"lvl1":"Release Notes","lvl2":"12.25"},"content":"Expanded fair price estimation chapter with fundamental models for fair pricing, including the stochastic discount factor model, with an extensive example from fixed income. Added new sections on Algorithmic Trading and Intro to Financial Instruments chapters. Fixed some typos in references.","type":"content","url":"/markdown/releases#id-12-25","position":5},{"hierarchy":{"lvl1":"Release Notes","lvl2":"11.25"},"type":"lvl2","url":"/markdown/releases#id-11-25","position":6},{"hierarchy":{"lvl1":"Release Notes","lvl2":"11.25"},"content":"Started chapter introducing the generalities of Algorithmic Trading. Continued working on financial instruments chapter and extended the discussion of fair value estimation using Kalman filters to add a discussion on specific pricing sources.","type":"content","url":"/markdown/releases#id-11-25","position":7},{"hierarchy":{"lvl1":"Release Notes","lvl2":"10.25"},"type":"lvl2","url":"/markdown/releases#id-10-25","position":8},{"hierarchy":{"lvl1":"Release Notes","lvl2":"10.25"},"content":"Completed the section Kalman filter for fair estimation, including an example with simulated data. Working on the introduction to the mechanics of financial instruments.","type":"content","url":"/markdown/releases#id-10-25","position":9},{"hierarchy":{"lvl1":"Release Notes","lvl2":"09.25"},"type":"lvl2","url":"/markdown/releases#id-09-25","position":10},{"hierarchy":{"lvl1":"Release Notes","lvl2":"09.25"},"content":"Completed the section on latent variable models within the Introduction to Bayesian Probability chapter. Added EM derivations for the Gaussian Mixture Model, the Hidden Markov Model and the Local Level Model (simple version of the Kalman Filter). Provided simulations in the accompanying notebooks.","type":"content","url":"/markdown/releases#id-09-25","position":11},{"hierarchy":{"lvl1":"Release Notes","lvl2":"08.25"},"type":"lvl2","url":"/markdown/releases#id-08-25","position":12},{"hierarchy":{"lvl1":"Release Notes","lvl2":"08.25"},"content":"Started section on RfQ models, describing a general model for the RfQ process, as well as models for the arrival of RfQs, attrition risk and abnormal client behavior. A new notebook has been uploaded with the full code for the simulation of synthetic client data and implementations of the models. Additionally, worked on Gen AI and Bayesian chapters.","type":"content","url":"/markdown/releases#id-08-25","position":13},{"hierarchy":{"lvl1":"Release Notes","lvl2":"05.25"},"type":"lvl2","url":"/markdown/releases#id-05-25","position":14},{"hierarchy":{"lvl1":"Release Notes","lvl2":"05.25"},"content":"Kalman filter and smoothing derivation for local level model. Introduction to Bayesian probability and decision theory.","type":"content","url":"/markdown/releases#id-05-25","position":15},{"hierarchy":{"lvl1":"Release Notes","lvl2":"02.25"},"type":"lvl2","url":"/markdown/releases#id-02-25","position":16},{"hierarchy":{"lvl1":"Release Notes","lvl2":"02.25"},"content":"Kalman filter models for mid-price estimation section started. New chapter on Gen AI started.","type":"content","url":"/markdown/releases#id-02-25","position":17},{"hierarchy":{"lvl1":"Release Notes","lvl2":"01.25"},"type":"lvl2","url":"/markdown/releases#id-01-25","position":18},{"hierarchy":{"lvl1":"Release Notes","lvl2":"01.25"},"content":"First version of intro to financial markets and market structure chapter finished.","type":"content","url":"/markdown/releases#id-01-25","position":19},{"hierarchy":{"lvl1":"Release Notes","lvl2":"12.24"},"type":"lvl2","url":"/markdown/releases#id-12-24","position":20},{"hierarchy":{"lvl1":"Release Notes","lvl2":"12.24"},"content":"First version of market microstructure chapter finished.","type":"content","url":"/markdown/releases#id-12-24","position":21},{"hierarchy":{"lvl1":"Release Notes","lvl2":"11.24"},"type":"lvl2","url":"/markdown/releases#id-11-24","position":22},{"hierarchy":{"lvl1":"Release Notes","lvl2":"11.24"},"content":"Started chapters on financial markets and market microstructure, with particular emphasis on the central limit order book. Added new notebooks on market microstructure. Added simulation of processes in stochastic calculus chapter","type":"content","url":"/markdown/releases#id-11-24","position":23},{"hierarchy":{"lvl1":"Release Notes","lvl2":"10.24"},"type":"lvl2","url":"/markdown/releases#id-10-24","position":24},{"hierarchy":{"lvl1":"Release Notes","lvl2":"10.24"},"content":"Added jump processes sections. Added several simulations for stochastic processes, including code in the notebooks section. Added market price of risk for options discussion in the Fair price estimation chapter.","type":"content","url":"/markdown/releases#id-10-24","position":25},{"hierarchy":{"lvl1":"Release Notes","lvl2":"09.24"},"type":"lvl2","url":"/markdown/releases#id-09-24","position":26},{"hierarchy":{"lvl1":"Release Notes","lvl2":"09.24"},"content":"Added section for the arbitrage - free theory of options (Black - Scholes - Merton theory) within the Fair Price Estimation chapter","type":"content","url":"/markdown/releases#id-09-24","position":27},{"hierarchy":{"lvl1":"Release Notes","lvl2":"05.24"},"type":"lvl2","url":"/markdown/releases#id-05-24","position":28},{"hierarchy":{"lvl1":"Release Notes","lvl2":"05.24"},"content":"Started working on chapter Fair Price Estimation. Added section on how to price derivatives using utility indifference theory","type":"content","url":"/markdown/releases#id-05-24","position":29},{"hierarchy":{"lvl1":"Release Notes","lvl2":"03.24"},"type":"lvl2","url":"/markdown/releases#id-03-24","position":30},{"hierarchy":{"lvl1":"Release Notes","lvl2":"03.24"},"content":"Created online book. Started work on the following chapters: Financial Markets, Bayesian Modelling and Stochastic Calculus","type":"content","url":"/markdown/releases#id-03-24","position":31},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets"},"type":"lvl1","url":"/markdown/rfq-models","position":0},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets"},"content":"","type":"content","url":"/markdown/rfq-models","position":1},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets","lvl2":"Introduction"},"type":"lvl2","url":"/markdown/rfq-models#introduction","position":2},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets","lvl2":"Introduction"},"content":"In the context of platforms where the client identity is known, like Dealer to Client platforms, the analysis of the client patterns of trading is relevant in order to fine-tune pricing and hedging models, or to optime the management of axes.","type":"content","url":"/markdown/rfq-models#introduction","position":3},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets","lvl2":"Probabilistic graphical model for RfQs"},"type":"lvl2","url":"/markdown/rfq-models#probabilistic-graphical-model-for-rfqs","position":4},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets","lvl2":"Probabilistic graphical model for RfQs"},"content":"A model that captures the Request for Quote process discussed in the \n\nMarket Microstructure chapter is the probabilistic graphical model proposed by \n\nMarín et al., 2025, which we reproduce in the following figure:\n\n\n\nFigure 1:Probabilistic graphical model for the Request for Quote process\n\nThe model is composed of the following elements. Some of them are known pre-trade, others post-trade depending on the result of the RfQ, and some are just simply non-observable (latent) variables:\n\nRfQ: a binary random variable indicating whether an RfQ is initiated. Each RfQ carries associated features, collectively represented by the variable RF in the model. These include, for example, the time t of the request, the side s (buy or sell) — where “buy” implies the dealer purchases from the client, and “sell” implies the dealer sells to the client — the volume v, and the number of competing dealers n chosen by the client (typically subject to platform-imposed limits). The trigger for an RfQ can stem from external client factors, such as a specific trading strategy, which fall outside the model’s scope. In some cases, it may also result from proactive engagement by the dealer’s sales force, captured in the variable call. Other factors that may shape the client’s decision to request a quote, and which are explicitly included in the model, include:\n\nPrice discovery (PD): The client is interested in discovering the fair value of the instrument but has no intention to execute a trade. This intent is unobservable (a latent variable), as the dealer cannot infer the client’s true motivation at the time the RfQ is received.\n\nClient information asymmetry (IA): If the client possesses meaningful information about the short-term direction of the market, this creates a situation of information asymmetry. There are several possible sources for this asymmetry:\n\nThe client’s trading activity itself may generate market impact, yet the RfQ submitted to the dealer often reflects only a portion of the total intended order. In this case, the information will affect the market and is captured in the model as a price drift (\\mu) variable, with a direct link represented as IA \\rightarrow \\mu.\n\nAlternatively, the client might have access to insider information or sophisticated analytics that provide insight into future price movements. This scenario is also modeled via the drift variable, though here there is no direct causal connection between information asymmetry and drift —rather, the client forms an estimate of the drift but does not influence its true value post-trade.\n\nAxe: A publicly expressed interest by the dealer in executing a trade at a favorable price (either bid or ask), which can increase the likelihood that a client will submit an RfQ.\n\nClient Features (CF): Includes the identity of the client as well as broader attributes that may influence trading behavior—such as industry sector, geographic location, or credit rating.\n\nBond Features (BF): Encompasses the bond’s unique identifier along with its financial attributes—such as coupon, yield, maturity, and interest rate sensitivity (e.g., DV01). It also includes factors related to liquidity, such as market bid-ask spreads, volatility, and the typical number of dealers quoting RfQs in that bond.\n\nRfQ Status (RS): The final outcome of the RfQ can be categorized into three main cases:\n\nHit: The client executes a trade with the dealer.\n\nMissed: The client trades with a different dealer.\n\nPassed: The client does not trade with any dealer. This may occur because the quotes were not attractive enough or the client was merely seeking price information (price discovery).\n\nThe drivers of the RfQ status in this model include:\n\nThe client’s intent to trade, which is what initiates a Request for Quote (RfQ) via a multi-dealer-to-client platform.\n\nIf the quote is requested solely for price discovery (PD) purposes, the only possible outcome with non-zero probability is a passed status (i.e., the client walks away without trading).\n\nThe half-spread (\\delta) quoted by the dealer. Although dealers provide price quotes to clients, we assume that the client’s trading decision depends on the quoted spread relative to a reference mid-price P_{m}. For example, using a composite price from the platform, the transaction price is modeled as:P = P_{m} + s \\delta, where s = 1 if the dealer is selling (ask), and s = -1 if buying (bid).\n\nThe spreads quoted by competing dealers, denoted \\delta_{dealer}. Clients may choose to include up to n dealers in the RfQ. Naturally, as n increases, so does the probability of receiving a more competitive quote.  Typically, all quoting dealers know the number of participants n, but not the individual prices submitted by others. The only partial post-trade information available comes when a dealer wins the RfQ and the platform discloses the cover price (i.e., the second-best price). The dealer who submitted this cover price is informed of their status as covered, but they do not receive information on the actual winning price.\n\nThe client’s reservation spread \\delta_{res}: the maximum acceptable spread relative to the mid-price, defined as\n\n\\delta_{res} = s(P_{res} - P_{m}),\n\nwhere s is the trade side as defined earlier.If all quoted prices exceed this threshold, the client will not trade (passed status). Since \\delta_{res} is not directly observable, it must be inferred from historical client behavior. Its determinants include client features (CF), bond features (BF), and RfQ characteristics (RF). Additionally, information asymmetry (IA) is expected to play a role—clients with better information may be more inclined to accept trades.Likewise, higher market volatility (\\sigma) may prompt clients to trade more readily (e.g., to reduce risk exposure).\n\nRevenue (R): The profitability of the trade. Dealers use several different revenue metrics depending on the context:\n\nInstantaneous Flow Value (R_0):  A simple per-trade measure of profitability, calculated as  R_0 = v \\delta where v is the trade volume and \\delta is the half-spread at the time of the trade. This metric does not consider the gains or losses realized when the inventory is eventually unwound via an opposite trade.\n\nRound-Trip Revenue (R_{rt}):  Addresses the limitations of the instantaneous flow value by considering the full round-trip (buy and sell or vice versa). Revenue is computed as:\n\nR_{rt} = s v (P_{t_{rt}} - P_t) = v \\delta_{t_{rt}} - v \\delta_{t} + s v (P_{m,t_{rt}} - P_{m,t})\n\nwhere t is the RfQ time and t_{rt} is the round-trip time.Assuming Brownian motion for price dynamics:\n\nP_{m,t_{rt}} - P_{m,t} = \\mu(t_{rt} - t) + \\sigma (W_{t_{rt}} - W_t)\n\nwhere \\mu is the drift, \\sigma is the volatility, and W_t is a Wiener process such that W_{t_{rt}} - W_t \\sim N(0, t_{rt} - t)  The stochastic component (W) captures the effect of market external factors (MXF).  The model also allows for a causal relationship between information asymmetry (IA) and drift (\\mu), to reflect situations where client trading influences market direction.\nBoth \\mu and \\sigma are latent variables—not observable at trade time.  However, the model assumes that \\sigma (volatility) is relatively predictable, so its pre- and post-trade values are treated as equivalent. This makes it a valid driver of both dealer pricing (\\delta, \\delta_{\\text{dealer}}) and client reservation spreads (\\delta_{\\text{res}}).  In contrast, \\mu (drift) is much harder to predict reliably, except in cases where clients have asymmetric information. Hence, post-trade drift is excluded from dealer pricing logic.\n\nEnd-of-Day Flow Value (R_{\\text{eod}}):  Used when trades remain open over long time horizons (e.g., illiquid bonds). Revenue is marked-to-market at the end of the day:\n\nR_{\\text{eod}} = v \\delta_t + s v (P_{m,T} - P_{m,t})\n\nwhere T is the end-of-day timestamp. This measure can incorporate liquidity penalties to reflect the risk of holding hard-to-sell positions.\n\nShort-Term Flow Value (R_{t+h}):  Evaluates revenue at a short time horizon h (seconds to minutes) after trade execution:\n\nR_{t+h} = v \\delta_t + s v (P_{m,t+h} - P_{m,t}) .\n\nThis is particularly useful for identifying information asymmetry (IA), as such informational advantages typically manifest within short timeframes before being diluted by general market dynamics (MXF).","type":"content","url":"/markdown/rfq-models#probabilistic-graphical-model-for-rfqs","position":5},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets","lvl2":"Generative models for the request for quote activity"},"type":"lvl2","url":"/markdown/rfq-models#generative-models-for-the-request-for-quote-activity","position":6},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets","lvl2":"Generative models for the request for quote activity"},"content":"","type":"content","url":"/markdown/rfq-models#generative-models-for-the-request-for-quote-activity","position":7},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets","lvl3":"Models for the arrival of RfQs","lvl2":"Generative models for the request for quote activity"},"type":"lvl3","url":"/markdown/rfq-models#models-for-the-arrival-of-rfqs","position":8},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets","lvl3":"Models for the arrival of RfQs","lvl2":"Generative models for the request for quote activity"},"content":"A dealer receives RfQs for different products from different clients every day or week. Each RfQ has a side, buy or sell, and a volume requested to be traded. A generative model that captures this distribution can be encoded by the following probability:P(\\tau_{\\text{RfQ}(\\text{c, p, s, v)}} \\in [t, t+dt]|F_t)\n\nwhere \\tau_{\\text{RfQ}(\\text{c, p, s, v})} is a random variable that specifies the time at which a client \\text{c} requests a quote for product \\text{p} of side \\text{s} and volume \\text{v} (for the moment we assume volumes to be discrete), and we have also a filtration F_t that includes all relevant information up to t. Alternatively we can introduce N_t^{\\text{RfQ}(\\text{c, p, s, v)}} as the number of such RfQs up to time t. These two formulations are in fact equivalent:P(\\tau_{\\text{RfQ}(\\text{c, p, s, v})} \\in [t, t+dt]|F_t) =P(N_{t+dt}^{\\text{RfQ}(\\text{c, p, s, v})} - N_t^{\\text{RfQ}(\\text{c, p, s, v})} = 1|F_t)\n\nFrom the Stochastic Calculus chapter we recognize this model as a counting process, whose probability distribution we can write as:P(\\tau_{\\text{RfQ}(\\text{c, p, s, v})} \\in [t, t+dt]|F_t) = \\lambda_t (c, p, s, v)dt\n\nwith the so-called intensity \\lambda_t being a general function of time, RfQ variables and any relevant information contained in F_t, for instance, previous RfQs. This is an instance of a non-homogeneous Poisson process. If it also depends on previous times of arrivals of RfQs, i.e. it is a self-exciting process, we have a Hawkes process. Such counting processes are natural candidates to model the arrival of RfQs.\n\nAlternatively, we can rewrite the model using the product rule of probability as:P(\\tau_{\\text{RfQ}(\\text{c, p, s, v})} \\in [t, t+dt]|F_t) =  P(v| s, c, p, \\text{RfQ}_{t,t+dt}, F_t) \\\\  P(s|c, p, \\text{RfQ}_{t,t+dt}, F_t)  P(c, p| \\text{RfQ}_{t,t+dt}, F_t) P(\\text{RfQ}_{t,t+dt}|F_t)\n\nwhere we have used the compact notation: \\text{RfQ}_{t,t+dt} instead of \\tau_{\\text{RfQ}} \\in [t, t+dt]. This way, we can explicitly model the statistics of each of the probabilities in the product. A typical hypothesis is to make the conditional probabilities independent of time and use simple distributions to model them. For instance, volumes are modelled in many cases as continuous distribution following a power law or a log-normal probability density, since volumes are positive and heavy tailed in their distribution:P(V \\in [v, v+dv]| s, c, p, \\text{RfQ}) = \\frac{\\alpha -1}{v_\\text{min}}\\left(\\frac{v}{v_\\text{min}}\\right)^{-\\alpha} dv\n\nwhere v \\geq v_\\text{min}, \\alpha > 1.\n\nThe side s, being a binary variable, buy or sell, is typically modelled using a Bernoulli distribution. Finally, the probabilities that RfQs come from a specific client or product can be modelled using a multinomial distribution.","type":"content","url":"/markdown/rfq-models#models-for-the-arrival-of-rfqs","position":9},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets","lvl3":"Attrition risk","lvl2":"Generative models for the request for quote activity"},"type":"lvl3","url":"/markdown/rfq-models#attrition-risk","position":10},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets","lvl3":"Attrition risk","lvl2":"Generative models for the request for quote activity"},"content":"The previous model considers that clients have a stable pattern of choosing the dealer for their requests for quotes. However, clients might at some point stop sending RfQs to the dealer, for instance when they perceive that prices are non competitive on average (i.e. they see low hit & miss ratios) or when dealers often don’t respond with their quotes.\n\nA simple extension of the previous model for the arrival of RfQs introduces a latent binary random variable to characterize if clients are actively engaging with the dealer or not. Denoting this variable a, with a=1 meaning the client is active, the model is decomposed as:P(\\tau_{\\text{RfQ}(\\text{c, p, s, v)}} \\in [t, t+dt]|F_t) = P(\\tau_{\\text{RfQ}(\\text{c, p, s, v)}} \\in [t, t+dt]|a = 1, F_t) P(a = 1|F_t)\n\nwhere we have implicitly used the fact that P(\\tau_{\\text{RfQ}(\\text{c, p, s, v)}} \\in [t, t+dt]|a = 0, F_t) = 0, i.e. we don’t expect the arrival of RfQs for clients that are inactive. We are therefore interested in characterizing P(a = 1|F_t) or most typically P(a = 0|F_t) = 1- P(a = 1|F_t), which is known in the marketing analytics literature as the attrition risk model.\n\nInferences of attrition risk can be done by analyzing the historical patterns of client trading activity.  A simple albeit elegant model is that from Fader et al \n\nFader et al., 2010, where they consider daily (or any other time scale) client activity as a set of independent identically distributed Bernoulli random variables Y_t with probability p. In our RfQ setup, this means Y_t = 1 if the client sends any RfQ, or Y_t = 0 otherwise.  Additionally, a client can become inactive at the beginning of the day with probability \\theta. Given a pattern of historical daily activity D of a client we can compute the probability that the client is active using Bayes’ theorem:P(a = 1|D, p, \\theta) = \\frac{P(D|a=1, p, \\theta)P(a = 1|p, \\theta)}{P(D|a=1, p, \\theta)P(a=1|p, \\theta) + P(D|a=0, p, \\theta)P(a=0|p, \\theta)}\n\nFor example, for the pattern D = 10100, the likelihood in the denominator is calculated as the probability that the the observed pattern is simply explained by the natural probabilities of requesting RfQs:P(D|a=1, p, \\theta) = p(1-p)p(1-p)^2\n\nwith the prior probability for the client being active corresponding to the scenario in which the client does not become inactive in all the observation periods:P(a = 1|p, \\theta) =  (1-\\theta)^5\n\nIn contrast, the alternative hypothesis in which the client is already inactive is actually the result of two different paths, one in which the client becomes inactive at the last day, whereas in the second it does it in the previous day. Therefore:P(D|a=0, p, \\theta)  P(a=0|p, \\theta)=  p(1-p)p(1-p)(1-\\theta)^4 \\theta +  p(1-p)p (1-\\theta)^3 \\theta\n\nThe model does not consider the possibility that clients which are inactive are activated again, which is definitely possible in reality, but not necessarily useful for a model that simply tries to detect clients that have recently stopped engaging with the dealer. Plugging these probabilities into Bayes’ theorem we get:P(a = 1|D, p, \\theta) = \\frac{p(1-p)p(1-p)^2(1-\\theta)^5}{p(1-p)p(1-p)^2(1-\\theta)^5+   p(1-p)p(1-p)(1-\\theta)^4 \\theta +  p(1-p)p (1-\\theta)^3 \\theta}=  \\frac{(1-p)^2(1-\\theta)^2}{(1-p)^2(1-\\theta)^2+ (1-p)(1-\\theta) \\theta +  \\theta}\n\nAs pointed out by the authors, any other trading pattern with the same number of active days --denoted as x (frequency in the marketing jargon), and the same number of days since the last RfQ --the recency r, has the same likelihood. For instance, P(a = 1|D = 10100, p, \\theta) = P(a = 1| D = 01100, p, \\theta). see the exercise at the end of the chapter. This is an interesting insight that naturally links the model with the traditional heuristics based on recency and frequency in the Marketing Analytics literature, see for example \n\nGrigsby, 2018. The general result for a pattern consisting of n days of trading activity, frequency x and recency r, we have:P(D|a=1, p, \\theta)P(a = 1|p, \\theta)= p^{x}(1-p)^{n-x} (1-\\theta)^nP(D|a=0, p, \\theta)P(a = 0|p, \\theta) =  \\sum_{i=1}^{r} p^x(1-p)^{n-x-i}(1-\\theta)^{n-i} \\theta\n\nNotice that the evidence in the denominator of Bayes’ theorem it is simply the complete likelihood of the data:P(D|p, \\theta) = p^{x}(1-p)^{n-x} (1-\\theta)^n + \\sum_{i=1}^{r} p^x(1-p)^{n-x-i}(1-\\theta)^{n-i} \\theta \\equiv L(D|p,\\theta)\n\nTherefore:P(a=1|D, p, \\theta) = \\frac{p^{x}(1-p)^{n-x} (1-\\theta)^n}{L(D|p,\\theta)}\n\nThe model in this form requires a separate estimation of parameters p, \\theta for each individual client. Alternatively, the model can be generalized to a segment of clients whose parameters follow a probability distribution but we cannot attach specific parameters to any of them. As it is natural to model distributions of probabilities, Fader et al propose the use of Beta distributions for p and \\theta:f(p |\\alpha, \\beta) = \\frac{p^{\\alpha - 1}(1 - p)^{\\beta - 1}}{B(\\alpha, \\beta)} \\quad \\text{for } 0 \\leq p \\leq 1, \\, \\alpha, \\beta > 0f(\\theta| \\gamma, \\delta) = \\frac{\\theta^{\\gamma - 1}(1 - \\theta)^{\\delta - 1}}{B(\\gamma, \\delta)} \n\\quad \\text{for } 0 \\leq \\theta \\leq 1,\\; \\gamma, \\delta > 0\n\nwhere the Beta function is by definition:B(\\alpha, \\beta) \\equiv \\int dp p^{\\alpha - 1} (1-p)^{\\beta - 1}\n\nThis allow us to characterize the segment of clients with four parameters, \\alpha, \\beta, \\gamma and \\delta. Notice that this model does not capture the possibility of multiple segments within the population of clients. To capture that, a simple extension is to use a mixture of Betas, see next section for a simple application of such model in anomaly detection.\n\nContinuing with the single segment model, the likelihood now requires to integrate over the distribution of p and \\theta:P(D|\\alpha, \\beta, \\gamma, \\delta) = \\int dp d\\theta f(p| \\alpha, \\beta) f(\\theta| \\gamma, \\delta) L(D|p, \\theta)\n\nwhere recall that in this model, the dataset D can be reduced to n, r and x. The calculation can be carried out analytically in terms of Beta functions, since it involves integrals of the form:\\int dp f(p|\\alpha, \\beta) p^n (1-p)^m = \\frac{B(\\alpha + n, \\beta + m)}{B(\\alpha, \\beta)}\n\nThe likelihood for a single client can be expressed in terms of Beta functions:P(D|\\alpha, \\beta, \\gamma, \\delta) = \\frac{B(\\alpha + x, \\beta + n - x)}{B(\\alpha, \\beta)} \\frac{B(\\gamma, \\delta + n)}{B(\\gamma, \\delta)} + \\sum_{i=1}^{r} \\frac{B(\\alpha + x, \\beta + n - x- i)}{B(\\alpha, \\beta)} \\frac{B(\\gamma+1, \\delta + n-i)}{B(\\gamma, \\delta)}\n\nApplying accordingly Bayes theorem on P(a = 1|D, \\alpha, \\beta, \\gamma, \\delta) we get:P(a = 1|D, \\alpha, \\beta, \\gamma, \\delta) = \\frac{B(\\alpha + x, \\beta + n - x)}{B(\\alpha, \\beta)} \\frac{B(\\gamma, \\delta + n)}{B(\\gamma, \\delta)} L^{-1}(D|\\alpha, \\beta, \\gamma, \\delta)\n\nwhere we have defined L(D|\\alpha, \\beta, \\gamma, \\delta) \\equiv P(D|\\alpha, \\beta, \\gamma, \\delta)\n\nFor model parameters estimation, the authors suggests using maximum likelihood over the historical dataset. For the one segment model this implies a joint estimation of the parameters for the collective of clients in the dataset, since they share parameters. Alternatively, a Bayesian estimation approach would seek to estimate P(p, \\theta|D) for the single client model or P(\\alpha, \\beta, \\gamma, \\delta|D_N) for the one segment model, where we have used D_N to denote the full dataset of N clients.","type":"content","url":"/markdown/rfq-models#attrition-risk","position":11},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets","lvl4":"Simulation of attrition risk","lvl3":"Attrition risk","lvl2":"Generative models for the request for quote activity"},"type":"lvl4","url":"/markdown/rfq-models#simulation-of-attrition-risk","position":12},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets","lvl4":"Simulation of attrition risk","lvl3":"Attrition risk","lvl2":"Generative models for the request for quote activity"},"content":"Let us see how this model works in practice. We simulate a segment of 50 clients that send RfQs to a dealer according to the Poisson model discussed in the previous section. For simplicity, we only consider the situation in which the client wants to buy from the dealer. Each client is characterized by an intensity of RfQ arrival drawn from a Gaussian distribution with mean 1 (i.e. a client on average sends one RfQ per day) and standard deviation 0.05. In principle such choice of parameters makes highly unlikely to generate negative intensities, but if that happens we simply force them to be positive. The clients’ reservation prices, i.e. the maximum prices they would accept in order to trade, also follows a Gaussian distribution, in this case with mean 100 (which could be considered the fair price of the financial product) and standard deviation of 10% around this mean. These reservation prices are not static, though: they fluctuate over time with some noise that models the potential impact of changing market conditions. We choose 20% of the mean for this parameter.\n\nWe model attrition in this setup by considering that clients will stop sending RfQs to the dealer if the average hit rate (percentage ot trades over RfQs sent) is below a given threshold over a window of time. We take 10% as the threshold for all clients (although we could also model a distribution over the segment) and suppose that clients evaluate the hit rate with the dealer over a 10 days window (again, this could be made client dependent but we choose not to in this simulation).\n\nFinally, in order to produce a rich dynamics, we model the behavior of the dealer as follows: on the one hand, she has her own target hit ratio that tries to balance client satisfaction with he own profitability. We choose a 40% hit ratio target for all clients. To hit this target, the dealer try to infer the distribution of each of her clients’ reservation price using Bayesian methods, i.e. estimating the posterior distribution of the reservation price conditioned to the data, which consist of the time series of hits and misses of previous RfQs sent by clients and the prices quoted. Mathematically, let r_n the reservation price of client n, then the dealer seeks to infer the density function f(r_n|D), where D = \\{(c_i, p_{i}, h_{i})\\} is the dataset comprised of clients c_i, the prices p_{i,n} quoted to client n, and the result of the negotiation h_{i,n} = (1,0) equal to one when the client accepts the price (hit) and zero otherwise (miss). The dealer updates the posterior dynamically after every negotiation, and uses this information to quote the next RfQ by choosing the price such that the probability of hit (which occurs when the price quoted is lower than the reservation price) is equal to the hit target of 40%:P(r_n \\geq p|D) = \\int_p^\\infty dr_n f(r_n|D) = 0.4\n\nAt the beginning of the simulation, we assume that the dealer has a prior estimation of the distribution of reservation prices given by a Gaussian distribution of mean 60 and standard deviation 10, which means the estimation is relatively off with respect to reality.\n\nLet us simulate the model over 200 days. The following figure shows the mean of daily hit ratios that the dealer achieves with the clients that send RfQs that day (which are active by definition, although there could be active clients that a given day don’t send RfQs):\n\n\n\nFigure 2:Evolution of daily averages of hit rates with clients that sent RfQs each day. The dealer start with an off estimation of the reservation prices, which explains why it takes some time for dealers to reach their target hit ratios.\n\nThe effect of the prior estimation of reservation prices by the dealer is clearly reflected in the first part of the simulation, where hit ratios are far from the target set by the dealer. In particular, since the dealer has a low biased estimation of reservation prices, this translates into high hit rates at the beginning of the simulation. We see the effect of the Bayesian learning, though, as time goes on, since in the range of 25-50 days the hit rates have stabilized around the target, which remains noisy given the fluctuations in client reservation prices that we have introduced in the model. Precisely because of these fluctuations we find situations in which clients get average hit rates over their 10 days evaluation windows below their 10% target, which makes them stop sending RfQs to the dealer --becoming inactive, although they don’t inform the dealer of this situation. The following figure reflects this situation by plotting the evolution of the attrition rate, i.e. the cumulative percentage of clients who have become inactive since the beginning of the simulation:\n\n\n\nFigure 3:Cumulative attrition rate over time, i.e. percentage of inactive clients over the total number of clients (50) at the beginning of the simulation.\n\nPrecisely because the dealer starts with a pessimistic estimation of the reservation price of the clients, which translates into a large hit rate, attrition rates don’t kick off until the dealer corrects her estimation of the reservation prices and reaches her target hit rates. At this point, fluctuations in the reservation price from clients trigger attrition rates, which steadily grow over the simulation. If the dealer does not do anything to address the attrition risk, eventually all clients will become inactive. That’s where the attrition risk model comes into play, as a tool used by the dealer to detect which clients might have become inactive and trigger corrective actions, for example using her salesforce to contact clients and try to engage them back.\n\nWe implement the model discussed in this section based on the work by Fader et al and fit it using maximum likelihood over the first 100 days of the simulation. Then we use it to evaluate the probability of being inactive for each client in the last 100 days, given their trading patterns. To check that the model is well calibrated, we plot the expected attrition rate from the model against the simulated one in the test set (last 100 days) in the following figure:\n\n\n\nFigure 4:Expected attrition rate computed by the model against the actual attrition rate from the simulation in the test set (the last 100 days of the simulation). The good match is an indication of a good calibration.\n\nThe figure shows a good agreement between model and simulation in the test set, implying that the model is reasonably well estimated. We further test the quality of the model by computing a confusion matrix that compares the prediction of the model: inactive if the probability of attrition is higher than 50%, active otherwise; against the hidden activity labels we have generated in the simulation, where clients become inactive when average hit rates are lower than 10% for the last 10 days. This is evaluated for each day and client in the test set (5000 evaluations, resulting from 50 clients over 100 days).\n\nThe results indicate that the model is able to discriminate well between active and inactive clients across all thresholds, as evidenced by a high AUC of 0.9884. At the specific threshold of 50%, overall accuracy is very high (98.48%) and precision is perfect (100%), meaning that every client flagged as “inactive” truly was inactive. However, recall is 88.03%, which implies that the model misses some truly inactive clients (76 false negatives vs. 559 true positives). In practice, this means the model is conservative in raising alerts: it waits for clear evidence of non-trading before flagging a client, avoiding false positives entirely but potentially reacting slightly late. Depending on the cost a dealer might incur in missing early signs of disengagement, it may be worth exploring a lower threshold to catch more potential churn cases earlier, even if that introduces some false positives.\n\nFinally, let us check the behavior of the model for individual clients over the test set, with selected clients shown in the following figure:\n\n\n\nFigure 5:Trading behavior and activity for selected clients, against the probability of being inactive (attrition risk) computed by the model.\n\nThe visualizations of Clients 3, 8, 9, and 30 illustrate how the model responds to different trading behaviors over time with its attrition probability signal.\n\nClient 3 remains consistently active throughout the period. Accordingly, the model keeps the attrition probability near zero, reflecting high confidence in continued engagement.\n\nClient 8 begins active but stops trading near the end of the test period. The model gradually increases its attrition probability, reaching high levels just as the client becomes inactive. This indicates that the model correctly detects disengagement with a slight delay, requiring some sustained evidence of inactivity before triggering an alert.\n\nClient 9 becomes inactive early (around day 15), stopping trading entirely. The attrition probability reacts sharply, climbing quickly toward 1 and staying there for the rest of the period. This is an ideal response, showing the model can detect early and persistent disengagement with high confidence.\n\nClient 30 shows intermittent trading throughout, with a notable gap around day 60. During this gap, the model’s probability of inactivity rises significantly, but it drops again once trading resumes. This demonstrates the model’s sensitivity to recent inactivity, while also its ability to recover when trading behavior picks up again—avoiding a false alert.\n\nOverall, the model shows consistent and interpretable behavior: it remains quiet when trading is stable, rises decisively when disengagement is clear, and adjusts adaptively when trading resumes after a gap.","type":"content","url":"/markdown/rfq-models#simulation-of-attrition-risk","position":13},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets","lvl3":"Abnormal client behavior","lvl2":"Generative models for the request for quote activity"},"type":"lvl3","url":"/markdown/rfq-models#abnormal-client-behavior","position":14},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets","lvl3":"Abnormal client behavior","lvl2":"Generative models for the request for quote activity"},"content":"In the attrition risk sector we introduced a model for the expected trading activity of a segment of clients, using a Beta distribution:f(p |\\alpha, \\beta) = \\frac{p^{\\alpha - 1}(1 - p)^{\\beta - 1}}{B(\\alpha, \\beta)} \\quad \\text{for } 0 \\leq p \\leq 1, \\, \\alpha, \\beta > 0\n\nThis model can be easily adapted to work as an anomaly detector for clients that suddenly change their pattern of trading behavior with respect to their peers. The attrition risk model in that sense is a specific instance of anomaly detection, in which we are testing the normal client behavior hypothesis against one in which the client has completely stopped trading. A less restrictive hypothesis is that the client has switched her pattern of activity to a different regime with a higher probability for tail scenarios of activity, i.e. the client is either trading much more often or much less often than expected. We model such trading scenario by using a mixture of two beta distributions:f(p |\\alpha, \\beta) = q_g \\frac{p^{\\alpha_g - 1}(1 - p)^{\\beta_g - 1}}{B(\\alpha_g, \\beta_g)}  + (1-q_g) \\frac{p^{\\alpha_b - 1}(1 - p)^{\\beta_b - 1}}{B(\\alpha_b, \\beta_b)}\n\nIn order to ensure we capture tail anomalies in both sides, we force that both Beta distributions have the same mean: \\frac{\\alpha_g}{\\alpha_g + \\beta_g} = \\frac{\\alpha_b}{\\alpha_b+\\alpha_g}.\nThis is an instance of a Good and Bad Data Model for anomaly detection, see \n\nSilvia, 2006, which we also briefly introduced in the \n\nBayesian Theory Chapter. In this generative model, a trading observation can originate from either the good distribution — representing the expected behavior of a client segment — or from the bad distribution, which is fitted to capture tail anomalies such as drops in activity, attrition, or bursts in trading activity. Anomaly detection involves inferring the segment (good or bad) based on recent observations D over a window of size n for a specific client. Following the attrition model, these observations are represented as a binary indicator series: a value of 1 if the client traded on a given day, and 0 otherwise. Using Bayes’ theorem:p({\\rm good}|D) = \\frac{p(D|{\\rm good}) q_g}{p(D|{\\rm good}) q_g + p(D|{\\rm bad}) (1-q_g)}\n\nwith:p(D|{\\rm good}) = \\int dp \\frac{p^{\\alpha_g - 1}(1 - p)^{\\beta_g - 1}}{B(\\alpha_g, \\beta_g)} p^x (1-p)^{n-x} = \\frac{B(\\alpha_g + x, \\beta_g + n - x)}{B(\\alpha_g, \\beta_g)}p(D|{\\rm bad}) = \\int dp \\frac{p^{\\alpha_b - 1}(1 - p)^{\\beta_b - 1}}{B(\\alpha_b, \\beta_b)} p^x (1-p)^{n-x} = \\frac{B(\\alpha_b + x, \\beta_b + n - x)}{B(\\alpha_b, \\beta_b)}\n\nwhere again x is the number of days that the client has traded in the window of size n, which is a sufficient statistic for this problem. q_g is the prior probability that the client might behave normally, although in practical setups the full model is estimated over a training set using maximum likelihood, from which the parameters \\alpha_g, \\beta_g, \\alpha_b, \\beta_b, q_g are estimated. Another possibility is to restrict the estimation to the coefficients of the Beta distribution and input q_g from pure prior business knowledge. Of course, as usual, the model can be estimated using a full Bayesian approach.","type":"content","url":"/markdown/rfq-models#abnormal-client-behavior","position":15},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets","lvl4":"Simulation of client abnormal behavior","lvl3":"Abnormal client behavior","lvl2":"Generative models for the request for quote activity"},"type":"lvl4","url":"/markdown/rfq-models#simulation-of-client-abnormal-behavior","position":16},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets","lvl4":"Simulation of client abnormal behavior","lvl3":"Abnormal client behavior","lvl2":"Generative models for the request for quote activity"},"content":"We extend the simulation from the attrition risk sector adding an extra mechanism, namely clients who are quoted too generously by the dealer will boost their RfQ request rate until the dealer quotes a worse enough price. In our simulation, clients boost their activity when prices quoted are 35% cheaper than their reservation prices, and return to normality when the dealer quotes a price which are approximately 50% more expensive than their reservation rates. When they boost their activity, they increase their RfQs rates ten-fold. We simulate 50 clients over 200 days.\n\nIn the following pictures we see the resulting dynamics. The first 25 days show a different dynamics until the dealer learns enough about the distribution of reservation prices of clients. Since it has a very conservative estimation of the value of the financial asset, at the beginning we see a large number of clients receiving quotes below $35% of their reservation prices and therefore boosting their activity. After 25 days the rate of boosted clients stabilizes in a smaller proportion. The first plot shows the number of clients boosted compared to the number of active clients. The second one shows for each individual client the periods where they are boosting their quoting activity.\n\n\n\nFigure 6:Number of clients who at a given time are boosting their rates of requesting RfQs ten-fold vs the number of client actives. Clients become boosted when they receive prices from the dealer that are 35% cheaper than their reservation prices, and revert back to normal when they receive quotes worse than 50% of their reservation price. As in the previous section, they become inactive if their hit rate with the dealer is below 10% over the last 10 days.\n\n\n\nFigure 7:Boosted clients over the simulation period. Each row represents a different client from the 50 ones simulated. Given the initial prior from the dealer regarding the reservation prices, which is too conservative, at the beginning of the simulation there is higher proportion of clients boosted, stabilizing after 25 days approximately.\n\nWe estimate the model using the first 100 days of simulation, although we exclude the first 25 days to ensure the training data only uses a stable regime. The following plot shows the two estimated Beta density functions. Here, q_g = 89\\%, which supports our hypothesis that the bad data density captures anomalies. This is also seen in the plot: on the one hand, the good data density captures the normal trading behavior concentrated around a region of probability of requesting RfQs, which actually becomes essentially Gaussian, given the high values of \\alpha_g and \\beta_g; this is not the case for the bad data density, which takes a U-shape where most of the probability is concentrated around the extreme values. This reflects the two patterns of abnormal behavior present in our simulation, namely inactivity and boosted activity.\n\n\n\nFigure 8:Probability densities for the god and bad components of the model. The good data density is highly concentrated around a range of trading probabilities, becoming essentially a Gaussian distribution. The bad data density, on the other hand, places most of its density in the extreme values of very high and very low trading activity. The probability q_g equals 89% when fitted to the data, reflecting that bad data captures anomalies.\n\nWith the model estimated, we use a 10 days window of days to compute the probability of abnormal client behavior over the second half of the dataset. This means that we only compute results for the last 90 days, since 10 days are needed to gather sufficient data. Bear in mind that this is not necessary in a Bayesian paradigm, since we could perfectly get estimations for any window, including zero data --defaulting to the prior probability of the hypotheses q_g.  We choose a 50% threshold on P({\\rm good} |D) to trigger alerts. Over a dataset of 50 clients and 90 days tested, i.e. 4500 points, the model produced 3,811 true negatives (correctly identifying normal cases) and 545 true positives (correctly identifying abnormal cases). It made 75 false positives (normal cases incorrectly flagged as abnormal) and 69 false negatives (abnormal cases missed). Overall, this means the global abnormality rule (inactive OR boosted) shows high specificity (≈ 98% of normal cases correctly classified) and strong sensitivity (≈ 89% of abnormal cases detected), with a relatively small number of errors on both sides. Therefore, the approach seems reliable at detecting abnormal clients while only rarely misclassifying normal ones.\n\n\n\nFigure 9:Selected clients that showcase the different types of client behavior and how the model reacts to them.\n\nIn the following figure we can see the predictions for specific clients showing different patterns of behavior:\n\nClient 30: An example of a client with no abnormal trading behavior. The model correctly reflects this, keeping probabilities P({\\rm good} |D) mostly above 80% and not triggering any alerts.\n\nClient 31: Shows a period of boosted activity that the model captures after a few days, once sufficient evidence accumulates. When boosting ends, the probability quickly returns to high values after the first non-trading day, a pattern consistent with normality rather than extreme regimes. This illustrates the model’s asymmetry: it requires stronger evidence to reject normal trading (continuous activity can still occur under normal conditions) but reverts quickly once activity normalizes.\n\nClient 35: Becomes inactive during the simulation. After eight consecutive days without trading, the model rejects the normal trading hypothesis and flags the client. As with Client 31, strong and sustained evidence (in this case, consecutive inactivity) is needed before the model issues an alert.\n\nClient 37: Provides an instance of a false positive. A long, naturally occurring stretch of continuous activity was incorrectly flagged as abnormal. Since the model tends to produce confident abnormality probabilities quickly, one way to reduce such false positives—without significantly increasing false negatives—would be to lower the classification threshold.\n\nIn conclusion, despite its simplicity, the Good vs. Bad Data model performs well in identifying abnormal clients in the simulation, both those with unusually high trading activity and those with unusually low activity. For the latter, it performs on par with the attrition risk model presented earlier. Moreover, the framework could be extended into a multi-label classifier by comparing trading probabilities against the mean of the good-data cluster, issuing attrition risk alerts for unusually low activity and boosted activity alerts for the opposite case.","type":"content","url":"/markdown/rfq-models#simulation-of-client-abnormal-behavior","position":17},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets","lvl2":"A Generative model for RfQs in negotiation"},"type":"lvl2","url":"/markdown/rfq-models#a-generative-model-for-rfqs-in-negotiation","position":18},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets","lvl2":"A Generative model for RfQs in negotiation"},"content":"","type":"content","url":"/markdown/rfq-models#a-generative-model-for-rfqs-in-negotiation","position":19},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets","lvl2":"Causal interventions"},"type":"lvl2","url":"/markdown/rfq-models#causal-interventions","position":20},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets","lvl2":"Causal interventions"},"content":"","type":"content","url":"/markdown/rfq-models#causal-interventions","position":21},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets","lvl3":"Optimal pricing","lvl2":"Causal interventions"},"type":"lvl3","url":"/markdown/rfq-models#optimal-pricing","position":22},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets","lvl3":"Optimal pricing","lvl2":"Causal interventions"},"content":"","type":"content","url":"/markdown/rfq-models#optimal-pricing","position":23},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets","lvl3":"Axe matcher","lvl2":"Causal interventions"},"type":"lvl3","url":"/markdown/rfq-models#axe-matcher","position":24},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets","lvl3":"Axe matcher","lvl2":"Causal interventions"},"content":"","type":"content","url":"/markdown/rfq-models#axe-matcher","position":25},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets","lvl2":"Exercises"},"type":"lvl2","url":"/markdown/rfq-models#exercises","position":26},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets","lvl2":"Exercises"},"content":"Prove the identity P(a = 1|D = 10100, p, \\theta) = P(a = 1| D = 01100, p, \\theta) by explicitly working out the second term in the equality.","type":"content","url":"/markdown/rfq-models#exercises","position":27},{"hierarchy":{"lvl1":"Stochastic Calculus"},"type":"lvl1","url":"/markdown/stochastic-calculus","position":0},{"hierarchy":{"lvl1":"Stochastic Calculus"},"content":"In this chapter we provide an introduction to Stochastic Calculus: a mathematical tool useful to build models of  systems that have a relevant degree of randomness in their evolution, and such randomness is a key determinant of the properties we want to model. The latter is important: the use of Stochastic Calculus complicates the analysis of a model. It is the role of the modeller to assess initially if a deterministic model is sufficient, say because we are just interested in average properties for which randomness averages out and can be neglected. Stochastic Calculus is useful when randomness might generate a variety of outcomes, so we want to study trajectories as scenarios weighted by probability, i.e. probability distributions.\n\nWe start the chapter introducing the study of dynamical systems, focusing initially in those deterministic models that could be a sufficient starting point for the modeller. They are also usually one part of the stochastic model, which as we will see is usually decomposed in a deterministic plus a stochastic dynamics. Then we introduce the Wiener process as a building block to build stochastic models of systems with continuous dynamics. From that, we move into stochastic differential equations as well as a few useful tools to analyze them and solve the probability distributions of their trajectories. There are not many tractable stochastic differential equations, i.e. those whose distribution can be derived in closed-form. We introduce the most typical cases when it is. Finally, we close the chapter introducing jump processes, a second useful tool to model stochastic systems, in this case those that show discontinuities in their dynamics.","type":"content","url":"/markdown/stochastic-calculus","position":1},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl2":"Modelling dynamical systems"},"type":"lvl2","url":"/markdown/stochastic-calculus#modelling-dynamical-systems","position":2},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl2":"Modelling dynamical systems"},"content":"We model a dynamical system a a set of variables that follow a time dynamics lawy = f(t,{X_t})\n\nwhere \\{X_t\\} is a set of external factors that influence the trajectory of the system.\n\nThe trajectory might be difficult to derive, and sometimes it is easier to derive the dynamics in small steps. Considering the case y = f(t)\\frac{dy}{dt} = g(t)\n\nwhere g(t) = \\frac{df}{dt} and then integrate ity(t) = y(0) + \\int_0^t g(t) dt\n\nIn the more general case where there are external factors with their own time dynamics:dy = \\frac{\\partial f}{\\partial t}dt + \\sum_{n=1}^N \\frac{\\partial f}{\\partial X_{i,t}} d X_{i,t}\n\nModelization can be done empirically or using fundamental laws if available, we will see examples in next section\n\nAs mentioned, the dynamics of the system might not be deterministic. In this case we need to model the source of randomness in the dynamical system, for which we use  stochastic differential equations (SDE) of the form:d X_t = \\mu(X_t, t) dt + \\sigma (X_t, t) d W_t\n\nwhere d W_t is the Wiener process (Brownian motion) that will be introduced later.","type":"content","url":"/markdown/stochastic-calculus#modelling-dynamical-systems","position":3},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl2":"Deterministic dynamical systems"},"type":"lvl2","url":"/markdown/stochastic-calculus#deterministic-dynamical-systems","position":4},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl2":"Deterministic dynamical systems"},"content":"We will introduce the topic by looking at some relevant deterministic differential equations that come across in the modelization of dynamical systems.\n\nExample: Newton’s Laws\n\nA first example comes from Newton’s laws applied to the dynamics of particles in a gravitational field. The second law of Newton states:F = m a\n\nWe consider an object of mass m drop at initial height h(0) with speed zero. It is only subjected to the gravitational force:\nF = m g where g is the gravitational constant close to Earth’s surface: g \\approx 9.81 m/s^2. The dynamics for the speed is therefore a first order differential equation with constant coefficients:m \\frac{d v}{d t} = - m g \\rightarrow \\frac{d v}{d t} = - g\n\nwhere we have taken into account that the force is exerted in the opposite direction to which the height axis grows. This equation is simple to integrate:v(t) = v(0) - \\int_0^t g dt = v(0) + g t = - gt\n\nsince in our case, v(0) = 0. The dynamics of the position reads:\\frac{d h}{dt} = v(t) = - g t\n\nwhich can be again be easily integrated:h(t) = h(0) - \\int_0^t g t' dt' = h(0) - g \\frac{t^2}{2}\n\nThese are particular examples of a general family of differential\nequations:\\frac{d^n y}{d t^n} = f(t)\n\nwhich can be simply integrated step by step by defining intermediate variables, e.g.:z \\equiv \\frac{d^{n-1} y}{d t^{n-1}} \\rightarrow \\frac{d z}{dt} = f(t) \\rightarrow z(t) = z(0) + \\int_{t_0}^{t_1} f(t) dt\n\nExample: simple inflation targeting model\n\nA simple model of the dynamics of inflation takes into account Central Bank interventions using monetary policy in order to adjust inflation to a given target. When inflation deviates from the target, monetary policy\nis used to bring it back to the target. This can be modelled using the following differential equation:\\frac{d \\pi_t}{d t} = \\theta (\\hat{\\pi} - \\pi_t)\n\nwhere \\pi is the current level of inflation, \\hat{\\pi} is the inflation target, and \\theta > 0 is a constant that models the speed at which monetary\npolicy takes effect. This is an example of a mean - reverting equation, a process whose dynamics is controlled by an external force that drives\nback the system towards a stable level (the mean).\n\nThis equation is a particular example of a general family of differential equations which we call separable, with the form\\frac{d y}{d t} = g(y)f(t)\n\nThey can be solved by exploiting the\nseparability:\\int_{y_0}^{y_t} \\frac{dy}{g(y)} = \\int_{t_0}^t f(t') dt'\n\nComing back to the particular example of the mean reverting equation:\\begin{aligned}\n\\int_{\\pi_{t_0}}^{\\pi_t}\\frac{d \\pi}{\\hat{\\pi} - \\pi} = \\theta \\int_{t_0}^t dt' \\rightarrow -\\log(\\frac{\\hat{\\pi} - \\pi_t}{\\hat{\\pi} - \\pi_{t_0}}) = \\theta (t - t_0) \\\\\n\\pi_t = \\hat{\\pi} + (\\pi_{t_0}-\\hat{\\pi}) e^{-\\theta(t-t_0)}\n\\end{aligned}\n\nThe solution shows as, as expected, a trajectory for the\ninflation that decays to the target. The time scale for decay is independent of the size of the initial gap \\pi_{t_0} -\\hat{\\pi}, depending exclusively on \\theta, which is called the mean reversion speed. In analogy to the physical process of radioactive decay, which\ncan be described by a similar equation, we can define a half-life \\tau, or time in which the gap is closed by half, as: \\tau = \\frac{\\log }{\\theta} Alternatively, 1/\\theta is the time it\ntakes to close the gap to 1/e \\approx 0.369\n\nExample: Free fall with friction\n\nWe can again resort to Newton’s laws to model the dynamics of an object in free fall in the atmosphere, where the air introduces an opposite resistance to the action of gravity. A simple model of such friction is\na force proportional to the speed of the object, namely F = \\eta v, where \\eta is a coefficient that depends on the properties of the atmosphere. Using Newton’s law, we have now:\\begin{aligned}\nm \\frac{dv} {dt} = mg - \\eta v \\\\\n\\frac{d h} {dt} = v\n\\end{aligned}\n\nWhich is a system of two equations. We can combine it to\nget a differential equation on the particle’s trajectory:m \\frac{d^2 h}{d t^2} + \\eta \\frac{dh}{dt} = mg\n\nwhich is, particularly, a second order non-homogeneous ODE with constant coefficients. A general non-homogeneous linear ODE with constant coefficients reads:a_n \\frac{d^n y}{dt^n} + a_{n-1} \\frac{d^{n-1} y}{dt^{n-1}} + ... + a_0 y = f(t)\n\nTo solve this equation first we find a general solution for the homogeneous equation, and then a particular solution for the non-homogeneous one.\n\nFor the general solution of the homogeneous we can actually exploit its connection to a system of first order differential equations, an example of which we saw when introducing the problem of a particle in free-fall with friction. In general, we can define auxiliary variables y_i = \\frac{d^i y}{dt^i} for i < n. We get the following system:\\begin{aligned}\n \\frac{d y_{n-1}}{dt} + \\frac{a_{n-1}}{a_n} y_{n-1} + ... + \\frac{a_0}{a_n} y_0 = 0 \\nonumber \\\\\n\\frac{d y_{n-2}}{dt} - y_{n-1} = 0 \\nonumber \\\\\n... \\nonumber \\\\\n\\frac{d y_0}{dt} - y_1 = 0\n\\end{aligned}\n\nwhich has the structure:\\frac{d {\\bf y}}{dt} + A {\\bf y} = 0\n\nwhere {\\bf y} = (y_0, ..., y_{n-1}) and A is the matrix form with the coefficients of the system of equations above. The general solution to this equation is:{\\bf y} =  e^{-A t} {\\bf y_0}\n\nThis is the formal solution, but to get a workable solution we need to diagonalize the matrix A, i.e. finding A = C \\Lambda C^{-1} where \\Lambda is a diagonal matrix with the eigenvalues of A, and C has its eigenvectors \\vec{w} as columns. One can then project the solution in the space of eigenvectors:{\\bf y_t} =  C C^{-1} e^{-A t} C C^{-1} {\\bf y_0} = C e^{-\\Lambda t} C^{-1} {\\bf y_0}\n\nThis is equivalent to writing:{\\bf y_t} = \\sum_i w_i e^{-\\lambda_i t}  {\\bf c_i}\n\nwhere {\\bf c_i} is the ith eigenvector with eigenvalue \\lambda_i and w_i = (C^{-1} {\\bf y_0})_i is the projection of the initial state into\nthe eigenvectors basis. In such basis, the dynamics is simple, driven by the exponential of its eigenvalue: e^{-\\lambda_i t}\n\nComing back to the particle in free-fall, we can obtain the general solution of the homogeneous equation, namely:\\frac{d^2h}{dt} + \\frac{\\eta}{m} \\frac{dh}{dt} = 0\n\nby applying directly the ansatz e^{\\lambda t}:\\lambda^2 + \\frac{\\eta}{m} \\lambda = 0\n\nwhich is equivalent to finding the eigenvalues of the matrix of the system of equations. This equation has two eigenvalues, \\lambda = 0 and \\lambda = -\\frac{\\eta}{m}, so the general solution reads:h_t = C_0 + C_1 e^{-\\frac{\\eta}{m}t}\n\nA particular solution is a linear function h_t = \\frac{m g}{\\eta} t. Therefore, the solution for the full equation is:h_t = \\frac{m g}{\\eta} t + C_0 + C_1 e^{-\\frac{\\eta}{m}t}\n\nThe speed is therefore:v_t = \\frac{m g}{\\eta} - C_1 \\frac{\\eta}{m} e^{-\\frac{\\eta}{m}t}\n\nFinally, by imposing initial conditions h_0 and v_0 = 0, then C_1 = \\frac{m^2 g}{\\eta^2} and C_0 = h_0 - C_1, so:h_t = h_0 + \\frac{m g}{eta} t +  \\frac{m^2 g}{eta^2} (e^{-\\frac{\\eta}{m}t} - 1)\n\nNotice that the speed of the particle is then:v_t = \\frac{m g}{\\eta} -\\frac{m g}{\\eta} h_0 e^{-\\frac{\\eta}{m}t}\n\nwhich tends to a constant when t \\rightarrow \\infty,\nv_\\infty = \\frac{m g}{\\eta}, which is called the terminal velocity. At this speed, the friction force equates the gravitational pull and the\nobject no longer accelerates, therefore reaching a constant velocity. Another interesting observation is that the speed equation has also the\nform of a mean reverting equation, in this case the mean being the terminal velocity, and the time-scale for mean reversion proportional to m/\\eta.","type":"content","url":"/markdown/stochastic-calculus#deterministic-dynamical-systems","position":5},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Numerical solutions of dynamical systems","lvl2":"Deterministic dynamical systems"},"type":"lvl3","url":"/markdown/stochastic-calculus#numerical-solutions-of-dynamical-systems","position":6},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Numerical solutions of dynamical systems","lvl2":"Deterministic dynamical systems"},"content":"When closed form solutions are not available, one can resort to numerical schemes to simulate dynamical systems. The most popular one is the Euler method. First we discretize the time domain using a grid\nt_i = t_0 + \\Delta * i, i = 0, .., N, where\n\\Delta = \\frac{t_1 - t_0}{N}. The differentials can be discretized over the grid as follows:\\begin{aligned}\n\\frac{dy}{dt} \\rightarrow \\frac{y_{t_{i+1}}-y_{t_i}}{\\Delta} \\\\\n\\frac{d^2y}{dt^2} = \\frac{d}{dt} \\frac{dy}{dt} \\rightarrow \\frac{y_{t_{i+1}}-2 y_{t_i} + y_{t_{i-1}}}{\\Delta^2} \\\\\n...\n\\end{aligned}\n\nNotice that there is some ambiguity on where to evaluate\nthe derivatives in the grid. For instance the second order one could also be evaluated in a \"forward\" scheme instead of the \"central\"\nused:\\frac{d^2y}{dt^2} \\rightarrow \\frac{y_{t_{i+2}}-2 y_{t_{i+1}} + y_{t_i}}{\\Delta^2}\n\nThe same applies to the evaluation of functions in the equation. There are multiple schemes to do it. The most simple one is the forward method, where we evaluate functions at the initial point of the grid\nstep, y(t) \\rightarrow y_{t_i}. In the backward case we evaluate it at the final point of the step, y(t) \\rightarrow y_{y_{i+1}}. There other\npossibilities, for example the Crank - Nicolson method uses an average of forward and backward:\ny(t) \\rightarrow \\frac{1}{2} (y_{t_i} + y_{y_{i+1}}). In general all these methods with the exception of the forward one yield implicit discrete equations, which require more work to solve, but have usually\nbetter convergence properties (speed, accuracy).\n\nLet us see an example. Previously, we introduced the following model for inflation targeting:\\frac{d \\pi_t}{d t} = \\theta (\\hat{\\pi} - \\pi_t)\n\nA forward Euler scheme yields the following discrete equation:\\pi_{t_{i+1}} = \\pi_{t_i} + \\Delta \\theta (\\hat{\\pi} - \\pi_{t_i})\n\nThis equation is ready for simulation. Starting with an initial condition \\pi_{t_{i_0}} and a grid size \\Delta, we can iteratively evaluate the trajectory of inflation.\n\nThe backward Euler scheme gives the following implicit equation:\\pi_{t_{i+1}} = \\pi_{t_i} + \\Delta \\theta (\\hat{\\pi} - \\pi_{t_{i+1}} )\n\nIn order to simulate it, we need to first solve for \\pi_{t_{i+1}}. In this case this is simple:\\pi_{t_{i+1}} = \\frac{\\pi_{t_i} + \\Delta \\theta \\hat{\\pi}}{1 - \\Delta \\theta }\n\nWe leave as an exercise the solution of the Crank Nicholson scheme for this equation.\n\nLet us now solve these equations analytically and numerically for particular values, using the forward and backward schemes. First we choose a set of parameters and grid size where both schemes yield accurate results, matching the analytical solution:\n\n\n\nFigure 1:Solution of the inflation targetting differential equation  comparing the analytical solution with the forward and backward schemes. We choose a set of parameters and grid size where both methods approximate well the analytical solution: \\theta = 0.1, \\Delta = 0.1, \\hat{\\pi} = 2.0, \\pi_0 = 5.0\n\nLet us now increase the grid size and choose another set of parameters to make the numerical approximation more challenging. We can see how the forward scheme deviates substantially from the analytical solution, whereas the backward scheme remains more robust:\n\n\n\nFigure 2:Solution of the inflation targetting differential equation  comparing the analytical solution with the forward and backward schemes. We choose a set of parameters and grid size where a numerical approximation is more challenging, to show the advantages of the backward scheme: \\theta = 10, \\Delta = 0.1, \\hat{\\pi} = 2.0, \\pi_0 = 5.0","type":"content","url":"/markdown/stochastic-calculus#numerical-solutions-of-dynamical-systems","position":7},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl2":"The Wiener Process"},"type":"lvl2","url":"/markdown/stochastic-calculus#the-wiener-process","position":8},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl2":"The Wiener Process"},"content":"","type":"content","url":"/markdown/stochastic-calculus#the-wiener-process","position":9},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Definition","lvl2":"The Wiener Process"},"type":"lvl3","url":"/markdown/stochastic-calculus#definition","position":10},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Definition","lvl2":"The Wiener Process"},"content":"So far we have discussed models where we neglect any uncertainty in the trajectories of the dynamical systems we are modelling. However, in many situations, randomness is a dominant characteristic of the process and\nwe need tools to model such stochastic systems.\n\nThe Wiener process is the main building block when modelling stochastic dynamical systems which exhibit continuous trajectories. In order to\nmodel discrete jumps we will introduce later another building block, the Poisson process.\n\nMany introductory books motivate the Wiener process as the continuous limit of a random walk, e.g. a discrete dynamical process in which each step is decided by a random binary variable Z_t = {1, -1} with\np = 0.5, i.e. analogous to flipping an unbiased coin. If we know that the process at time t is X_t, then X_{t+1} = X_t + Z_t.\n\nHere, we will instead introduce the Wiener process as a fundamental building block for the modelization of stochastic processes, i.e. a tool\nthat will allow us to capture certain observed or proposed behaviours of those systems in a model from which we can draw inferences. In such way,\nwe define a Wiener process as a type of stochastic process, i.e. an (in principle infinite) sequence of random variables W_t indexed by time\nt, with the following particular properties:\n\nW_0 = 0\n\nW_{t+s} - W_s \\sim N(0, s) where N(0,s) is a Gaussian\ndistribution with zero mean and variance s, i.e. the increments of\nthe Wiener process are Gaussian\n\nW_t has independent increments, meaning that any\nW_{t_1}-W_{t_2}, W_{t_3} - W_{t_4} where t_1 < t_2 < t_3 < t_4\nare independent\n\nW_t has continuous trajectories","type":"content","url":"/markdown/stochastic-calculus#definition","position":11},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Connection to Gaussian Processes","lvl2":"The Wiener Process"},"type":"lvl3","url":"/markdown/stochastic-calculus#connection-to-gaussian-processes","position":12},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Connection to Gaussian Processes","lvl2":"The Wiener Process"},"content":"The specific properties of the Wiener process make it part of the family of Gaussian processes, which are stochastic processes with the property that any finite set of them, X_{t_1}, ..., X_{t_n} follow a joined\nmultivariate Gaussian distribution. The Gaussian process can be described then by a mean function \\mu_t = \\mathbb{E}[X_t] and a covariance function k(t_1, t_2) = \\textrm{cov}[X_{t_1}, X_{t_2}] that is usually referred as the kernel function of the Gaussian process.\n\nThe index for the stochastic process does not need to refer to time, another typical indexing is space, for instance. When using time as an\nindex, we talk about Gaussian Processes for time-series modelling. This is the case for the Wiener process, which is a Gaussian process with\nmean \\mu_t = 0 and kernel:\\begin{aligned}\nk(t_1, t_2) = \\textrm{cov}[W_{t_1}, W_{t_2}] = \\mathbb{E}[W_{t_1} W_{t_2}] = \\nonumber \\\\ \\left(\\mathbb{E}[(W_{t_1} - W_{t_2}) W_{t_2}] + \\mathbb{E}[W_{t_2}^2]\\right)1_{t_1 > t_2} +  \\left(\\mathbb{E}[W_{t_1}(W_{t_2} - W_{t_1})] + \\mathbb{E}[W_{t_1}^2]\\right)1_{t_1 \\leq t_2} = \\nonumber \\\\\nt_2 1_{t_1 > t_2} + t_1 1_{t_1 \\leq t_2} = min(t_1,t_2)\n\\end{aligned}\n\nIn which sense it is useful to introduce this relationship? The theory of Gaussian processes has been pushed in the last years within the Machine Learning community, and there are multiple tools available for building, training and making inferences with these\nmodels. Such toolkit can be useful when building models for dynamical systems using stochastic processes.","type":"content","url":"/markdown/stochastic-calculus#connection-to-gaussian-processes","position":13},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Filtrations and the Martingale Property","lvl2":"The Wiener Process"},"type":"lvl3","url":"/markdown/stochastic-calculus#filtrations-and-the-martingale-property","position":14},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Filtrations and the Martingale Property","lvl2":"The Wiener Process"},"content":"The Wiener process satisfies the Martingale property, meaning that if we have a series of observations of the process W_{t_1}, W_{t_2}, ... W_{t_n} then the expected value of an observation t_{n+1} > t_n > ... > t_1 conditioned to the previous observations only depends on the last observation:\\mathbb{E}[W_{t_{n+1}}|W_{t_1}, W_{t_2}, ... W_{t_n}] = W_{t_n}\n\ni.e., the information from previous observations is irrelevant. We can generalize the martingale property by introducing the notion of filtration F_t at\ntime t, which contains all the information set available until time t. We can rewrite the Martingale property using the filtration as:\\mathbb{E}[W_{t_{n+1}}|F_{t_n}] = W_{t_n}\n\nWe can prove the Martingale property using the defining properties of the Wiener process:\\begin{aligned}\n\\mathbb{E}[W_{t_{n+1}}|F_{t_n}] = \\mathbb{E}[W_{t_{n+1}} - W_{t_n} + W_{t_n}|F_{t_n}] = \\nonumber \\\\\n \\mathbb{E}[W_{t_{n+1}} - W_{t_n}|F_{t_n}]+ \\mathbb{E}[W_{t_n}|F_{t_n}] = W_{t_n}\n\\end{aligned}\n\nwhere we have used that the increments of the Wiener\nprocess have zero mean.","type":"content","url":"/markdown/stochastic-calculus#filtrations-and-the-martingale-property","position":15},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"The Tower Law","lvl2":"The Wiener Process"},"type":"lvl3","url":"/markdown/stochastic-calculus#the-tower-law","position":16},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"The Tower Law","lvl2":"The Wiener Process"},"content":"A useful property that exploits the concept of filtration in many applications is the so-called Law of Iterated Expectations or Tower Law. It simply states that for any random variable Y, if t_m > t_n:\\mathbb{E}[\\mathbb{E}[Y|F_{t_m}]|F_{t_n}] = \\mathbb{E}[Y|F_{t_n}]\n\nThis is a useful property to compute expectations using a divide and conquer philosophy: maybe the\nfull expectation \\mathbb{E}[Y|F_{t_n}] is not obviously tractable, but by conditioning it at intermediate filtrations\n\\mathbb{E}[\\mathbb{E}[\\mathbb{E}[...\\mathbb{E}[Y|F_{t_m}] ... |F_{t_{n+2}}|F_{t_{n+1}}|F_{t_n}] we can\nwork out the solution. We will see examples later on.","type":"content","url":"/markdown/stochastic-calculus#the-tower-law","position":17},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Multivariate Wiener process","lvl2":"The Wiener Process"},"type":"lvl3","url":"/markdown/stochastic-calculus#multivariate-wiener-process","position":18},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Multivariate Wiener process","lvl2":"The Wiener Process"},"content":"We can construct a multivariate Wiener process as a vector\n{\\bf W}_t= (W_{1t}, W_{2t}, ..., W_{Nt}) with the following properties:\n\n{\\bf W}_t = {\\bf 0}\n\n{\\bf W}_{t+s} - {\\bf W_t}_s \\sim N(0, \\Sigma s) where \\Sigma is a NxN correlation matrix. This means that the increments of the components of the multivariate Wiener process might be correlated\n\n{\\bf W}_t has independent increments, meaning that any {\\bf W}_{t_1}-{\\bf W}_{t_2}, {\\bf W}_{t_3} - {\\bf W}_{t_4} where t_1 < t_2 < t_3 < t_4 are independent\n\nEach of the components of {\\bf W}_t has continuous trajectories\n\nIto’s Lemma\nWe have seen that given the trajectory of a deterministic dynamical system, y_t = f(t, {X_t}), its behavior over a infinitesimal time-step is described by the following differential equation:dy = \\frac{\\partial f}{\\partial t}dt + \\sum_{n=1}^N \\frac{\\partial f}{\\partial X_{i,t}} d X_{i,t}\n\nwhere \\{X_t\\} is a set of deterministic external factors. But what if the external factors are Wiener processes, e.g. in the most simple case\nf(t,W_t)?. We might be tempted to just reuse the previous equation:dy = \\frac{\\partial f}{\\partial t}dt + \\frac{\\partial f}{\\partial W_t} dW_t\n\nhowever, one of the peculiarities of the Wiener process is that this equation is incomplete, since there is an additional term at this order of the expansion. There are different ways to derive this result,\ntypically starting from the discrete Wiener process (random walk), but a simple non-rigorous argument can be made by expanding the differential to second order in the Taylor series:dy = \\frac{\\partial f}{\\partial t}dt + \\frac{\\partial f}{\\partial W_t} dW_t +  \\frac{1}{2}\\frac{\\partial^2 f}{\\partial t^2}dt^2 + \\frac{1}{2} \\frac{\\partial^2 f}{\\partial W_t^2} dW_t^2 + \\frac{\\partial^2 f}{\\partial t \\partial W_t}dt dW_t\n\nand take the expected value of this differential conditional to the\nfiltration at t:\\begin{aligned}\n\\mathbb{E}[dy|F_t] = \\frac{\\partial f}{\\partial t}dt + \\frac{\\partial f}{\\partial W_t} \\mathbb{E}[dW_t|F_t] +  \\frac{1}{2}\\frac{\\partial^2 f}{\\partial t^2}dt^2 \\nonumber \\\\ + \\frac{1}{2} \\frac{\\partial^2 f}{\\partial W_t^2} \\mathbb{E}[dW_t^2|F_t] + \\frac{\\partial^2 f}{\\partial t \\partial W_t}dt \\mathbb{E}[dW_t|F_t] \n\\end{aligned}\n\nThe key for the argument is that whereas \\mathbb{E}[dW_t|F_t] = 0, we have \\mathbb{E}[dW_t^2|F_t] = dt, getting:\\mathbb{E}[dy|F_t] = \\frac{\\partial f}{\\partial t}dt + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial t^2}dt^2 + \\frac{1}{2} \\frac{\\partial^2 f}{\\partial W_t^2} dt\n\nso if we keep only terms at first order in the expansion:\\mathbb{E}[dy|F_t] = (\\frac{\\partial f}{\\partial t} + \\frac{1}{2} \\frac{\\partial^2 f}{\\partial W_t^2})dt + O(dt^2)\n\nwe have an extra term coming from the variance of the Wiener process, whose expected value is linear in dt. This motivates us to propose the following expression for the differential of a function of the Wiener\nprocess, the so-called Ito’s lemma:\\boxed{\ndy = (\\frac{\\partial f}{\\partial t} + \\frac{1}{2} \\frac{\\partial^2 f}{\\partial W_t^2})dt + \\frac{\\partial f}{\\partial W_t} dW_t \n}\n\nThe same argument can be applied for a multivariate Wiener process. Let us for example analyze the case of a function of two correlated Wiener processes, y = f(t, W_{1t}, W_{2t}). Expanding to second order\nin a Taylor series we get:\\begin{aligned}\ndy = \\frac{\\partial f}{\\partial t}dt + \\frac{\\partial f}{\\partial W_{1t}} dW_{1t} +   \\frac{\\partial f}{\\partial W_{2t}} dW_{2t} \\nonumber \\\\ + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial t^2}dt^2 + \\frac{1}{2} \\frac{\\partial^2 f}{\\partial W_{1t}^2} dW_{1t}^2 + \n\\frac{1}{2} \\frac{\\partial^2 f}{\\partial W_{2t}^2} dW_{2t}^2  \\nonumber \\\\\n+ \\frac{\\partial^2 f}{\\partial t \\partial W_{1t}}dt dW_{1t} + \n\\frac{\\partial^2 f}{\\partial t \\partial W_{2t}}dt dW_{2t} +\n\\frac{\\partial^2 f}{\\partial W_{1t} \\partial W_{2t}}d W_{1t} dW_{2t}\n\\end{aligned}\n\nNow, using the expected value argument and keeping only\nterms O(dt):\\begin{aligned}\n\\mathbb{E}[dy|F_t] = \\frac{\\partial f}{\\partial t}dt + \\frac{1}{2} \\frac{\\partial^2 f}{\\partial W_{1t}^2} dt + \n\\frac{1}{2} \\frac{\\partial^2 f}{\\partial W_{2t}^2} dt +\n\\frac{\\partial^2 f}{\\partial W_{1t} \\partial W_{2t}} \\rho_{12} dt\n\\end{aligned}\n\nwhere we have used\n\\mathbb{E}[d W_{1t} dW_{2t} |F_t] = \\rho_{12} dt as discussed in the section on multivariate Wiener processes. This motivates the expression for the two-dimensional Ito’s lemma:\\begin{aligned}\nd y = (\\frac{\\partial f}{\\partial t} + \\frac{1}{2} \\frac{\\partial^2 f}{\\partial W_{1t}^2} + \n\\frac{1}{2} \\frac{\\partial^2 f}{\\partial W_{2t}^2}  +\n\\frac{\\partial^2 f}{\\partial W_{1t} \\partial W_{2t}} \\rho_{12}) d t  \\nonumber \\\\ + \\frac{\\partial f}{\\partial W_{1t}} d W_{1t} +  \\frac{\\partial f}{\\partial W_{2t}} d W_{2t}\n\\end{aligned}\n\n###Integrals of the Wiener process\n\nA firs relevant integral of the Wiener process is:I_{1t} = \\int_0^t f(u) dW_u\n\nWhat is the distribution of I_{1t}? A simple way to deduce it is to discretize the integral and then take again the continuous limit:I_{1t} = \\lim_{\\Delta \\rightarrow 0} \\sum_{i=0}^{N-1} f(u_i) (W_{u_{i+1}} - W_{u_i})\n\nwhere \\Delta = t / N, u_i = \\Delta i. This is a sum of independent Gaussian random variables, since by definition the increments of the Wiener process are independent. Therefore its distribution is itself\nGaussian . Its mean is easily computed as:\\mathbb{E}[I_{1t}|F_0] = \\lim_{\\Delta \\rightarrow 0}  \\sum_{i=0}^{N-1} f(u_i) \\mathbb{E}[(W_{u_{i+1}} - W_{u_i})|F_0] =\n\nFor the variance, we could simply use the result that the variance of independent variables is the sum of variances, but it is instructive to compute it directly over the discrete expression:\\mathbb{E}[I_{1t}^2|F_0] = \\lim_{\\Delta \\rightarrow 0} \\sum_{i=0}^{N-1}  \\sum_{j=0}^{N-1}   f(u_i)  f(u_j) \\mathbb{E}[(W_{u_{i+1}} - W_{u_i})(W_{u_{j+1}} - W_{u_j})|F_0]\n\nThe computation of \\mathbb{E}[(W_{u_{i+1}} - W_{u_i})(W_{u_{j+1}} - W_{u_j})|F_0] needs to consider two cases. When i \\neq j the expectation is zero, since increments of the Wiener process are independent. This is no longer the\ncase of i = j, for which we have \\mathbb{E}[(W_{u_{i+1}} - W_{u_i})^2|F_0] = u_{i+1} - u_i = \\Delta. Therefore:\\mathbb{E}[(W_{u_{i+1}} - W_{u_i})(W_{u_{j+1}} - W_{u_j})|F_0] = \\delta_{i,j} \\Delta\n\nPlugging this into the previous equation:\\mathbb{E}[I_{1t}^2|F_0] = \\lim_{\\Delta \\rightarrow 0} \\Delta \\sum_{i=0}^{N-1}  f^2(u_i) = \\int_0^t f^2(u) du\n\nTherefore, the distribution of I_{1t} is given by:I_{1t} \\sim N(0, \\int_0^t du f^2(u))\n\nNotice that once we have motivated the solution by using a discretization of the equation, we can directly skip it and use the continuous version, for instance:\\begin{aligned}\n\\mathbb{E}[I_{1t}^2|F_0] = \\int_0^t \\int_0^t f(u) f(u') \\mathbb{E}[dW_u dW_{u'}] \\nonumber \\\\ = \\int_0^t \\int_0^t du du' f(u) f(u') \\delta(u-u') du = \\int_0^t f^2(u) du\n\\end{aligned}\n\nLet us now move into the second relevant integral of the\nWiener process, namely:I_{2t} = \\int_0^t du f(W_u)\n\nWhat is the distribution of this random variable? In this case, applying the discretization argument does not provide a direct simple path to figure out the distribution, since we have a sum of correlated variables (e.g. f(W_{u_i}) and f(W_{u_j}) are correlated over the common paths shared by them, say if u_i < u_j, the path below u_i). We can use Ito’s lemma to advance our comprehension:d(u f(W_u)) = u df(W_u) + f(W_u) du\n\nwhich being linear it does not have the Ito’s convexity terms. Essentially we can then use integration by parts:\\int_0^t du f(W_u) = t f(W_t) - \\int_0^t u df(W_u)\n\nUsing again Ito’s lemma, now on f(W_u):df(W_u) = \\frac{\\partial f}{\\partial W_u} dW_u + \\frac{1}{2} \\frac{\\partial^2f }{\\partial W_u^2} du\n\nwe get:\\int_0^t du f(W_u) = t f(W_t) - \\int_0^t u \\frac{\\partial f}{\\partial W_u} dW_u - \\frac{1}{2} \\int_0^t u\\frac{\\partial^2 f}{\\partial W_u^2} du\n\nAt this point we cannot proceed further in general unless we specify specific functions. For instance, the most simple non trivial case is the linear function f(W_u) = W_u. Applying this formulation:\\int_0^t W_u du = t W_t - \\int_0^t u dW_u = \\int_0^t (t - u) dW_u\n\nwhich now we can identify as a specific case of the previous integral, with f(u) = t - u. The distribution is therefore:\\int_0^t W_u du \\sim N(0, \\frac{t^3}{3})\n\nwhere we have used \\int_0^t (t-u)^2 du = \\frac{t^3}{3}\n\n###Simulation of the Wiener process\n\nWe can simulate the Wiener process using a discretization like the Euler scheme for deterministic processes. Again, defining a grid t_i = t_0 + \\Delta * i, i = 0, .., N, where \\Delta = \\frac{t_N - t_0}{N}, we can exploit directly the Wiener process properties to get:dW_t \\rightarrow W_{t_{i+1}} - W_{t_i} \\sim N(0, \\Delta)\n\nSo we can simulate numerically the Wiener process simply as:W_{t_{i+1}} =  W_{t_i} + \\sqrt{\\Delta} Z\n\nwhere Z is a standard normal distribution. The following plot simulates the Wiener process numerically for a few different paths:\n\n\n\nFigure 3:Simulation of five different paths of the the Wiener process using t_0=0 t_N=1, N = 100, W_0 = 0\n\nAlternatively, we can simulate complete paths of the Wiener process by exploiting its connection to Gaussian processes. Using a Gaussian process framework we get the following results:\n\n\n\nFigure 4:Simulation of five different paths of the the Wiener process using the same parameters, but sampling from a Gaussian process with a Wiener min(t_1, t_2) kernel\n\nMultivariate processes\n\nLet us now simulate a multivariate Wiener process of dimension N. The strategy in this case is to start simulating N independent Wiener processes and then use them to generate the correlated paths. This can\nbe done using the Cholesky decomposition of the correlation matrix \\Sigma. Since \\Sigma is symmetric and positive define, the decomposition reads:\\Sigma = L L^T\n\nwhere L is a lower triangular matrix. If we now we have a vector of N uncorrelated Wiener increments\nd \\hat{{\\bf W_t}}, we can obtain the correlated process by using L such as d {\\bf W_t} = L d \\hat{{\\bf W_t}}. We can see that this vector has the distribution of the multivariate Wiener process increment:\\begin{aligned}\n\\mathbb{E}[d {\\bf W_t} d {\\bf W_t}^T] = \\mathbb{E}[L d \\hat{{\\bf W}}_{t} d \\hat{{\\bf W}}_{t}^T L^T]  \\nonumber \\\\ = L \\mathbb{E}[d \\hat{{\\bf W}}_{t} d \\hat{{\\bf W}}_{t}^T] L = L L^T dt = \\Sigma dt\n\\end{aligned}\n\nwhere we have used \\mathbb{E}[d \\hat{{\\bf W}}_{t} d \\hat{{\\bf W}}_{t}^T] = I dt where I is the identity matrix.\n\nIt is illustrative to show the case for a two-dimensional multivariate Wiener process. The correlation matrix reads:\\begin{aligned}\n\\begin{bmatrix}\n1 & \\rho  \\\\ \\rho & 1\n\\end{bmatrix}\n\\end{aligned}\n\nThe Cholesky lower triangular matrix is simply:\\begin{aligned}\nL = \n\\begin{bmatrix}\n1 & 0  \\\\ \\rho & \\sqrt{1-\\rho^2}\n\\end{bmatrix}\n\\end{aligned}\n\nNotice that, as expected:\\begin{aligned}\nL L^T = \n\\begin{bmatrix}\n1 & 0  \\\\ \\rho & \\sqrt{1-\\rho^2}\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & \\rho  \\\\ 0 & \\sqrt{1-\\rho^2}\n\\end{bmatrix} = \n\\begin{bmatrix}\n1 & \\rho\\\\ \\rho & 1\n\\end{bmatrix}\n\\end{aligned}\n\nWe can also write the decomposition in equations:\\begin{aligned}\nd W_{1t} &=& d \\hat{W}_{1t} \\nonumber \\\\\nd W_{2t} &=& \\rho d \\hat{W}_{1t} + \\sqrt{1-\\rho^2}  d \\hat{W}_{2t}\n\\end{aligned}","type":"content","url":"/markdown/stochastic-calculus#multivariate-wiener-process","position":19},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl2":"Stochastic differential equations"},"type":"lvl2","url":"/markdown/stochastic-calculus#sde","position":20},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl2":"Stochastic differential equations"},"content":"As pointed out in the introduction to the chapter, after studying the Wiener process now we have the tools to model the dynamics of a random or stochastic process X_t using stochastic differential equations (SDEs) of the general form:d X_t = \\mu(X_t, t) dt + \\sigma(X_t, t) dW_t\n\nwhich generalizes the deterministic differential equation adding a noise term modelled using the Wiener process. Solving a SDEs means working out the distribution of X_t at arbitrary times given some initial conditions, which for arbitrary choices of \\mu(X_t, t) and \\sigma(X_t,t) can only be solved numerically, for instance using Monte-Carlo sampling techniques. In the next section we will see particular SDEs that are analytically tractable.","type":"content","url":"/markdown/stochastic-calculus#sde","position":21},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl2":"The Feynman - Kac Theorem"},"type":"lvl2","url":"/markdown/stochastic-calculus#feynman-kac","position":22},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl2":"The Feynman - Kac Theorem"},"content":"The Feynman-Kac theorem provides a link between partial differential equations and stochastic processes, specifically, between certain types of PDEs and expectations of integrals of the Wiener process. Its usefulness extends to subjects like physics and finance, where it is used to introduce statistical interpretations of solutions to deterministic systems.\n\nSpecifically, the Feynman - Kac theorem it relates the solution u(x,t) of the PDE\\frac{\\partial u}{\\partial t} + \\mu(x,t) \\frac{\\partial u}{\\partial x} + \\frac{1}{2} \\sigma^2(x,t) \\frac{\\partial^2 u}{\\partial x^2} - r(x,t)u = 0\n\nwith the boundary condition u(x,T) = f(x), to the expected valueu(x,t) = \\mathbb{E}\\left[ e^{-\\int_t^T r(X_s,s) ds} f(X_T) \\Big| X_t = x \\right]\n\nwhere X_t satisfies the general SDE introduced in section  {ref}` stochastic differential equation (SDE):d X_t = \\mu(X_t, t) dt + \\sigma(X_t, t) dW_t\n\nTo prove it we use Ito’s lemma to compute the differential of the ansatz Y_s = e^{-\\int_t^s r(X_v,v) dv} u(X_s, s):\\begin{aligned}\nd Y_s = -r(X_s,s) e^{-\\int_t^s r(X_v,v) ds}  u(X_s, s) ds \\\\ + e^{-\\int_t^u r(X_v,v) dv} \\left(\\frac{\\partial u}{\\partial s} d s + \\frac{\\partial u}{\\partial  X_s} dX_s + \\frac{1}{2} \\frac{\\partial ^2 u}{\\partial X_s^2} \\sigma^2(X_s,s) ds\\right) \n\\end{aligned}\n\nwhich we can rewrite as\\begin{aligned}\nd Y_s = e^{-\\int_t^s r(X_v,v) dv} \\frac{\\partial u}{\\partial  X_s} \\sigma(X_s, s) dW_s + \\\\\n e^{-\\int_t^s r(X_v,v) dv} \\left(-r(X_s, s) u + \\frac{\\partial u}{\\partial  s} + \\frac{\\partial u}{\\partial  X_s} \\mu(X_s,s)+ \\frac{1}{2} \\frac{\\partial ^2 u}{\\partial X_s^2} \\sigma^2(X_s,s)  \\right) ds = \\\\ \n  e^{-\\int_t^s r(X_v,v) dv} \\frac{\\partial u}{\\partial  X_s} \\sigma(X_s, s) dW_s\n\\end{aligned}\n\nwhere we have used that u is the solution to the above PDE to make the term in parenthesis zero. We can now integrate this equation from t to T:\\begin{aligned}\n\\int_t^T d Y_s = Y_T - Y_t = \\int_t^T e^{-\\int_t^s r(X_v,v) dv} \\frac{\\partial u}{\\partial  X_s} \\sigma(X_s, s) dW_s \n\\end{aligned}\n\nIf we take now expectations, the right hand term is zero by using the properties of the integral of the Wiener process show in previous sections. Therefore:\\mathbb{E}[Y_T|X_t = x] = Y_t\n\ni.e. Y_t is a martingale. But this is actually the Feynman - Kac solution if we replace Y_t by its definition, since u(X_T, T) = f(X_T), which completes the proof.","type":"content","url":"/markdown/stochastic-calculus#feynman-kac","position":23},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl2":"Basic stochastic processes"},"type":"lvl2","url":"/markdown/stochastic-calculus#basic-stochastic-processes","position":24},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl2":"Basic stochastic processes"},"content":"","type":"content","url":"/markdown/stochastic-calculus#basic-stochastic-processes","position":25},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Brownian motion with drift","lvl2":"Basic stochastic processes"},"type":"lvl3","url":"/markdown/stochastic-calculus#brownian-motion-with-drift","position":26},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Brownian motion with drift","lvl2":"Basic stochastic processes"},"content":"The most simple stochastic process using the Wiener process as a building block is the Brownian motion with drift:d S_t = \\mu_t dt + \\sigma_t d W_t\n\nHere we are essentially shifting and rescaling the Wiener process to describe dynamical systems whose stochastic fluctuations have a deterministic non-zero mean mu_t\n(drift) and arbitrary variance, the latter controlled by the so-called volatility \\sigma_t. In practice, this is the process to use for any real application of stochastic calculus to model random time-series, since the simple Wiener process is simply too restrictive to describe\nany real process. The name Brownian motion comes actually from the early application of this kind of process to describe the random fluctuations of pollen suspended in water observed by English botanist Robert Brown\nin 1827. The actual modelling of this process would be pioneered by Albert Einstein \n\nEinstein, 1905, in one of the four papers published in 1905, his \"annus mirabilis\", each of them transformative of its\nrespective field.\n\nThe distribution of dS_t is easily derived by using the property that multiplying and summing scalars to a Gaussian random variable (dW_t) yields another Gaussian variable. Hence since dW_t \\sim N(0, t) then\nwe have:d S_t \\sim N(\\mu_t, \\sigma_t^2 dt)\n\nThis stochastic differential equation can be integrated to get the solution for arbitrary times:S_t = S_0 + \\int_0^t \\mu_u du + \\int_0^t \\sigma_u d W_u\n\nwhich again follows a Gaussian distribution:S_t \\sim N(S_0 + \\int_0^t \\mu_u du, \\int_0^t \\sigma_u^2 du)\n\nwhere we have used the properties of the stochastic integral discussed in section \n\nStochastic differential equations:\n\nConnection to Gaussian processes\n\nThe Brownian motion is also a Gaussian process, since any finite set S_{t_1}, S_{t_2}, S_{t_3}, ..., S_{t_n} follows a multivariate Gaussian process. The kernel of the Brownian process is simply to\nderive:\\begin{aligned}\nk(t_1, t_2) = \\textrm{cov}[X_{t_1}, X_{t_2}] = \\mathbb{E}[\\int_0^{t_1} \\sigma_u dW_u \\int_0^{t_2} \\sigma_v dW_v] = \\int_0^{min(t_1, t_2)}  \\sigma_u^2 du\n\\end{aligned}\n\nIf \\sigma_t = \\sigma, i.e. is a constant, we have simply:\\begin{aligned} k(t_1, t_2) = \\sigma^2 \\textrm{min}(t_1, t_2)\n\\end{aligned}\n\nwhich is the kernel of the Wiener process rescaled by\n\\sigma^2, as expected given the construction of the Brownian process.\n\nTo complete the specification we need to add a mean function, which is simply:\\mu_t^{GP}= S_0 + \\int_0^t \\mu_u du\n\nSimulation\n\nSimilarly to the Wiener process, simulation can be carried out using a discretization scheme. To improve numerical stability and convergence it is convenient to integrate first the distribution between points in the\nsimulation grid t_0, ..., t_N, t_{i+1} = t_i + \\Delta t_i, namely:S_{t_{i+1}} |F_{t_n} \\sim N(S_{t_i} + \\int_{t_i}^{t_{i+1}} \\mu_u du, \\int_{t_i}^{t_{i+1}} \\sigma^2_u du)\n\nIf we have a generator of standard Gaussian random numbers Z, we can perform the simulation using:S_{t_{i+1}} = S_{t_i} + \\int_{t_i}^{t_{i+1}} \\mu_u du + \\left(\\int_{t_i}^{t_{i+1}} \\sigma^2_u du \\right)^{1/2} Z\n\nIf \\mu_u and \\sigma_u are smooth functions of time and \\Delta t_i is small enough such that the variation in the interval is negligible, we can approximate the integrals:S_{t_{i+1}} |F_{t_n} \\sim N(S_{t_i} + \\mu_{t_i} \\Delta t_i,  \\sigma^2_{t_i} \\Delta)\n\nor simply:S_{t_{i+1}} = S_{t_i} + \\mu_{t_i} \\Delta t_i + \\sigma_{t_i} \\sqrt{\\Delta} Z\n\nThe following graph shows samples of the Brownian motion for specific values of drift and volatility:\n\n\n\nFigure 5:Simulation of five different paths of the the Brownian Motion with drift process using t_0=0 t_N=1, N = 100, W_0 = 0, \\mu = 5, \\sigma = 2.\n\nAlternatively, we could use the connection to Gaussian processes to perform the simulations using standard packages. We leave it as an exercise for the reader.\n\nEstimation\n\nIf we have a set of observations of a process D = \\{S_{t_0}, ..., S_{t_N}\\} that we want to model as a Brownian\nmotion, we need to find the value of the parameters of the process that best fit the data.\n\nLet us consider for simplicity that \\mu_t = \\mu,\n\\sigma_u = \\sigma and \\Delta t_i = \\Delta t . As discussed in the context of Bayesian models, a general estimation process models the parameters as random variables that capture our uncertainty about their exact values, which comes from\nthe fact that the sample is finite, or also in general because the data generation process will not necessarily be the one we are proposing, being models a simplification of reality. In this setup, the best\ninference of the parameters is carried out by computing the posterior distribution P(\\mu, \\sigma^2|D, I_P) where I_P represents the prior information about the parameters.\n\nSince as shown in the previous section we can write the process between observations as\nS_{t_{i+1}} - S_{t_i} = \\mu \\Delta t + \\sigma \\sqrt{\\Delta t_i} Z with Z\\sim N(0,1), this is equivalent to fitting a Bayesian linear regression model om S_{t_{i+1}} - S_{t_i} with an intercept \\mu \\Delta t and a noise term \\epsilon \\sim N(0, \\sigma^2 \\Delta t). As\nwe shown in chapter \n\nBayesian Modelling, if we model the prior distributions of the parameters as a Normal Inverse Gamma (NIG), meaning that:\\begin{aligned}\n    \\mu \\Delta | \\sigma^2 \\Delta t \\sim N(\\mu_0 \\Delta t, \\sigma^2 \\Delta t V_0) \\\\\n    \\sigma^2 \\Delta t \\sim IG(\\alpha_0, \\beta_0)\n\\end{aligned}\n\nwhere V_0, \\mu_0, \\alpha_0, \\beta_0 are parameters that define the prior, this distribution is a conjugate prior of the likelihood:P(D|\\mu \\Delta, \\sigma^2 \\Delta t,  P_I) = \\Pi_{n=1}^N \\frac{1}{\\sqrt{2\\pi\\Delta t ^2\\sigma^2}} e^{-\\frac{(S_{t_n} - S_{t_{n-1}} - \\Delta t \\mu)^2}{2 \\Delta t^2 \\sigma^2}}\n\nTherefore, the posterior is also a NIG, with updated parameters:\\begin{aligned}\n    \\mu \\Delta t | D, \\sigma^2  \\Delta t\\sim N(\\mu_N \\Delta t, \\sigma^2 \\Delta t V_N) \\\\\n    \\sigma^2 \\Delta t | D \\sim IG(\\alpha_N, \\beta_N)\n\\end{aligned}\n\nTo get the updated parameters from the general equations, bear in mind that in this representation with only an\nintercept our feature-set can be seen as a N dimensional vector {\\bf X} = (1, ..., 1). Therefore:\\begin{aligned}\n    V_N = (V_0^{-1} + N)^{-1} \\\\\n   \\mu_N \\Delta = \\mu_0 \\Delta t \\frac{V_N}{V_0} + \\frac{1}{V_0^{-1} + N } \\sum_{n=1}^N (S_{t_{n}} - S_{t_{n-1}})\\\\\n   \\alpha_N = \\alpha_0 + N/2 \\\\\n   \\beta_N = \\beta_0 + \\frac{1}{2}\\left(\\sum_{n=1}^N(S_{t_n}-S_{t_{n-1}})^2 + \\mu_0^2 \\Delta t^2 V_0^{-1} -\\mu_N^2 \\Delta^2 V_N^{-1} \\right)\n\\end{aligned}\n\nThe predictive marginal for the \\mu \\Delta t parameter requires integrating out the \\sigma^2 \\Delta t, resulting in:\\mu \\Delta t |D \\sim T(\\mu_N \\Delta t, \\frac{\\beta_N}{\\alpha_N} V_N, 2 \\alpha_N)\n\nwhere T is the Student’s distribution.\n\nFollowing our discussion in chapter \n\nBayesian Modelling, the best estimator of parameters extracted from the posterior in a mean squared error minimizing sense is to compute the mean of the posterior marginals. For \\sigma^2 \\Delta t, we\nhave:\\begin{aligned}\n\\mathbb{E}[\\sigma^2 \\Delta t |D, I_P] = \\frac{\\beta_N}{\\alpha_N-1}=  \\frac{\\beta_0}{\\alpha_0 -1 + N/2} + \\nonumber \\\\ \\frac{1}{2 \\alpha_0 + N -1}\\left(\\sum_{n=1}^N(S_{t_n}-S_{t_{n-1}})^2+\\mu_0^2 \\Delta t^2 V_0^{-1} -\\mu_N^2 \\Delta t^2 V_N^{-1} \\right) \n\\end{aligned}\n\nwhereas for \\mu we have:\\mathbb{E}[\\mu \\Delta t|D, I_P] = \\mu_N \\Delta  t= \\mu_0 \\Delta t \\frac{V_0}{V_N} + \\frac{1}{V_0^{-1} + N} \\sum_{n=1}^N (S_{t_{n}} - S_{t_{n-1}}))\n\nA special case deserved our attention: if we take the limit in the prior distribution V_0\\rightarrow\\infty and\n\\alpha_0, \\beta_0 \\rightarrow 0, the prior distribution becomes a non-informative or Jeffrey’s prior, with\np(\\sigma^2 \\Delta t) \\sim 1/(\\sigma^2 \\Delta t), and\np(\\mu\\Delta t|\\sigma^2\\Delta t) becoming a flat prior. Jeffrey’s prior is considered the best choice to describe non-informative priors of scale parameters as is the case of \\sigma^2 \\Delta t. In this situation:\\begin{aligned}\n\\mathbb{E}[\\mu \\Delta t|D, I_P] \\rightarrow \\mu_{MLE}\\Delta t = \\frac{1}{N} \\sum_{n=1}^N (S_{t_{n}} - S_{t_{n-1}})\\\\ \n\\mathbb{E}[\\sigma^2 \\Delta t|D, I_P] \\rightarrow \\frac{N}{N -1} \\sigma_{MLE}^2 \\Delta t \\nonumber \\\\ \\frac{N}{N -1}\\left(\\frac{1}{N} \\sum_{n=1}^N(S_{t_n}-S_{t_{n-1}})^2 -\\left(\\frac{1}{N} \\sum_{n=1}^N (S_{t_{n}} - S_{t_{n-1}})\\right)^2 \\right) \n\\end{aligned}\n\nwhich are the maximum likelihood estimators (MLE) of the\nmean and the variance of a Gaussian distribution, which is expected for a non-informative prior since 1) for a Gaussian distribution the mean and the mode coincide, 2) maximizing the posterior with the\nnon-informative prior is equivalent to maximizing the likelihood. MLE estimators are the standard approach to learn the parameters of stochastic processes in many practical situations, since the Bayesian\nestimations might become intractable for more complex processes as the Brownian motion. However, we must keep in mind their limitations, particularly that they neglect any prior information that might be useful in certain setups, and that they throw away the rest of the\ninformation contained in the posterior distribution.\n\nWe will see in later chapters that keeping the full posterior distribution can be relevant when we want to incorporate model estimation uncertainty into the inferences that are derived using stochastic processes. For instance, in the presence of a non-negligible\nestimation uncertainty of the parameters of our Brownian process, the correct predictive distribution of the next value of the process given its past history is actually given by:S_{t_{N+1}}|S_{t_N}, D, I_P \\sim T(\\mu_N \\Delta t, \\frac{\\beta_N}{\\alpha_N}(1+V_N), 2\\alpha_N)\n\nIf we consider a non-informative prior, this becomes:S_{t_{N+1}}|S_{t_N}, D, I_P \\sim T(\\mu_{MLE} \\Delta t, \\sigma_{MLE}^2(1 + \\frac{1}{N}), N)\n\nThis is to be compared with the inference using the MLE estimator, which would essentially plug the point MLE estimator into the Brownian process, yielding:S_{t_{N+1}}|S_{t_N}, D, I_P \\sim N(\\mu_{MLE} \\Delta t, \\sigma_{MLE}^2 \\Delta)\n\nThe mean and the mode of both distributions is the same, namely \\mu_{MLE} \\Delta t, but the Student distribution has fatter tails than the Gaussian distribution, reflecting the extra uncertainty coming from\nthe estimation error. Only in the case of having a very large data-set N\\rightarrow \\infty, both distributions converge, as in this limit the Student distribution becomes a Gaussian N(\\mu_{MLE} \\Delta t, \\sigma_{MLE} \\Delta t). Another way of seeing this is to interpret the variance term of the Student distribution as the sum\nof two terms: the first one coming from the intrinsic noise of the Brownian process, and the second one, the 1/N correction, coming from the estimation uncertainty.\n\nDimensional analysis\n\nIn order to reason about stochastic processes it is important to keep track or the units of their parameters. This can also be useful as a consistency check on the estimators derived, for instance. We will use a\nbrackets notation for dimensional analysis. For the case of the Brownian motion we write:[dS_t]= [\\mu_t] [dt] + [\\sigma_t] [dW_t]\n\nIn here, we know that [dt] has time units, for instance seconds, hours, or days. We denote them generically as [t] The Wiener process being distributed as dW_t \\sim N(0, dt) has therefore units of [t]^{1/2}, i.e. the square root of time. Therefore, the parameters have the following units:\\begin{aligned}\n[\\mu_t ] &=& [S] / [t] \\\\\n[\\sigma_t] &=& [S] / [t]^{1/2}\n\\end{aligned}\n\nwhere we have denoted generically [S] as the units of\nthe process. If for instance S is modelling a price of a financial instrument in a given currency like dollars, the units are dollars: [dS_t] = \\$. If additionally we are using seconds as time units, [t] = s. then we have [\\mu_t] = \\$ / s and [\\sigma_t] = \\$ / s^{1/2}. We can see that these units are consistent with the Gaussian distribution for the evolution of S_t: the argument\nof the exponential should be a-dimensional. Applying the previous units to the expression we can see that it is indeed the case:([S] - [\\mu_t] [t])^2 / ([\\sigma_t]^2 [t]^2) = 1\n\nApplications\n\nThe Brownian motion model has plenty of applications. In the financial domain it can be used to model financial or economical indicators for which there is no clear pattern in the time-series beyond potentially a drift to motivate a more sophisticated modelling, i.e. the time-series\nis mostly unpredictable. Notice that the Brownian motion has a domain that extends from (-\\infty, \\infty), so indicators that have a more restrictive domain might not be suitable for being modelled using the\nBrownian motion process. A typical example are prices of financial instruments, which cannot on general grounds be negative. However, there might be exceptions to this rule, for instance if we are modelling\nprices in a time - domain such as the probability of reaching negative values is negligible, the model might be well suited. For instance, when modelling intra-day prices.","type":"content","url":"/markdown/stochastic-calculus#brownian-motion-with-drift","position":27},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Geometric Brownian motion","lvl2":"Basic stochastic processes"},"type":"lvl3","url":"/markdown/stochastic-calculus#geometric-brownian-motion","position":28},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Geometric Brownian motion","lvl2":"Basic stochastic processes"},"content":"The geometric Brownian motion is a simple extension of the Brownian motion that introduces a constraint in the domain of the process, allowing only positive values. The stochastic differential equation is\nthe following one:d S_t = \\mu_t S_t dt + \\sigma_t S_t d W_t\n\nIn order to find the distribution of S_t for arbitrary times, let us apply Ito’s lemma over the function f(S_t) = \\log S_t. Such ansatz is motivated by looking at the deterministic part of the equation,\ndS_t = \\mu_t S_t dt, which can be rewritten as\nd \\log S_t = \\mu_t dt. Coming back to the stochastic differential equation:\\begin{aligned}\nd \\log S_t = \\frac{1}{S_t} d S_t - \\frac{1}{2}\\frac{1}{S_t^2} \\sigma_t^2 S_t^2 dt = \\nonumber \\\\\n= (\\mu_t - \\frac{1}{2} \\sigma_t^2)dt + \\sigma_t d W_t\n\\end{aligned}\n\nThis is the stochastic differential equation of the\nBrownian motion, which we already now how to integrate from the previous section:\\log S_t = \\log S_0 + \\int_0^t du (\\mu_u - \\frac{1}{2} \\sigma_u^2) +  \\int_0^t \\sigma_u d W_u\n\nwhich means that \\log S_t follows a Gaussian distribution\\log S_t \\sim N(\\log S_0 + \\int_0^t du (\\mu_u - \\frac{1}{2} \\sigma_u^2), \\int_0^u \\sigma_u^2 du)\n\nThe distribution of S_t is the well studied Log-normal distribution, which by definition is the distribution of a random variable whose logarithm follows a Gaussian distribution:S_t \\sim LN(\\log S_0 + \\int_0^t du (\\mu_u - \\frac{1}{2} \\sigma_u^2), \\int_0^u \\sigma_u^2 du)\n\nWe can also simply write:S_t = S_0 \\exp(\\int_0^t du (\\mu_u - \\frac{1}{2} \\sigma_u^2) +  \\int_0^t \\sigma_u d W_u)\n\nAn interesting property of the Log-normal distribution is the simple solution for its moments: \\begin{aligned}\nX \\sim LN(\\mu, \\sigma^2) \\nonumber \\\\\n\\mathbb{E}[X^n] = e^{n \\mu + \\frac{1}{2} n^2 \\sigma^2}\n\\end{aligned}\n\nIn particular, the mean of the distribution is\n\\mathbb{E}[X] = e^{\\mu + \\sigma^2/2} which we will use in multiple applications. This means, coming back to the Geometric Brownian Motion, that:\\mathbb{E}[S_t] = S_0 e^{\\int_0^t du (\\mu_u - \\frac{1}{2} \\sigma_u^2) +  \\frac{1}{2} \\int_0^t \\sigma_u^2 du} = S_0 e^{\\int_0^t \\mu_u du}\n\nConnection to Gaussian processes\n\nThe Geometric Brownian Motion (GBM) is also closely related to Gaussian processes, as its logarithm follows a Brownian motion with drift.\nTherefore, any finite set \\log(S_{t_1}), \\log(S_{t_2}), \\log(S_{t_3}), ..., \\log(S_{t_n})  follows a multivariate Gaussian distribution.\n\nThe covariance kernel of the logarithmic process can be derived as:k(t_1, t_2) = \\text{cov}[\\log(S_{t_1}), \\log(S_{t_2})] = \\mathbb{E}\\left[\\int_0^{t_1} \\sigma dW_u \\int_0^{t_2} \\sigma dW_v\\right] = \\int_0^{\\min(t_1, t_2)} \\sigma_u^2 du\n\nwhich for constant volatility \\sigma_u = \\sigma simplifies to:k(t_1, t_2) = \\sigma^2 \\min(t_1, t_2)\n\nThis is the same covariance kernel as that of a standard Brownian motion, but applied to the logarithm of the process, reflecting the Gaussian nature of the logarithmic returns of the GBM.\n\nIt only remains to add the mean process to the specification, which we have already calculated. Since we are working with the logarithm of the process, this is:\\mu_t^{GP}= \\log S_0 + \\int_0^t du (\\mu_u - \\frac{1}{2} \\sigma_u^2)\n\nSimulation\n\nWe can again simulate the GBM either using a discretization scheme or the connection to Gaussian processes. When using the discretization scheme more robust results can attained by simulating a Wiener process and then using a discrete integration of the GBM:S_{t_{i+1}} = S_{t_{i}} e^{(\\mu_{t_i} - \\frac{1}{2} \\sigma_{t_{i}}^2)\\Delta t + \\sigma_{t_{i}} Z}\n\nwhere Z \\sim N(0,1). We perform such simulation with constant parameters in the following figure:\n\n\n\nFigure 6:Simulation of five different paths of the Geometric Brownian Motion with drift process using t_0=0 t_N=1, N = 1000, S_0 = 100, \\mu = 0.05, \\sigma = 0.2$.\n\nEstimation\n\nThe estimation of the Geometric Brownian Motion is relatively straightforward by linking it to a Brownian motion using log-returns. Therefore, we can transform a set of observations D = \\{S_{t_0}, ..., S_{t_N}\\} into D' = \\{\\log S_{t_0}, ..., \\log S_{t_N}\\} and apply any of the estimation techniques discussed in the previous sections.\n\nApplications\n\nThe Geometric Brownian Motion is a corner-stone in financial modelling since it is the most simple model that captures non-negativity of asset prices like stocks. Therefore it has been applied in multiple context. For example, it was the model used by Black, Merton and Scholes for their infamous option pricing theory. It is also used typically in risk management use cases to simulate paths to compute value at risk (VaR).","type":"content","url":"/markdown/stochastic-calculus#geometric-brownian-motion","position":29},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Arithmetic mean of the Brownian motion","lvl2":"Basic stochastic processes"},"type":"lvl3","url":"/markdown/stochastic-calculus#arithmetic-mean-of-the-brownian-motion","position":30},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Arithmetic mean of the Brownian motion","lvl2":"Basic stochastic processes"},"content":"As the name suggests, this is a derived process from the standard Brownian motion in which the latter is averaged continuously from a certain initial time to t. We take for simplicity the initial time t_0 = 0:\\begin{aligned}\nM_t = \\frac{1}{t}\\int_0^t S_u du \n\\end{aligned}\n\nTo derive its distribution we integrate by parts the\nBrownian process:\\begin{aligned}\nM_t = \\frac{t S_t}{t} - \\frac{1}{t}\\int_0^t u d S_u  \n= S_0 + \\int_0^t (1-\\frac{u}{t}) dS_u\n\\end{aligned}\n\nIn this form, it is easy to derive the distribution, which is normal. Assuming for simplicity constant mean and volatility:M_t \\sim N(S_0 + \\frac{1}{2} \\mu t, \\frac{1}{3}\\sigma^2t)\n\nNotice that the variance of the average process is smaller than the variance of the process itself, by a factor 1/3. This makes sense, since taking averages smooths out the random behavior of the process.\n\nConnection to Gaussian Processes\n\nThe arithmetic mean of Brownian motion is closely connected to Gaussian processes. Since the underlying process, S_t, is itself a Gaussian process, the arithmetic mean M_t, being a linear transformation of the Brownian motion, also follows a Gaussian process.\n\nFor any finite set of times  M_{t_1}, M_{t_2}, \\dots, M_{t_n} , the joint distribution of these variables is multivariate Gaussian. To derive the covariance, let us first consider the zero-mean process:\\tilde{M_t} = M_t - \\mathbb{E}[M_t] = \\sigma \\int_0^t (1-\\frac{u}{t}) dW_t\n\nwhere have simply used the definition of the process and used dS_u = \\mu du + \\sigma dW_u. The covariance structure of the arithmetic zero-mean process can be derived as follows:\\text{cov}(M_{t_1}, M_{t_2}) = \\text{cov}(\\tilde{M}_{t_1}, \\tilde{M}_{t_2}) = \\mathbb{E}\\left[ \\sigma \\int_{0}^{t_1} (1-\\frac{u}{t_1}) dW_u \\cdot \\sigma \\int_{0}^{t_2} (1-\\frac{v}{t_2}) dW_v\\right]= \\sigma^2 \\int_0^{\\min(t_1,t_2)} (1-\\frac{u}{t_1}) (1-\\frac{u}{t_2}) du\n\nWhen t_1 < t_2 we have:\\text{cov}(M_{t_1}, M_{t_2}) = \\sigma^2 \\frac{t_1}{2}(1 - \\frac{t_1}{3 t_2})\n\nSymmetrically, when t_1 > t_2:\\text{cov}(M_{t_1}, M_{t_2}) = \\sigma^2 \\frac{t_2}{2}(1 - \\frac{t_2}{3 t_1})\n\nTherefore:\\text{cov}(M_{t_1}, M_{t_2}) = \\sigma^2 \\frac{\\min(t_1,t_2)}{2}(1 - \\frac{\\min(t_1,t_2)}{3 \\max(t_1, t_2)})\n\nAs in the previous processes, if we are starting from a non-zero mean and / or the Brownian motion has drift, we need to add the mean:\\mu_t^{GP} = S_0 + \\frac{1}{2}\\mu t\n\nSimulation\n\nSimulating the arithmetic mean does not present any extra complication with respect to the Brownian motion. We can discretize the underlying Brownian motion and simulate it, computing the mean at every step using the previous simulated values, i.e.:M_{t_i} = \\frac{1}{t_i} \\sum_{j = 0}^i S_{t_j} \\Delta t_j\n\nAlternatively, we can use the kernel function derived above and generate paths of the Gaussian process. The following example uses GPs:\n\n\n\nFigure 7:Simulation of five different paths of the arithmetic mean of the Geometric Brownian Motion with drift process using the correspondence to Gaussian Processes. Parameters: t_0=0 t_N=1, N = 1000, S_0 = 0, \\mu = 0.5, \\sigma = 0.3$.\n\nAs expected, the paths from the arithmetic mean are smoother than the ones from the underlying process, since they are averaged.\n\nEstimation\n\nGiven a dataset of observations of the mean, D = \\{M_{t_1}, ..., M_{t_N}\\}, the question is how to estimate the parameters of the underlying Brownian motion with drift. A priori, since the means are correlated, this could be a potentially complicated task. However, if we make the following transformation of the dataset:t_i M_{t_i}  - t_{i-1} M_{t_{i-1}}= \\int_{t_{i-1}}^{t_i} S_u du \\sim N(\\frac{1}{2} \\mu \\Delta_{t_i}, \\frac{1}{3}\\sigma^2 \\Delta t_i)\n\nthese new variables are independent. We can therefore use standard Maximum Likelihood Estimators for the mean and variance of independent Gaussian random variables to obtain \\mu and \\sigma.\n\nApplications\n\nIn the financial modelling domain, arithmetic means of stochastic processes are used for instance when modelling so-called Asian options, whose underlying is precisely the arithmetic mean of the price of a financial instrument. For Asian options on stocks, which is the typical case, a Geometric Brownian Motion (GBM) is used to model prices to ensure non-negativity, and therefore rigorously speaking is the arithmetic mean of the GBM the underlying.","type":"content","url":"/markdown/stochastic-calculus#arithmetic-mean-of-the-brownian-motion","position":31},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Orstein-Uhlenbeck process","lvl2":"Basic stochastic processes"},"type":"lvl3","url":"/markdown/stochastic-calculus#orstein-uhlenbeck-process","position":32},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Orstein-Uhlenbeck process","lvl2":"Basic stochastic processes"},"content":"The Orstein - Uhlenbeck process is an extension of the Brownian motion process in which the drift term is modified to prevent large deviations from the mean \\mu. This is achieved by making the drift penalize those\ndeviations:d S_t = \\theta (\\mu - S_t) dt + \\sigma d W_t\n\nThe strength of the penalty is controlled by a parameter \\theta > 0, which is called the mean reversion speed: the larger this parameter, the faster the process S_t reverts to the mean. Such property of mean\nreversion is not unique to the Orstein - Uhlenbeck process, but the latter is probably the most simple way to achieve this effect in a stochastic differential equation, allowing for the computation of\nclosed-form solutions for the probability distribution of the process. To derive it, we use the following ansatz:\\begin{aligned}\nd(e^{\\theta t} S_t) = \\theta e^{\\theta t} S_t dt + e^{\\theta t} dS_t = \\nonumber \\\\\ne^{\\theta t}(\\theta \\mu dt + \\sigma dW_t)\n\\end{aligned}\n\nIntegrating this equation:\\begin{aligned}\ne^{\\theta t} S_t - S_0 = (e^{\\theta t} - 1) \\mu + \\sigma \\int_0^t e^{\\theta u} dW_u\n\\end{aligned}\n\nFinally:S_t = S_0 e^{-\\theta t} + \\mu (1 - e^{-\\theta t}) + \\sigma \\int_0^t e^{\\theta (u-t)} dW_u\n\nTherefore, the Orstein - Uhlenbeck process follows a Gaussian distribution with time dependent drifts and variances:S_t \\sim N(S_0 e^{-\\theta t} + \\mu (1 - e^{-\\theta t}), \\frac{\\sigma^2}{2\\theta} (1 - e^{-2\\theta t}))\n\nThe process has a natural time-scale, the mean reversion time, \\tau \\equiv 1/\\theta. For times t \\gg \\tau the distribution becomes stationary around the long-term mean:S_t \\sim N(\\mu, \\frac{\\sigma^2}{2\\theta})\n\nThe variance of the stationary distribution is no longer time - dependent, as in the Brownian motion process. Due to the pull-back effect that the mean reversion induces in the process around the long-term mean, the process remains bounded and stable over time.\n\nConnection to Gaussian Processes\n\nGiven the previous analysis on the distribution of the Orstein - Uhlenbeck process, which is Gaussian, we can readily see that for any finite set of times, the joint distribution of  S_{t_1}, S_{t_2}, \\dots, S_{t_n} , is multivariate Gaussian. The Orstein - Uhlenbeck is therefore a Gaussian Process with kernel given by:k(t_1, t_2) = \\textrm{cov}[S_{t_1} S_{t_2}] = \\sigma^2 \\int_0^{\\min(t_1,t_2)} e^{-\\theta(t_1-u)} e^{-\\theta(t_2 -u)}du \\nonumber \\\\ = \\sigma^2 e^{-\\theta (t_1 + t_2) }\\left( \\frac{e^{2 \\theta \\min(t_1,t_2)}-1}{2\\theta}\\right) = \\frac{\\sigma^2}{2\\theta} e^{-\\theta |t_1 - t_2| }\\left( 1-e^{2 \\theta \\min(t_1,t_2)}\\right)\n\nwhere we have used that t_1 + t_2 - 2 \\min(t_1, t_2) = |t_1 - t_2| to simplify the expression.\n\nThe specification so far only describes the mean-reverting fluctuations around the long-term mean. To complete the mapping to a Gaussian process of the full Orstein - Uhlenbeck process we need to add the mean to the specification:\\mu_t^{GP} = S_t = S_0 e^{-\\theta t} + \\mu (1 - e^{-\\theta t})\n\nSimulation\n\nThe simulation of the Orstein - Uhlenbeck process can be done once again by using a Euler discretization scheme of the SDE or exploiting the connection to Gaussian processes. If we use the former, it is convenient as it happened with the Geometric Brownian Motion, not to work directly with a discretized SDE, but to solve the equation for the discrete time step and sample from the Gaussian distribution:S_{t_{i+1}} \\sim N(S_{t_i} e^{-\\theta \\Delta t_i} + \\mu (1 - e^{-\\theta \\Delta t_i}), \\frac{\\sigma^2}{2\\theta} (1 - e^{-2\\theta \\Delta t_i}))\n\nThe following picture shows such simulation. The initial condition is far from the long-term mean, showing a clear reversion until the stationary state is reached, over a time-scale proportional to the inverse of the mean-reversion speed \\theta.\n\n\n\nFigure 8:Simulation of five different paths of the Orstein Uhlenbeck process using t_0=0 t_N=1, N = 1000, S_0 = 5, \\mu = 1, \\theta = 10, \\sigma = 1$.\n\nEstimation\n\nGiven a set of observations D = \\{ S_{t_i}, ..., S_{t_N}\\}, we can use maximum likelihood estimation to find the parameters of the model. The\nThe log-likelihood function \\mathcal{L} for the observed data is:\\mathcal{L}(\\theta, \\mu, \\sigma^2) = -\\frac{n}{2} \\ln(2\\pi) - \\frac{1}{2} \\sum_{i=1}^n \\left[ \\ln v_i + \\frac{(S_{t_i} - m_i)^2}{v_i} \\right]\n\nwhere:m_i \\equiv \\mu + (S_{t_{i-1}} - \\mu) e^{-\\theta \\Delta t_i}v_i \\equiv \\frac{\\sigma^2}{2\\theta} \\left( 1 - e^{-2\\theta \\Delta t_i} \\right)\n\nDue to the complexity of the log-likelihood function, finding closed-form solutions for all the parameters is not possible, and we need to resort to numerical optimization.\n\nHowever, under the assumption of equally spaced observations \\Delta t_i = \\Delta t, we can simplify the estimation process linking the model to a linear regression problem. Define:\\phi = e^{-\\theta \\Delta t}\\gamma = 1 - \\phi = 1 - e^{-\\theta \\Delta t}\\psi^2 = \\frac{\\sigma^2}{2\\theta} (1 - \\phi^2)\n\nThe process can be written in discrete time as:S_i = \\mu \\gamma + \\phi S_{t_{i-1}} + \\varepsilon_i\n\nwhere \\varepsilon_i \\sim \\mathcal{N}(0, \\psi^2) . We can estimate \\phi and \\mu by regressing S_i on S_{i-1}:S_{t_i} = \\beta_0 + \\beta_1 S_{t_{i-1}} + \\varepsilon_i\n\nwhere \\beta_0 = \\mu \\gamma and \\beta_1 = \\phi. The ordinary least squares (OLS) estimation yields:\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (S_{t_{i-1}} - \\bar{S}_{-1})(S_{t_i} - \\bar{S})}{\\sum_{i=1}^n (S_{t_{i-1}} - \\bar{S}_{-1})^2} \\equiv \\hat{\\rho}_{t_i, t_{i-1}}\n\nwhich is the first-order auto-correlation function. Moreover:\\hat{\\beta}_0 = \\bar{S} - \\hat{\\beta}_1 \\bar{S}_{t_{i-1}} \\approx \\bar{S} (1 - \\hat{\\beta}_1)\n\nwhere the last approximation holds for a large enough number of samples. We can now come back to the original parameters. Starting with the long-term mean,  Since \\beta_0 = \\mu \\gamma = \\mu (1 - \\phi) and \\phi = \\beta_1:\\hat{\\mu} = \\frac{\\hat{\\beta}_0}{1 - \\hat{\\phi}} = \\frac{\\hat{\\beta}_0}{1 - \\hat{\\beta_1}}  \\approx \\bar{S}\n\nThe mean-reversion speed reads\\hat{\\theta} = -\\frac{1}{\\Delta t} \\ln \\hat{\\phi}  =  -\\frac{1}{\\Delta t} \\ln \\hat{\\rho}_{t_i, t_{i-1}}\n\nFinally, to estimate the variance, we use the definition of  \\psi^2 :\\psi^2 = \\frac{\\sigma^2}{2\\theta} (1 - \\phi^2)\n\nTherefore, the estimator is:\\hat{\\sigma}^2 = \\frac{2 \\hat{\\theta} \\hat{\\psi}^2}{1 - \\hat{\\phi}^2}\n\nwhere \\psi^2 is the residual variance of the regression, which is estimated as:\\hat{\\psi}^2 = \\frac{1}{n} \\sum_{i=1}^n (S_{t_i} - \\hat{\\beta}_0 - \\hat{\\beta}_1 S_{t_{i-1}})^2\n\nApplications\n\nThe Orstein - Uhlenbeck process can be applied to a variety of real phenomena. In the financial and economical domain, we find mean reversion in multiple processes like:\n\nInflation: which is influenced by central bank policy with specific inflation targets. For example, 2% in the Eurozone.\n\nInterest rates: also influenced by central bank policy, as a way to affect inflation. It is also monitored by governments in order to ensure individual and companies can borrow money in competitive conditions.\n\nCommodity prices: subject to supply and demand forces, stationality and, in some cases like oil, explicit intervention by organizations like OPEC, which use their oligopolistic control of supply to ensure prices remain within a range that is not too low to affect profits and too high to incentivise the switch to alternative sources.\n\nStatistical arbitrage: prices of financial instruments might not show mean reversion, but combinations of them sometimes do. For example, pairs of stocks of companies with similar fundamentals.\n\nOther mean reverting processes\n\nDespite the popularity of the Orstein - Uhlenbeck process to model mean - reverting processes due to its simplicity and tractability, it has limitations in the type of models that can describe. For instance, for processes that can only take positive values, and the mean reversion time is independent of the distance to the long-term mean.\n\nA popular related model is the Cox-Ingersoll-Ross (CIR) process, which is a variation of the Orstein-Uhlenbeck process, but with the added feature that the volatility term depends on the square root of the process itself, ensuring that the process remains positive.\n\nThe CIR process is given by:d S_t = \\theta (\\mu_t - S_t) dt + \\sigma_t S_t^{1/2} d W_t\n\nAgain, \\theta > 0 controls the speed of mean reversion, \\mu represents the long-term mean level, and \\sigma is the volatility. Unlike the Orstein-Uhlenbeck process, where the volatility is constant, the CIR model introduces a state-dependent volatility term \\sqrt{S_t}, which ensures that S_t stays positive (as the volatility goes to zero when S_t approaches zero). As a byproduct, the mean reversion time also depends on the actual value of the process.\n\nThough no closed-form solution exists for the CIR process as simple as the one for the Orstein-Uhlenbeck process, it is often analyzed through numerical methods or approximations. The Fokker-Planck equation corresponding to the CIR process can be solved to obtain the transition probability density, which follows a non-central chi-squared distribution.\n\nThe CIR process is widely used in financial applications, such as in the modeling of short-term interest rates and in the Heston model for stochastic volatility, due to its ability to capture mean reversion and maintain non-negativity.\n\nOther alternatives to the Orstein - Uhlenbeck process that allow to specifically model the behaviour of the mean reversion time are non linear drift models in the form:d S_t = \\theta f(\\mu_t - S_t) (\\mu_t - S_t) dt + \\sigma_t d W_t\n\nwhere f(\\cdot) is function of the distance to the mean, for instance a power law.","type":"content","url":"/markdown/stochastic-calculus#orstein-uhlenbeck-process","position":33},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Brownian bridge","lvl2":"Basic stochastic processes"},"type":"lvl3","url":"/markdown/stochastic-calculus#brownian-bridge","position":34},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Brownian bridge","lvl2":"Basic stochastic processes"},"content":"Another simple extension of the Wiener process is the Brownian bridge, that we denote as B_t. It defines a stochastic process within a time interval [0,T] that behaves as a Wiener process with the exception of\nhaving an extra constraint, namely that B_T = 0 (in addition to B_0 = 0 as in the Wiener process). Such process can be constructed from a single Wiener process W_t as follows:B_t =  W_t - \\frac{t}{T} W_T\n\nIn order to derive the distribution of B_t, we decompose it in two independent Gaussian variables:B_t =  W_t - \\frac{t}{T} (W_T - W_t + W_t) = (1-\\frac{t}{T})W_t - \\frac{t}{T}(W_T - W_t)\n\nwhere the two terms of the expression are independent by using the property that non overlapping differences of Wiener processes are independent. This is therefore the distribution of two independent Gaussian variables, which we know is also Gaussian:B_t \\sim N(0, t(1 - \\frac{t}{T}))\n\nwhere we have used:\\begin{aligned}\n\\mathbb{E}[(1-\\frac{t}{T})W_t|F_0]  - \\mathbb{E}[\\frac{t}{T}(W_T - W_t)|F_0] = 0 \\\\\n\\mathbb{E}[((1-\\frac{t}{T})W_t)^2|F_0]  + \\mathbb{E}[(\\frac{t}{T}(W_T - W_t))^2|F_0] = \\nonumber \\\\ (1-\\frac{t}{T})^2 t + (\\frac{t}{T})^2 (T-t) = t(1 - \\frac{t}{T})\n\\end{aligned}\n\nThe result fits well our expectation of such process:\nfor t = 0 and t = T, the process has no variance, since those points correspond to the constraints where the process is set to zero. Given those constraints, and the symmetry of the Brownian bridge with respect\nto the transformation t \\rightarrow T-t, we expect the maximum variance to be at the half-point T/2, which is indeed the maximum of the function t(1-\\frac{t}{T})\n\nConnection to Gaussian processes\n\nThe Brownian bridge is also a Gaussian process, with the following kernel:\\begin{aligned}\nk(t_1,t_2) = \\textrm{cov}[B_{t_1} B_{t_2}] = \\mathbb{E}[(W_{t_1} - \\frac{t_1}{T} W_T])(W_{t_2} - \\frac{t_2}{T} W_T)] = \\nonumber \\\\ \\textrm{min}(t_1, t_2) -  \\frac{t_2}{T} t_1 -  \\frac{t_2}{T} t_1 + \\frac{t_1 t_2}{T} = \\textrm{min}(t_1, t_2) - \\frac{t_1 t_2}{T}\n\\end{aligned}\n\nThe kernel has two terms: the first one corresponds to the kernel of a Wiener process, whereas the second one is a correction that reflects the boundary conditions of the Brownian bridge, i.e. the increasing certainty as we approach t \\rightarrow T. Mathematically, we have k(0,0) = 0 as in the Brownian motions, but the additional constraint k(T, T) = 0, specific from the Brownian motion.\n\nSimulation\n\nAs usual, we can simulate the process either discretizing the stochastic differential equations or exploiting the Gaussian processes connection. If we want to approach the problem using discretization, the most simple way to do it is to simulate a discrete Wiener process which is then plugged into the Brownian Bridge expression.\n\nIn the following picture we follow the other path and simulate the Brownian Bridge using Gaussian Processes:\n\n\n\nFigure 9:Simulation of five different paths of the the Brownian Bridge using t_0=0 t_N=1, N = 100.\n\nApplications\n\nBrownian bridges or more realistic but similarly constructed processes (e.g. using the Brownian motion as the primitive instead of the Wiener process, or specifying fixed but non-zero boundaries), can be used to\nmodel situations where there is no uncertainty about the final value of a stochastic process. This is for instance the case of financial instruments like bonds, in particular zero-coupon bonds, which are bonds\nthat simply return their principal at maturity without paying a coupon.","type":"content","url":"/markdown/stochastic-calculus#brownian-bridge","position":35},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl2":"Jump processes"},"type":"lvl2","url":"/markdown/stochastic-calculus#jump-processes","position":36},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl2":"Jump processes"},"content":"So far we have described a family of continuous stochastic processes, with the Wiener process as the primitive to model different behaviors that can be useful to describe multiple phenomena. However, in many real situations we observe discrete jumps that are not rigorously described by these stochastic models, for which the probability of a jump of any size tends to zero as dt \\rightarrow 0.\n\nOf course, one could argue that a continuous model is in general wrong in many applications, and it is only used in practice because of tha mathematical tractability. Take for instance price series of financial instruments. At some time-scale determined by the infrastructure of the exchange, prices simply move discretely tick by tick, and a continuous model is no longer a good description of the times-series at this time-scale.\n\nAs mentioned, though, continuous models are in fact a good description of financial price-series at time-scales of interests for many trading models, continuity being a natural reflection of investors taking the last price traded as the reference for successive valuations. However, even in those time-scales we observe sometimes price movements in a short period of a magnitude that a continuous model cannot account for. Examples are for example 2010’s flash-crash in the US market when using a time resolution of minutes, or October 1989’s Black Friday when using end of day prices.\n\nThis is not because the distributions derived in the continuous stochastic processes studied so far don’t allow those scenarios. Think for instance that the Gaussian distribution has non-zero probability density over the entire domain, and in real applications one could argue that the jump does not happen instantaneously if short enough time-scales are observed. But they are so unlikely under the model that any real application built upon them will not take into account those scenarios in practice, introducing potential risks. In other words, they are not good generative models of the phenomenon at hand, since such scenarios will not in practice expected when sampling or simulating from them.\n\nGiven that we still want to exploit the mathematical simplicity of continuous stochastic processes, we would like to introduce mathematical tools that allow us to incorporate discrete jumps with a meaningful probability in these models. The motivation is again that of mathematical simplicity, since one could argue that more complex continuous models could be built to describe those situations, as far as we agree that at a short enough time - scale what we observe as a jump can be described as a rapidly series of continuous changes. For instance, a model with regime switches in volatility. However, such complexity would penalize the applications of the model, hence the interest in modelling jumps in continuous time.\n\nIn the following section we will motivate such model, that we will formalize later on.","type":"content","url":"/markdown/stochastic-calculus#jump-processes","position":37},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Poisson processes","lvl2":"Jump processes"},"type":"lvl3","url":"/markdown/stochastic-calculus#poisson-processes","position":38},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Poisson processes","lvl2":"Jump processes"},"content":"Motivation: a simple model for jumps in continuous time\n\nWe want to develop a model that allows for jumps of a given size J to happen instantaneously in continuous time. We denote by N_tthe number of jumps that have accumulated in the time interval [0, t]. We make the hypothesis that a jump between time t and t + dt happens with a probability that is independent of the number of jumps so far:P(N_{t+dt} = n \\mid N_t = n-1) = \\lambda dt\n\nwhere \\lambda is a constant. This means, on the other hand, that:P(N_{t+dt} = n \\mid N_t = n) = 1 - \\lambda \\, dt,\n\nsince we are keeping terms up to O(dt).\n\nApplying the rules of probability, we can express the probability of having n jumps up to time t + dt as:P(N_{t+dt} = n) = P(N_{t+dt} = n \\mid N_t = n) P(N_t = n) + P(N_{t+dt} = n \\mid N_t = n-1) P(N_t = n -1) \\lambda dt \\nonumber \\\\ \n= P(N_t = n) \\left( 1 - \\lambda dt \\right) + P(N_t = n-1) \\lambda dt\n\nReorganizing the equation we get:\\frac{P(N_{t+dt} = n) - P(N_t = n)}{dt} = -\\lambda P(N_t = n) + \\lambda P(N_t = n - 1)\n\nTaking the limit as dt \\rightarrow 0, this becomes a differential equation:\\frac{d}{dt} P(N_t = n) = -\\lambda P(N_t = n) + \\lambda P(N_t = n - 1)\n\nFor n = 0, this equation simplifies to:\\frac{d}{dt} P(N_t = 0) = -\\lambda P(N_t = 0)\n\nwhose solution is, with the initial condition P(N_0 = 0) = 1:P(N_t = 0) = e^{-\\lambda t}\n\nwhich is called the first jump probability. For n = 1 we have:\\frac{d}{dt} P(N_t = 1) = -\\lambda P(N_t = 1) + \\lambda P(N_t = 0) = -\\lambda P(N_t = 1) + \\lambda e^{-\\lambda t}\n\nWe can rewrite the equation as:e^{\\lambda t} \\frac{d}{dt} P(N_t = 1) + \\lambda e^{\\lambda t} P(N_t = 1) = \\lambda \\\\ \n\\rightarrow \\frac{d}{dt}\\left(e^{\\lambda t} P(N_t = 1)\\right) = \\lambda\n\nTherefore the solution that satisfies the initial condition is P(N_t = 1) = e^{\\lambda t} \\lambda t. Continuing with n = 2 we have the differential equation:\\frac{d}{dt} P(N_t = 2) + \\lambda P(N_t = 2) = \\lambda^2 t e^{-\\lambda t}\n\nwhich can be again rewritten as:\\frac{d}{dt}\\left(e^{\\lambda t} P(N_t = 1)\\right) = \\lambda^2 t\n\nAnd the solution is now P(N_t = 2) = \\frac{(\\lambda t)^2}{2} e^{-\\lambda t}.\n\nAt this point a patterns starts to be clear in the recursive solution as to apply the inductive step and propose a solution of the following form that generalizes the previous solutions:P(N_t = n) = \\frac{(\\lambda t)^{n}}{n!} e^{- \\lambda t }\n\nWhich we can see that satisfies the recursive differential equation:\\frac{d}{dt}P(N_t = n) = -\\lambda P(N_t = n) + \\frac{n \\lambda (\\lambda t)^{n-1}}{n!} e^{- \\lambda t } \\nonumber \\\\ = -\\lambda P(N_t = n) + \\lambda P(N_t = n-1)\n\nWe recognize this expression as the Poisson distribution function with an intensity \\lambda. Therefore this is called a homogeneous Poisson process, in contrast to non-homogeneous Poisson processes where the intensity is a deterministic function of time \\lambda_t.\n\nThe previous demonstration can be easily reproduced in this case. The differential equation reads now:\\frac{P(N_{t+dt} = n) - P(N_t = n)}{dt} = -\\lambda_t P(N_t = n) + \\lambda_t P(N_t = n - 1)\n\nwith the following solution, that can also be constructed inductively:P(N_t = n) = \\frac{(\\int_0^t\\lambda_s ds)^{n}}{n!} e^{- \\int_0^t \\lambda_s ds }\n\nThe previous processes are specific cases of counting processes, stochastic models that count the number of random events over time. There are other two related counting processes that is worth to mention: we have a self-exciting or Hawkes process when the intensity is a function of previous number of jumps, namely:\\lambda_t = \\mu_t + \\sum_{i, \\tau_i < t} \\nu(t-\\tau_i)\n\nwhere \\mu_t is a deterministic function (the background intensity). \\tau_i is the time at which jump i happened, and \\nu is the excitation function, a deterministic function that controls the clustering of jumps. A popular choice is an exponential shape: \\mu(t - \\tau_i) = \\alpha e^{-\\beta(t- \\tau_i)}.\n\nFinally, if the intensity is itself a stochastic process, we have a Cox process.\n\nDefinition and general properties\n\nA counting process N_t is called a Poisson process with rate \\lambda > 0 if it satisfies the following properties:\n\nN_0 = 0: the process starts with zero events at time zero.\n\nIndependent increments: The numbers of events occurring in disjoint time intervals are independent. That is, for any  0 \\leq s < t , the number of events in [s, t] is independent of the events before time s.\n\nStationary increments: The probability distribution of the number of events occurring in any time interval depends only on the length of the interval, not on its position on the time axis. Specifically, for a homogeneous Poisson process, the number of events in an interval of length t follows a Poisson distribution with parameter \\lambda:P(N_{t + s} - N_s = n) = \\frac{(\\lambda t)^n}{n!} e^{-\\lambda t}\n\nNo simultaneous events: The probability of more than one event occurring in an infinitesimally small interval dt is negligible, specifically of order O(dt).\n\nAn important property of the Poisson process is that the times between consecutive events, known as inter-arrival times, are independently and identically distributed (i.i.d.) exponential random variables with parameter \\lambda:P(T_i > t) = e^{-\\lambda t}, \\quad t \\geq 0\n\nwhere T_i is the time between the (i-1)-th and the i-th event, a random variable with probability density:f(t) = \\frac{d P(T_i \\leq t)}{dt} = \\lambda e^{-\\lambda t}\n\nThe derivation of this property is straightforward since we can link both distributions:P(T_i > t) = P(N_{t+T_{i-1}} - N_{T_{i-1}} = 0) = e^{-\\lambda t}\n\nThe exponential distribution of inter-arrival times implies the memory-less property: the probability that an event occurs in the next t units of time is independent of how much time has already elapsed since the last event. The average inter-arrival time is easy to compute as:E[T_i]=\\int_0^\\infty dt t f(t) = \\frac{1}{\\lambda}\n\nMean and variance:\n\nFor a Poisson process with rate \\lambda, both the expected value and the variance of N_t are linear in time:E[N_t] = \\lambda t, \\quad Var(N_t) = \\lambda t\n\nThis reflects that the mean number of events grows linearly with time, and the variance equals the mean.\n\nEstimation\n\nThe estimation of Poisson processes is relatively straightforward since they are characterized by a single parameter \\lambda, which is directly linked with the mean of the process E[N_t] = \\lambda t. Therefore, by computing the average number of jumps in a given time interval we can quickly estimate this parameter.\n\nSimulation\n\nSimulating Poisson processes is relatively simple by discretizing time in a grid with time-step \\Delta t. Then we can approximate:P(N_{t_i+\\Delta t} = n \\mid N_{t_i} = n-1) \\simeq \\lambda \\Delta tP(N_{t_i+\\Delta t} = n \\mid N_{t_i} = n) \\simeq 1-\\lambda \\Delta t\n\nwhich is a random binary variable with probability \\Delta t. Therefore, starting at t = 0, we can build paths of the Poisson process by using samples of this binary variable. At every time-step t_i we keep or increase the counter N_{t_i} depending on the result. The following plot shows such simulation of a Poisson process.\n\n\n\nFigure 10:Simulation of a Poisson process in a discrete grid with 50 time-steps and intensity \\lambda = 0.2\n\nThe same method can be used for more complex jump processes. For example, the following plot has been generated for a Hawkes process using a exponential excitation function. In this case, jumps tend to cluster together in comparison with the behaviour of the Poisson process.\n\n\n\nFigure 11:Simulation of a Hawkes process in a discrete grid with 50 time-steps. The baseline intensity is again \\lambda = 0.2. The excitation function is an exponential with \\alpha = 0.5 and \\beta = 1.\n\nApplications:\n\nPoisson processes are widely used to model random events in various fields, like queueing theory or telecommunications. In finance, they are typically used to model the arrival of orders (RfQs, limit or market orders), or to simulate defaults of companies, in which case only a single jump is allowed (jump to default).\n\nThey can also model jumps in financial prices, for instance in end of day time series, that incorporate the effect of unexpected news. For this application, though, these processes need to be generalized to incorporate random jumps (compound Poisson processes) and be included in Brownian price diffusions that model the evolution of prices in normal situations (jump diffusion processes).","type":"content","url":"/markdown/stochastic-calculus#poisson-processes","position":39},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Compound Poisson processes","lvl2":"Jump processes"},"type":"lvl3","url":"/markdown/stochastic-calculus#compound-poisson-processes","position":40},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Compound Poisson processes","lvl2":"Jump processes"},"content":"In many applications, events not only occur randomly over time but also have random magnitudes or impacts. A compound Poisson process extends the Poisson process by allowing for random jump sizes.\n\nA compound Poisson process X_t is defined as:J_t = \\sum_{i=1}^{N_t} Y_i\n\nwhere:\n\nN_t is a Poisson process with rate \\lambda.\n\n\\{ Y_i \\} are i.i.d. random variables representing the sizes of the jumps occurring at event times.\n\nThe Y_i are independent of the Poisson process N_t.\n\nCompound Poisson processes have the following properties:\n\nIndependent increments: The increments X_{t + s} - X_s over non-overlapping intervals are independent.\n\nStationary increments: For a homogeneous Poisson process, the statistical properties of increments depend only on the length of the interval, not its position.\n\nDistribution of X_t: The distribution of X_t is determined by both the distribution of N_t and the distribution of the jump sizes Y_i.\n\nMean and variance:\n\nThe mean and variance of a compound Poisson process can be calculated using Wald’s equation, which states that the mean of the sum of real-valued, independent and identically distributed random variables x_i, of which there are a random number of them N \\geq 0, is:\\mathbb{E}[x_1 + ... + x_N] = \\mathbb{E}[N] \\mathbb{E}[x_1]\n\nThis result can be proven simply by conditioning the expected value on N:\\mathbb{E}[x_1 + ... + x_N] = \\sum_{n=0}^\\infty P(N=n) \\mathbb{E}[x_1 + ... + x_N|N]  \\nonumber \\\\ = \\sum_{n=0}^\\infty P(N=n) N \\mathbb{E}[x_1] = \\mathbb{E}[N] \\mathbb{E}[x_1]\n\nwhere we have used that \\mathbb{E}[x_1] is independent of N. Coming back to the definition of compound Poisson process:\\mathbb{E}[J_t] = \\mathbb{E}[N_t] \\mathbb{E}[Y_i] = \\lambda t \\mathbb{E}[Y_i]\n\nA similar reasoning can be applied to the variance:\\mathbb{Var}(J_t) = \\mathbb{E}[N_t] \\mathbb{Var}(Y_i) + \\mathbb{Var}(N_t) \\left( \\mathbb{E}[Y_i] \\right)^2 = \\lambda t \\left( \\mathbb{Var}(Y_i) + \\left( \\mathbb{E}[Y_i] \\right)^2 \\right)\n\nSimulation:\n\nTo simulate a compound Poisson process we simulate first  the number of events N_t occurring up to time t using a Poisson distribution with parameter \\lambda t. Then, for each event i = 1, \\dots, N_t, we sample a jump size Y_i from the specified distribution. Finally, we sum the jump sizes to obtain J_t = \\sum_{i=1}^{N_t} Y_i.\n\nA result of such simulation is shown in the following figure, with jumps being driven by an exponential model.\n\n\n\nFigure 12:Simulation of a compound Poisson process in a discrete grid with 50 time-steps. The baseline intensity is \\lambda = 0.2. For the distribution of jumps we use a exponential distribution with rate \\eta = 2.5. We plot separately the underlying Poisson process result and the simulated jump sizes, as well as the resulting compound process.","type":"content","url":"/markdown/stochastic-calculus#compound-poisson-processes","position":41},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Jump Diffusion Processes","lvl2":"Jump processes"},"type":"lvl3","url":"/markdown/stochastic-calculus#jump-diffusion-processes","position":42},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Jump Diffusion Processes","lvl2":"Jump processes"},"content":"So far we have discussed jump processes that simulate discrete random events. But we would like to combine them with the continuous stochastic processes we studied in the previous part of this chapter, in order to generate richer dynamics to model real phenomena.\n\nA jump diffusion process X_t is a stochastic process that combines a standard diffusion process with a jump component. It can be defined by the stochastic differential equation (SDE):dX_t = \\mu(X_{t^-}, t) \\, dt + \\sigma(X_{t^-}, t) \\, dW_t + dJ_t\n\nwhere:\n\n\\mu(X_{t^-}, t) is the drift coefficient, representing the deterministic trend.\n\n\\sigma(X_{t^-}, t) is the diffusion coefficient, representing the volatility.\n\nW_t is a Wiener process\n\nJ_t represents the jump component, with J_t a compound Poisson process J_t =  \\sum_{i=1}^{N_t} Y_i\n\nX_{t^-} denotes the value of X_t just before time t, accounting for any discontinuities at t.\n\nAs with the Wiener process, the differential of the jump process is a shorthand notation for dJ_t = J_{t+dt} - J_t, i.e. the number of jumps happening between t and t+dt. As we saw above, in a compound Poisson process since the intensity is \\lambda dt only one or zero jumps are possible in the limit dt \\rightarrow 0. Therefore, we can simply write:dJ_t = Y \\, dN_t\n\nwith P(dN_t = 1) = \\lambda dt.\n\nIto’s Lemma with Jumps\n\nTo analyze functions of jump diffusion processes, we need to extend the classical Ito’s lemma to account for jumps. The Ito’s lemma with jumps provides a framework for calculating the differential of a function applied to a process that includes both continuous and jump components.\n\nLet X_t be a jump diffusion process defined above, and let f(X_t, t) be a twice continuously differentiable function in x and once differentiable in t. Then, the differential df(X_t, t) is given by:\\begin{align*}\ndf(X_t, t) &= \\left( \\frac{\\partial f}{\\partial t} + \\mu(X_{t^-}, t) \\frac{\\partial f}{\\partial x} + \\frac{1}{2} \\sigma^2(X_{t^-}, t) \\frac{\\partial^2 f}{\\partial x^2} \\right) dt \\\\\n&\\quad + \\sigma(X_{t^-}, t) \\frac{\\partial f}{\\partial x} \\, dW_t + \\left[ f(X_{t^-} + \\Delta X_t, t) - f(X_{t^-}, t) \\right] dN_t,\n\\end{align*}\n\nwhere \\Delta X_t = X_t - X_{t^-} = Y is the jump size at time t.\n\nAs we argued in the case of a Wiener process, the derivation can be motivated using a Taylor series expansion on the function for the continuous part, only adding the jump process, which is orthogonal to the continuous stochastic dynamics. When adding the jump component, the function f(X_t,t) changes discretely, hence the term f(X_{t^-} + \\Delta X_t, t) - f(X_{t^-}, t) instead of the derivative. We only keep in this case the expansion up to dN_t since \\mathbb{E}[dN_t] = \\lambda dt which is already of order O(dN_t), so any extra term in the expansion will have a higher order.\n\nExample: Merton’s jump diffusion model\nOne of the main applications of jump diffusion models in the financial context is to model jumps in financial instruments that otherwise diffuse continuously.\n\nOne famous instance of such models incorporating jumps is Merton’s jump diffusion model \n\nMerton, 1976, which model the dynamics of the asset with a Geometric Brownian Motion with jumps in the returns. The SDE for the asset price S_t is:dS_t = \\mu S_{t^-} \\, dt + \\sigma S_{t^-} \\, dW_t + S_{t^-} (Y - 1) \\, dN_t,\n\nwhere \\mu is the expected return rate, \\sigma is the volatility, Y is a random variable representing the jump multiplier, and dN_t indicates the occurrence of a jump, modeled by a Poisson process with intensity \\lambda.\n\nA typical choice to model jumps in this setup is using a log-normal distribution for the jumps, \\log Y \\sim \\mathcal{N}(k, \\delta^2)\n\nIn order to derive the distribution that follows this process, we can apply Ito’s lemma with jumps to the logarithm of the process, f(S_t) = \\log S_t, as we did analogously with the Geometric Brownian Motion, to which this model reduces in the absence of jumps. Since we have:\\frac{\\partial f}{\\partial t} = 0\\frac{\\partial f}{\\partial S_t} = \\frac{1}{S_t}\\frac{\\partial^2 f}{\\partial S_t^2} = - \\frac{1}{S_t^2}f(S_{t^-} + \\Delta S_t, t) - f(S_{t^-}, t)=\\log (Y S_{t^-}) - \\log (S_{t^-})= \\log Y\n\nThen:d \\log S_t = (\\mu - \\frac{\\sigma^2}{2} )dt + \\sigma dW_t + \\log Y dN_t\n\nTherefore, the logarithm of the process follows a more simple dynamics composed of a Brownian motion with drift plus a jump that does not depend on the magnitude of the process. For Merton, this was a convenient way of introducing jumps in the returns of a financial instrument.\n\nSimulation\n\nSimulating jump diffusion models is relatively simple given what we have learnt so far, since we can proceed step by step by simulating independently the deterministic, the continuous stochastic and the jump components. The following simulation of the Merton model has been generated using such method.\n\n\n\nFigure 13:Simulation of five paths of a jump diffusion process in a discrete grid with 1000 time-steps. The baseline intensity is \\lambda = 1. The Geometric Brownian Motion has drift \\mu = 0.1 and volatility \\sigma = 0.2 For the distribution of jumps we use a log-normal distribution with mean k = -0.1 and standard deviation \\delta = 0.1.\n\nApplications\n\nWe have already mentioned the application of these models to simulating price series with jumps that might happen due to unexpected news or other disruptions in markets like sudden drops of liquidity. These models can be used to improve pricing and risk management of derivatives, or optimal market-making models. We discussed above that another typical application is modeling the jump to default of a corporation or government. Jump diffusion models are then used to model the impact in traded instruments influenced by credit risk, like bonds issued by those entities or credit default swaps linked to them.\n\nFor more theory or applications of jump diffusion processes, a good reference is that or Cont & Tankov \n\nCont & Tankov, 2004.","type":"content","url":"/markdown/stochastic-calculus#jump-diffusion-processes","position":43},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl2":"Exercises"},"type":"lvl2","url":"/markdown/stochastic-calculus#exercises","position":44},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl2":"Exercises"},"content":"Derive the Crank - Nikolson scheme discretization of the inflation targeting model. Simulate it numerically and compare with the exact solution\n\nUse Ito’s lemma to derive the differential of:\n\nf(W_t) = W_t^2\n\nf(W_t) = t W_t\n\nf(W_t) = \\exp(W_t)\n\nSimulate a Brownian motion with drift using the connection to Gaussian Processes.\n\nFind the solution to the recursive differential equation:\\frac{d}{dt} P(N_t = n) = -\\lambda P(N_t = n) + \\lambda P(N_t = n - 1)\n\nusing the method of the generating function, which we introduce as:G(t,s) \\equiv \\sum_{n=0}^{\\infty} P(N_t=n) s^n\n\nHint: multiply the differential equation by s^n and sum over n to rewrite it as a function of the generating function. Solve the equation and express the solution as a Taylor series, matching the coefficients with P(N_t =n)\n\nThe demonstration is relatively straight-forward by computing the characteristic function of a sum of independent random Gaussian variables","type":"content","url":"/markdown/stochastic-calculus#exercises","position":45},{"hierarchy":{"lvl1":"Stochastic optimal control"},"type":"lvl1","url":"/markdown/stochastic-optimal-control","position":0},{"hierarchy":{"lvl1":"Stochastic optimal control"},"content":"","type":"content","url":"/markdown/stochastic-optimal-control","position":1},{"hierarchy":{"lvl1":"Stochastic optimal control","lvl2":"Introduction"},"type":"lvl2","url":"/markdown/stochastic-optimal-control#introduction","position":2},{"hierarchy":{"lvl1":"Stochastic optimal control","lvl2":"Introduction"},"content":"","type":"content","url":"/markdown/stochastic-optimal-control#introduction","position":3},{"hierarchy":{"lvl1":"Stochastic optimal control","lvl2":"Exercises"},"type":"lvl2","url":"/markdown/stochastic-optimal-control#exercises","position":4},{"hierarchy":{"lvl1":"Stochastic optimal control","lvl2":"Exercises"},"content":"","type":"content","url":"/markdown/stochastic-optimal-control#exercises","position":5},{"hierarchy":{"lvl1":"Symbols"},"type":"lvl1","url":"/markdown/symbollist","position":0},{"hierarchy":{"lvl1":"Symbols"},"content":"\\log(x) Natural logarithm of x\n\n\\mathbf{x, y} Vectors are bold lowercase, thus we write a column vector as \\mathbf{x}=[x_1,\\dots,x_n]^T\n\n\\mathbf{X, Y} Matrices are bold uppercase\n\nX, Y Random variables are specified as upper case Roman letters\n\n\\boldsymbol{\\theta} Greek lowercase characters are generally used for model parameters.\n\n\\hat \\theta Point estimate of \\boldsymbol{\\theta}\n\n\\mathbb{E}[X|Y] Expectation of X with respect to Y\n\n\\textrm{var}[X|Y] Variance of X with respect to Y\n\n\\textrm{cov}[X,Y] Covariance matrix of X and Y\n\nX \\sim p Random variable X is distributed as p\n\np(\\cdot) Probability density or probability mass function\n\np(y \\mid \\boldsymbol{x}) Probability (density) of y given \\boldsymbol{x}.\n\n\\mathcal{N}(\\mu, \\sigma^2) A Gaussian (or normal) distribution with mean \\mu and standard deviation \\sigma\n\n\\mathbb{KL}(p \\parallel q) Kullback-Leibler divergence from p to q","type":"content","url":"/markdown/symbollist","position":1},{"hierarchy":{"lvl1":"Trend following strategies"},"type":"lvl1","url":"/markdown/trend-following","position":0},{"hierarchy":{"lvl1":"Trend following strategies"},"content":"","type":"content","url":"/markdown/trend-following","position":1},{"hierarchy":{"lvl1":"Trend following strategies","lvl2":"Introduction"},"type":"lvl2","url":"/markdown/trend-following#introduction","position":2},{"hierarchy":{"lvl1":"Trend following strategies","lvl2":"Introduction"},"content":"","type":"content","url":"/markdown/trend-following#introduction","position":3},{"hierarchy":{"lvl1":"Trend following strategies","lvl2":"Exercises"},"type":"lvl2","url":"/markdown/trend-following#exercises","position":4},{"hierarchy":{"lvl1":"Trend following strategies","lvl2":"Exercises"},"content":"","type":"content","url":"/markdown/trend-following#exercises","position":5},{"hierarchy":{"lvl1":"Bayesian Modelling"},"type":"lvl1","url":"/notebooks/bayesian-theory","position":0},{"hierarchy":{"lvl1":"Bayesian Modelling"},"content":"","type":"content","url":"/notebooks/bayesian-theory","position":1},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl2":"Bayesian probability"},"type":"lvl2","url":"/notebooks/bayesian-theory#bayesian-probability","position":2},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl2":"Bayesian probability"},"content":"","type":"content","url":"/notebooks/bayesian-theory#bayesian-probability","position":3},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl3":"Example: estimating the probability of heads in a coin toss experiment","lvl2":"Bayesian probability"},"type":"lvl3","url":"/notebooks/bayesian-theory#example-estimating-the-probability-of-heads-in-a-coin-toss-experiment","position":4},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl3":"Example: estimating the probability of heads in a coin toss experiment","lvl2":"Bayesian probability"},"content":"\n\n# Generate a sequence of binary random variables\nfrom scipy.stats import bernoulli, beta, uniform\np = 0.3\nr = bernoulli.rvs(p, size=400)\nr[:10]\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# prior distribution: uniform prior\nprior_alpha = 1\nprior_beta = 1\n\n# Now we plot the distribution as we add more data points\nfig, ax = plt.subplots(nrows=3, ncols=3, figsize=(17,17))\nN = np.linspace(0, len(r), 9)\nfor i in range(9):\n  n = N[i]\n  i_x = int(i/3)\n  i_y = i % 3 \n  r_trunc = r[:int(n)]  \n  p_grid = np.linspace(0, 1, 1000)\n  post_alpha = prior_alpha + np.sum(r_trunc)\n  post_beta = prior_beta + len(r_trunc)- np.sum(r_trunc)\n  mean = beta.mean(post_alpha, post_beta)\n  std = beta.std(post_alpha, post_beta)\n  ax[i_x][i_y].plot(p_grid, beta.pdf(p_grid, post_alpha, post_beta))\n  ax[i_x][i_y].set_title(\"N = \" + str(int(n))+ \", est = \" + str(np.round(mean, 3)) + \" +/- \" + str(np.round(std, 3)))\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# prior distribution: non - informative prior (approximation)\nprior_alpha = 0.000001\nprior_beta = 0.000001\n\n# Now we plot the distribution as we add more data points\nfig, ax = plt.subplots(nrows=3, ncols=3, figsize=(17,17))\nN = np.linspace(0, len(r), 9)\nfor i in range(9):\n  n = N[i]\n  i_x = int(i/3)\n  i_y = i % 3 \n  r_trunc = r[:int(n)]  \n  post_alpha = prior_alpha + np.sum(r_trunc)\n  post_beta = prior_beta + len(r_trunc)- np.sum(r_trunc)\n  mean = beta.mean(post_alpha, post_beta)\n  std = beta.std(post_alpha, post_beta)\n  ax[i_x][i_y].plot(p_grid, beta.pdf(p_grid, post_alpha, post_beta))\n  ax[i_x][i_y].set_title(\"N = \" + str(int(n))+ \", est = \" + str(np.round(mean, 3)) + \" +/- \" + str(np.round(std, 3)))\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# prior distribution: wrong confident prior\nprior_alpha = 30\nprior_beta = 30\n\n# Now we plot the distribution as we add more data points\nfig, ax = plt.subplots(nrows=3, ncols=3, figsize=(17,17))\nN = np.linspace(0, len(r), 9)\nfor i in range(9):\n  n = N[i]\n  i_x = int(i/3)\n  i_y = i % 3 \n  r_trunc = r[:int(n)]  \n  post_alpha = prior_alpha + np.sum(r_trunc)\n  post_beta = prior_beta + len(r_trunc)- np.sum(r_trunc)\n  mean = beta.mean(post_alpha, post_beta)\n  std = beta.std(post_alpha, post_beta)\n  ax[i_x][i_y].plot(p_grid, beta.pdf(p_grid, post_alpha, post_beta))\n  ax[i_x][i_y].set_title(\"N = \" + str(int(n))+ \", est = \" + str(np.round(mean, 3)) + \" +/- \" + str(np.round(std, 3)))\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# prior distribution: correct confident prior\nprior_alpha = 18\nprior_beta = 42\n\n# Now we plot the distribution as we add more data points\nfig, ax = plt.subplots(nrows=3, ncols=3, figsize=(17,17))\nN = np.linspace(0, len(r), 9)\nfor i in range(9):\n  n = N[i]\n  i_x = int(i/3)\n  i_y = i % 3 \n  r_trunc = r[:int(n)]  \n  post_alpha = prior_alpha + np.sum(r_trunc)\n  post_beta = prior_beta + len(r_trunc)- np.sum(r_trunc)\n  mean = beta.mean(post_alpha, post_beta)\n  std = beta.std(post_alpha, post_beta)\n  ax[i_x][i_y].plot(p_grid, beta.pdf(p_grid, post_alpha, post_beta))\n  ax[i_x][i_y].set_title(\"N = \" + str(int(n))+ \", est = \" + str(np.round(mean, 3)) + \" +/- \" + str(np.round(std, 3)))\n\n\n\n","type":"content","url":"/notebooks/bayesian-theory#example-estimating-the-probability-of-heads-in-a-coin-toss-experiment","position":5},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl3":"Estimation of Latent Variable Models","lvl2":"Bayesian probability"},"type":"lvl3","url":"/notebooks/bayesian-theory#estimation-of-latent-variable-models","position":6},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl3":"Estimation of Latent Variable Models","lvl2":"Bayesian probability"},"content":"","type":"content","url":"/notebooks/bayesian-theory#estimation-of-latent-variable-models","position":7},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl4":"Expectation Maximization","lvl3":"Estimation of Latent Variable Models","lvl2":"Bayesian probability"},"type":"lvl4","url":"/notebooks/bayesian-theory#expectation-maximization","position":8},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl4":"Expectation Maximization","lvl3":"Estimation of Latent Variable Models","lvl2":"Bayesian probability"},"content":"","type":"content","url":"/notebooks/bayesian-theory#expectation-maximization","position":9},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl5":"Example 1: Gaussian Mixture Models","lvl4":"Expectation Maximization","lvl3":"Estimation of Latent Variable Models","lvl2":"Bayesian probability"},"type":"lvl5","url":"/notebooks/bayesian-theory#example-1-gaussian-mixture-models","position":10},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl5":"Example 1: Gaussian Mixture Models","lvl4":"Expectation Maximization","lvl3":"Estimation of Latent Variable Models","lvl2":"Bayesian probability"},"content":"\n\nfrom scipy.stats import norm\nimport numpy as np\n\nclass TheGoodAndBadDataModel():\n  def __init__(self, prior_p_good, prior_mean, prior_std_good, prior_std_bad):\n    self.p_good = prior_p_good\n    self.mean = prior_mean\n    self.std_good = prior_std_good\n    self.std_bad = prior_std_bad\n    self.loglik_history = []  # store likelihood values\n\n  def predict(self, X):\n    p_x_bad_pbad = (1 - self.p_good) * norm.pdf(X, loc=self.mean, scale=self.std_bad)\n    p_x_good_pgood = self.p_good * norm.pdf(X, loc=self.mean, scale=self.std_good)\n    p_bad_x = p_x_bad_pbad / (p_x_good_pgood + p_x_bad_pbad)\n    return p_bad_x\n\n  def compute_loglik(self, X):\n    mixture_pdf = (\n        self.p_good * norm.pdf(X, loc=self.mean, scale=self.std_good) +\n        (1 - self.p_good) * norm.pdf(X, loc=self.mean, scale=self.std_bad)\n    )\n    return np.sum(np.log(mixture_pdf + 1e-12))  # add epsilon to avoid log(0)\n\n  def learn(self, X, max_iter=1000, tolerance=1e-5, print_error=False, track_likelihood=True):\n    iter = 0\n    while True:\n      iter += 1\n      # E-step\n      p_bad_s = self.predict(X)\n      p_good_s = 1 - p_bad_s\n\n      # M-step\n      p_good_sp1 = np.mean(p_good_s)\n      std_good_sp1 = np.sqrt(np.sum(p_good_s * (X - self.mean)**2) / (len(X) * p_good_sp1))\n      std_bad_sp1 = np.sqrt(np.sum(p_bad_s * (X - self.mean)**2) / (len(X) * (1 - p_good_sp1)))\n\n      # compute change (for stopping)\n      error = np.sqrt(((p_good_sp1 - self.p_good)/self.p_good)**2\n                      + ((std_good_sp1 - self.std_good)/self.std_good)**2\n                      + ((std_bad_sp1 - self.std_bad)/self.std_bad)**2)\n\n      # update parameters\n      self.p_good = p_good_sp1\n      self.std_good = std_good_sp1\n      self.std_bad = std_bad_sp1\n\n      # track log-likelihood\n      if track_likelihood:\n        ll = self.compute_loglik(X)\n        self.loglik_history.append(ll)\n        if print_error:\n          print(f\"Iter {iter}: error={error:.6f}, loglik={ll:.6f}\")\n\n      # stopping condition\n      if (error < tolerance or iter >= max_iter):\n        break\n\n\n\n\nimport yfinance as yf\nimport numpy as np\n\n# Define ticker\nbbva_tkr = yf.Ticker(\"BBVA.MC\")\n\n# Get 10 years of daily adjusted close prices\nend_date = \"2025-07-31\"\nstart_date = \"2015-07-31\"  # 10 years earlier\ndata = bbva_tkr.history(start=start_date, end=end_date, interval=\"1d\")[\"Close\"]\n\ndata = bbva_tkr.history(period=\"10y\", interval=\"1d\")[\"Close\"]\n\n# Calculate daily returns\nbbva_ret = data.pct_change().dropna()\n\n# Print mean and standard deviation of returns\nprint(\"Mean return:\", np.mean(bbva_ret))\nprint(\"Std deviation:\", np.std(bbva_ret))\n\n\n\n# We learn the model over the historical data. As a prior we assume there are only 10% anomalies in the dataset\n# The result is relatively robust to the choice of prior\ngbdm = TheGoodAndBadDataModel(0.99, np.mean(bbva_ret), np.std(bbva_ret), 2*np.std(bbva_ret))\ngbdm.learn(bbva_ret)\n\n# We have a look at the results: according to the model, there are 15% anomalies, with roughly a 3x standard deviation\n# This means the model detects that the distribution of returns is not accurately described by a single Gaussian\nprint(gbdm.p_good, gbdm.mean, gbdm.std_good, gbdm.std_bad)\n\n\n\n# Plot the log-likelihood history\nimport matplotlib.pyplot as plt\nplt.plot(gbdm.loglik_history, marker=\"o\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Log-likelihood\")\nplt.title(\"EM Log-likelihood evolution\")\nplt.show()\n\n\n\nimport matplotlib.pyplot as plt\n\nbbva_ret.hist(bins=50, figsize=(8,5))\nplt.xlabel(\"Daily Return\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram of BBVA Daily Returns (10y)\")\nplt.show()\n\n\n\n# Let us flag anomalies as those with 50% of more probability of belonging to the \"bad data\" mixture component\nanomalies = (gbdm.predict(bbva_ret) > 0.5)\n# Add anomaly flag to DataFrame\npd_bbva_ret = bbva_ret.reset_index()\npd_bbva_ret.columns = [\"Date\", \"Returns\"]\npd_bbva_ret[\"anomaly\"] = anomalies\n\n# Plot histogram of returns, segmented by anomaly flag\npd_bbva_ret[pd_bbva_ret[\"anomaly\"] == True][\"Returns\"].hist(\n    bins=100, alpha=0.7, color=\"red\", label=\"Anomalies\")\npd_bbva_ret[pd_bbva_ret[\"anomaly\"] == False][\"Returns\"].hist(\n    bins=50, alpha=0.7, color=\"blue\", label=\"Normal\")\n\nplt.xlabel(\"Daily Return\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram of BBVA Daily Returns\")\nplt.legend()\nplt.show()\n\n\n\n\n# In terms of time-series, we flat the anomalies in the following plot\ncolors = {False: \"blue\", True: \"red\"}\npd_bbva_ret.reset_index().plot.scatter(x = \"Date\", y = \"Returns\", c = pd_bbva_ret[\"anomaly\"].map(colors).values, title = \"Time Series of BBVA Daily Returns\")\n\n\n\n\n\n# An interesting question is whether these anomalies tend to cluster. Looking at the auto-correlation it hints this is a possibility\n# This means it could make sense to analyse these anomalies as regime changes, i.e. use a hidden markov model\nfrom pandas.plotting import autocorrelation_plot\nax = autocorrelation_plot(pd_bbva_ret[\"anomaly\"])\nax.set_xlim([0, 100])\n\n\n\n\n\n# Check that relevant periods of financial stress have been correctly classified\npd_bbva_ret[\"Date\"] = pd.to_datetime(pd_bbva_ret[\"Date\"]).dt.date\n\n# Define event dates\nevent_dates = [\n    pd.to_datetime(\"2020-03-16\").date(),  # COVID lockdown Spain\n    pd.to_datetime(\"2016-06-24\").date(),  # Brexit referendum (first trading day)\n    pd.to_datetime(\"2016-11-09\").date()   # Trump election (first trading day after)\n]\n\n# Filter rows that match event dates\nevents_df = pd_bbva_ret[pd_bbva_ret[\"Date\"].isin(event_dates)][[\"Date\", \"Returns\", \"anomaly\"]]\n\nprint(events_df)\n\n\n\n\n","type":"content","url":"/notebooks/bayesian-theory#example-1-gaussian-mixture-models","position":11},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl5":"Example 3: Local Level Model","lvl4":"Expectation Maximization","lvl3":"Estimation of Latent Variable Models","lvl2":"Bayesian probability"},"type":"lvl5","url":"/notebooks/bayesian-theory#example-3-local-level-model","position":12},{"hierarchy":{"lvl1":"Bayesian Modelling","lvl5":"Example 3: Local Level Model","lvl4":"Expectation Maximization","lvl3":"Estimation of Latent Variable Models","lvl2":"Bayesian probability"},"content":"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n%matplotlib inline\n\n\n\n\ndef simulate_local_level(T=200, sigma_v2=1.0, sigma_w2=0.05, seed=42):\n    rng = np.random.default_rng(seed)\n    y = np.zeros(T)\n    x = np.zeros(T)\n    y[0] = rng.normal(0.0, np.sqrt(sigma_w2))\n    x[0] = y[0] + rng.normal(0.0, np.sqrt(sigma_v2))\n    for t in range(1, T):\n        y[t] = y[t-1] + rng.normal(0.0, np.sqrt(sigma_w2))\n        x[t] = y[t] + rng.normal(0.0, np.sqrt(sigma_v2))\n    return x, y\n\n\n\ndef kf_rts_identity_cross(x, sigma_v2, sigma_w2, m0=0.0, P0=1e6):\n    T = len(x)\n    m_pred = np.zeros(T)\n    P_pred = np.zeros(T)\n    m_filt = np.zeros(T)\n    P_filt = np.zeros(T)\n    K = np.zeros(T)\n    innov = np.zeros(T)\n    S = np.zeros(T)\n\n    m_prev, P_prev = m0, P0\n    loglik = 0.0\n    for t in range(T):\n        # predict\n        m_pred[t] = m_prev\n        P_pred[t] = P_prev + sigma_w2\n        # update\n        innov[t] = x[t] - m_pred[t]\n        S[t] = P_pred[t] + sigma_v2\n        K[t] = P_pred[t] / S[t]\n        m_filt[t] = m_pred[t] + K[t] * innov[t]\n        P_filt[t] = (1 - K[t]) * P_pred[t]\n        # log-likelihood\n        loglik += -0.5 * (np.log(2*np.pi*S[t]) + innov[t]**2 / S[t])\n        m_prev, P_prev = m_filt[t], P_filt[t]\n\n    # RTS smoother\n    m_smooth = np.zeros(T)\n    P_smooth = np.zeros(T)\n    J = np.zeros(T-1)\n    m_smooth[-1] = m_filt[-1]\n    P_smooth[-1] = P_filt[-1]\n    for t in range(T-2, -1, -1):\n        J[t] = P_filt[t] / P_pred[t+1]\n        m_smooth[t] = m_filt[t] + J[t] * (m_smooth[t+1] - m_pred[t+1])\n        P_smooth[t] = P_filt[t] + J[t]**2 * (P_smooth[t+1] - P_pred[t+1])\n\n    # lag-one smoothed covariance via identity: P_{t-1,t|T} = J_{t-1} P_{t|T}\n    P_cross = np.zeros(T-1)\n    for t in range(1, T):\n        P_cross[t-1] = J[t-1] * P_smooth[t]\n\n    return {\n        \"m_pred\": m_pred, \"P_pred\": P_pred,\n        \"m_filt\": m_filt, \"P_filt\": P_filt,\n        \"m_smooth\": m_smooth, \"P_smooth\": P_smooth,\n        \"innov\": innov, \"S\": S, \"K\": K, \"J\": J,\n        \"P_cross\": P_cross, \"loglik\": loglik\n    }\n\n\n\ndef em_local_level(x, sigma_v2_init=2.0, sigma_w2_init=0.2, m0=0.0, P0=1e6, max_iter=500, tol=1e-8):\n    sigma_v2, sigma_w2 = float(sigma_v2_init), float(sigma_w2_init)\n    ll_hist = []\n    for it in range(max_iter):\n        out = kf_rts_identity_cross(x, sigma_v2, sigma_w2, m0, P0)\n        mu, P, Pc = out[\"m_smooth\"], out[\"P_smooth\"], out[\"P_cross\"]\n        ll_hist.append(out[\"loglik\"])\n\n        # E-step expectations\n        E1 = (x - mu)**2 + P\n        diff_mu = mu[1:] - mu[:-1]\n        E2 = diff_mu**2 + P[1:] + P[:-1] - 2.0 * Pc\n\n        # M-step (MLE scaling)\n        sigma_v2_new = max(np.mean(E1), 1e-12)\n        sigma_w2_new = max(np.mean(E2), 1e-12)\n\n        # convergence\n        rel = max(abs(sigma_v2_new - sigma_v2) / (sigma_v2 + 1e-12),\n                  abs(sigma_w2_new - sigma_w2) / (sigma_w2 + 1e-12))\n        sigma_v2, sigma_w2 = sigma_v2_new, sigma_w2_new\n        if rel < tol:\n            ll_hist.append(kf_rts_identity_cross(x, sigma_v2, sigma_w2, m0, P0)[\"loglik\"])\n            break\n    return {\"sigma_v2\": sigma_v2, \"sigma_w2\": sigma_w2, \"ll_history\": np.array(ll_hist)}\n\n\n\n\n\n\n# --- Configuration ---\nT = 200\nsigma_v2_true = 1.0\nsigma_w2_true = 0.05\nseed = 42\n\n# EM seeds\nsigma_v2_init = 2.0\nsigma_w2_init = 0.2\n\n# --- Simulate ---\nx, y = simulate_local_level(T, sigma_v2_true, sigma_w2_true, seed)\n\n# --- Run EM ---\nem = em_local_level(x, sigma_v2_init=sigma_v2_init, sigma_w2_init=sigma_w2_init, max_iter=1000, tol=1e-5)\nsigma_v2_hat, sigma_w2_hat = em[\"sigma_v2\"], em[\"sigma_w2\"]\nll = em[\"ll_history\"]\n\n# --- Smooth with estimated params ---\nout = kf_rts_identity_cross(x, sigma_v2_hat, sigma_w2_hat)\n\n# --- Compare parameters ---\ndf = pd.DataFrame({\n    \"Parameter\": [r\"$\\sigma_v^2$\", r\"$\\sigma_w^2$\"],\n    \"True\": [sigma_v2_true, sigma_w2_true],\n    \"EM estimate\": [sigma_v2_hat, sigma_w2_hat],\n})\ndf\n\n\n\n\n# Log-likelihood (should be non-decreasing up to numerical noise)\nplt.figure()\nplt.plot(ll, marker=\"o\")\nplt.title(\"EM log-likelihood over iterations\")\nplt.xlabel(\"iteration\"); plt.ylabel(\"log-likelihood\")\nplt.tight_layout()\nplt.show()\n\n# Data and smoothed state with ±1σ band\nt = np.arange(len(x))\nm = out[\"m_smooth\"]\nstd = np.sqrt(out[\"P_smooth\"])\n\nplt.figure()\nplt.plot(x, label=\"observations $x_t$\")\nplt.plot(y, label=\"true state $y_t$\")\nplt.plot(m, label=\"smoothed mean $\\hat{y}_{t|T}$\")\nplt.fill_between(t, m - 2*std, m + 2*std, alpha=0.3, facecolor=\"tab:green\", \n                  edgecolor=\"none\", zorder=0,label=\"smoother ±2σ\")\nplt.legend()\nplt.title(\"Local level model simulation and estimation\")\nplt.xlabel(\"t\"); plt.ylabel(\"value\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n","type":"content","url":"/notebooks/bayesian-theory#example-3-local-level-model","position":13},{"hierarchy":{"lvl1":"Fair Price Estimation"},"type":"lvl1","url":"/notebooks/fair-price-estimation","position":0},{"hierarchy":{"lvl1":"Fair Price Estimation"},"content":"\n\n","type":"content","url":"/notebooks/fair-price-estimation","position":1},{"hierarchy":{"lvl1":"Fair Price Estimation"},"type":"lvl1","url":"/notebooks/fair-price-estimation#fair-price-estimation","position":2},{"hierarchy":{"lvl1":"Fair Price Estimation"},"content":"\n\n","type":"content","url":"/notebooks/fair-price-estimation#fair-price-estimation","position":3},{"hierarchy":{"lvl1":"Fair Price Estimation","lvl2":"Pricing of flow products"},"type":"lvl2","url":"/notebooks/fair-price-estimation#pricing-of-flow-products","position":4},{"hierarchy":{"lvl1":"Fair Price Estimation","lvl2":"Pricing of flow products"},"content":"","type":"content","url":"/notebooks/fair-price-estimation#pricing-of-flow-products","position":5},{"hierarchy":{"lvl1":"Fair Price Estimation","lvl3":"The Kalman Filter model for pricing","lvl2":"Pricing of flow products"},"type":"lvl3","url":"/notebooks/fair-price-estimation#the-kalman-filter-model-for-pricing","position":6},{"hierarchy":{"lvl1":"Fair Price Estimation","lvl3":"The Kalman Filter model for pricing","lvl2":"Pricing of flow products"},"content":"\n\n# Compromise execution: run full algorithm but with fewer timesteps/day so it completes here.\n# Use 22 days × 60 steps/day = 1320 timesteps (faster but pattern preserved).\nimport numpy as np\nimport matplotlib.pyplot as plt\nnp.random.seed(100)\n\ndef make_obs_pattern(days=22, steps_per_day=60):\n    pattern = []\n    for _ in range(days):\n        base = steps_per_day // 3\n        rem = steps_per_day - 3 * base\n        for _ in range(base): pattern.append(np.array([1, 0], dtype=bool))\n        for _ in range(base): pattern.append(np.array([1, 1], dtype=bool))\n        for _ in range(base + rem): pattern.append(np.array([0, 1], dtype=bool))\n    return np.array(pattern)\n\ndef simulate_continuous_trades(Q, R, obs_pattern):\n    # Q is the covariance matrix of the mid-price process\n    # R is the covariance matrix of the observation process\n    T = len(obs_pattern); n = Q.shape[0]\n    mids = np.zeros((T, n)); trades = np.zeros((T,n))\n    for t in range(1, T):\n        mids[t] = mids[t-1] + np.random.multivariate_normal(np.zeros(n), Q)\n    for t in range(T):\n        trades[t] = mids[t] + np.random.multivariate_normal(np.zeros(n), R)\n    return mids, trades\n\ndef kalman_filter_timevarying(y, R_time, A, Q, x0=None, P0=None):\n    T = y.shape[0]; n = A.shape[0]\n    xs = np.zeros((T, n)); Ps = np.zeros((T, n, n))\n    if x0 is None: x0 = np.zeros(n)\n    if P0 is None: P0 = np.eye(n)\n    x_pred = x0.copy(); P_pred = P0.copy()\n    for t in range(T):\n        R_t = R_time[t]\n        S = P_pred + R_t + np.eye(n)*1e-12\n        K = P_pred @ np.linalg.inv(S)\n        innov = y[t] - x_pred\n        x_upd = x_pred + K @ innov\n        P_upd = (np.eye(n) - K) @ P_pred\n        xs[t] = x_upd; Ps[t] = P_upd\n        x_pred = A @ x_upd\n        P_pred = A @ P_upd @ A.T + Q\n    return xs, Ps\n\ndef rts_smoother(xs, Ps, A, Q):\n    T, n = xs.shape\n    x_s = np.zeros_like(xs); P_s = np.zeros_like(Ps)\n    x_s[-1] = xs[-1].copy(); P_s[-1] = Ps[-1].copy()\n    for t in range(T-2, -1, -1):\n        P_pred = A @ Ps[t] @ A.T + Q + np.eye(n)*1e-12\n        J = Ps[t] @ A.T @ np.linalg.inv(P_pred)\n        x_s[t] = xs[t] + J @ (x_s[t+1] - A @ xs[t])\n        P_s[t] = Ps[t] + J @ (P_s[t+1] - P_pred) @ J.T\n    return x_s, P_s\n\ndef em_timevaryingR(y, obs_pattern, A, large_R=1e8, max_iter=25, tol=1e-6):\n    T, n = y.shape\n    Q = np.eye(n) * 1e-4\n    R_diag = np.array([1e-3, 2e-3])\n    prev_ll = -np.inf; history = []\n    for it in range(max_iter):\n        R_time = np.zeros((T,n,n))\n        for t in range(T):\n            diag = np.array([R_diag[i] if obs_pattern[t,i] else large_R for i in range(n)])\n            R_time[t] = np.diag(diag)\n        xs_filt, Ps_filt = kalman_filter_timevarying(y, R_time, A, Q)\n        xs_smooth, Ps_smooth = rts_smoother(xs_filt, Ps_filt, A, Q)\n        Exx = Ps_smooth + np.einsum('ti,tj->tij', xs_smooth, xs_smooth)\n        Exx_lag = np.zeros((T-1,n,n))\n        for t in range(1,T):\n            P_pred = A @ Ps_filt[t-1] @ A.T + Q\n            J = Ps_filt[t-1] @ A.T @ np.linalg.inv(P_pred + np.eye(n)*1e-12)\n            P_t_t1 = J @ Ps_smooth[t]\n            Exx_lag[t-1] = P_t_t1 + np.outer(xs_smooth[t], xs_smooth[t-1])\n        sumQ = np.zeros((n,n))\n        for t in range(1,T):\n            sumQ += Exx[t] - Exx_lag[t-1] - Exx_lag[t-1].T + Exx[t-1]\n        Q_new = (sumQ / (T-1) + sumQ.T / (T-1)) / 2\n        R_new = np.zeros(n); counts = np.zeros(n,dtype=int)\n        for t in range(T):\n            for i in range(n):\n                if obs_pattern[t,i]:\n                    y_i = y[t,i]; Ex_i = xs_smooth[t,i]; Exx_ii = Exx[t,i,i]\n                    R_new[i] += (y_i*y_i - 2*y_i*Ex_i + Exx_ii)\n                    counts[i] += 1\n        for i in range(n):\n            if counts[i]>0:\n                R_new[i] = R_new[i] / counts[i]\n            else:\n                R_new[i] = R_diag[i]\n        Q_new += np.eye(n)*1e-12\n        R_new = np.maximum(R_new, 1e-12)\n        Q = 0.7*Q + 0.3*Q_new\n        R_diag = 0.7*R_diag + 0.3*R_new\n        history.append((Q.copy(), R_diag.copy()))\n        # approx ll\n        ll = 0.0; x_pred = np.zeros(n); P_pred = np.eye(n)\n        for t in range(T):\n            R_t = np.diag([R_diag[i] if obs_pattern[t,i] else large_R for i in range(n)])\n            S = P_pred + R_t + np.eye(n)*1e-12\n            innov = y[t] - x_pred\n            invS = np.linalg.inv(S)\n            ll += -0.5*(np.log(np.linalg.det(2*np.pi*S)) + innov.T @ invS @ innov)\n            K = P_pred @ invS\n            x_upd = x_pred + K @ innov\n            P_upd = (np.eye(n) - K) @ P_pred\n            x_pred = A @ x_upd\n            P_pred = A @ P_upd @ A.T + Q\n        if it>0 and abs(ll - prev_ll) < tol:\n            break\n        prev_ll = ll\n    return Q, np.diag(R_diag), history, ll\n\n# Run with 22 days x 60 steps/day\ndays = 22; steps_per_day = 60\nobs_pattern = make_obs_pattern(days, steps_per_day); T = len(obs_pattern)\n# Parameters\nrho = 0.9   # correlation\nsigma1 = np.sqrt(5e-4)   # std of instrument 1\nsigma2 = np.sqrt(4e-4)   # std of instrument 2\n# Construct covariance matrix\nQ_true = np.array([\n    [sigma1**2, rho * sigma1 * sigma2],\n    [rho * sigma1 * sigma2, sigma2**2]\n]) # We make transition covariance non-diagonal\nR_true = np.diag([1e-3,2e-3]) # We make observation covariance diagonal\nmids_true, trades = simulate_continuous_trades(Q_true, R_true, obs_pattern)\nT_half = T//2; y_train = trades[:T_half]; y_test = trades[T_half:]\nobs_train = obs_pattern[:T_half]; obs_test = obs_pattern[T_half:]\nmids_test = mids_true[T_half:]; A = np.eye(2)\n\nQ_est, R_est, history, ll_final = em_timevaryingR(y_train, obs_train, A, large_R=1e8, max_iter=25, tol=1e-5)\ndef build_R_time_from_diag(R_diag, obs_pattern, large_R=1e8):\n    T = len(obs_pattern); n = len(R_diag); R_time = np.zeros((T,n,n))\n    for t in range(T):\n        diag = np.array([R_diag[i] if obs_pattern[t,i] else large_R for i in range(n)])\n        R_time[t] = np.diag(diag)\n    return R_time\nR_time_test = build_R_time_from_diag(np.diag(R_est), obs_test, large_R=1e8)\nmids_est, _ = kalman_filter_timevarying(y_test, R_time_test, A, Q_est)\nrmse = np.sqrt(np.mean((mids_est - mids_test)**2, axis=0))\n\nprint(\"T (timesteps):\", T)\nprint(\"True Q:\\n\", Q_true)\nprint(\"Estimated Q:\\n\", Q_est)\nprint(\"True R diag:\", np.diag(R_true))\nprint(\"Estimated R diag:\", np.diag(R_est))\nprint(\"Final approx loglik:\", ll_final)\nprint(\"RMSE on second half:\", rmse)\n\n# Plot window without showing trades when market closed\nplt.figure(figsize=(14, 8))\nw0 = 200\nw1 = min(w0 + 200, y_test.shape[0])\nt = np.arange(T_half + w0, T_half + w1)\n\n# Rerun filter on test to get covariance sequence for error bars\nmids_est, Ps_est = kalman_filter_timevarying(y_test, R_time_test, A, Q_est)\n\n# Extract ±1 std from diagonal of P (filter uncertainty)\nstds = np.sqrt(np.array([np.diag(P) for P in Ps_est]))\n\n# --- Instrument 1 ---\nplt.subplot(2, 1, 1)\nplt.plot(t, mids_test[w0:w1, 0], label='True mid 1', lw=2)\nplt.plot(t, mids_est[w0:w1, 0], label='Estimated mid 1', lw=1.5, ls='--')\nplt.fill_between(\n    t,\n    mids_est[w0:w1, 0] - stds[w0:w1, 0],\n    mids_est[w0:w1, 0] + stds[w0:w1, 0],\n    color='gray', alpha=0.2, label='±1 std (KF)'\n)\nmask_plot = obs_test[w0:w1, 0]\nplt.scatter(t[mask_plot], y_test[w0:w1, 0][mask_plot],\n            s=8, label='Trades 1 (open)', alpha=0.6)\nplt.title('Instrument 1')\nplt.legend()\n\n# --- Instrument 2 ---\nplt.subplot(2, 1, 2)\nplt.plot(t, mids_test[w0:w1, 1], label='True mid 2', lw=2)\nplt.plot(t, mids_est[w0:w1, 1], label='Estimated mid 2', lw=1.5, ls='--')\nplt.fill_between(\n    t,\n    mids_est[w0:w1, 1] - stds[w0:w1, 1],\n    mids_est[w0:w1, 1] + stds[w0:w1, 1],\n    color='gray', alpha=0.2, label='±1 std (KF)'\n)\nmask_plot1 = obs_test[w0:w1, 1]\nplt.scatter(t[mask_plot1], y_test[w0:w1, 1][mask_plot1],\n            s=8, label='Trades 2 (open)', alpha=0.6)\nplt.title('Instrument 2')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n","type":"content","url":"/notebooks/fair-price-estimation#the-kalman-filter-model-for-pricing","position":7},{"hierarchy":{"lvl1":"Fair Price Estimation","lvl2":"The utility indifference theory for derivatives pricing"},"type":"lvl2","url":"/notebooks/fair-price-estimation#the-utility-indifference-theory-for-derivatives-pricing","position":8},{"hierarchy":{"lvl1":"Fair Price Estimation","lvl2":"The utility indifference theory for derivatives pricing"},"content":"\n\n","type":"content","url":"/notebooks/fair-price-estimation#the-utility-indifference-theory-for-derivatives-pricing","position":9},{"hierarchy":{"lvl1":"Fair Price Estimation","lvl3":"Call option premium using the utility indifference principle","lvl2":"The utility indifference theory for derivatives pricing"},"type":"lvl3","url":"/notebooks/fair-price-estimation#call-option-premium-using-the-utility-indifference-principle","position":10},{"hierarchy":{"lvl1":"Fair Price Estimation","lvl3":"Call option premium using the utility indifference principle","lvl2":"The utility indifference theory for derivatives pricing"},"content":"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Black-Scholes formula with drift implementation\ndef black_scholes_with_drift(S, K, T, t, r, mu, sigma):\n    d1_mu = (np.log(S / K) + (mu + 0.5 * sigma**2) * (T - t)) / (sigma * np.sqrt(T - t))\n    d2_mu = d1_mu - sigma * np.sqrt(T - t)\n    call_price_with_drift = S * np.exp((mu - r) * (T - t)) * norm.cdf(d1_mu) - K * np.exp(-r * (T - t)) * norm.cdf(d2_mu)\n    return call_price_with_drift\n\n# Parameters for the plots\nS_t = 100   # Current stock price\nK = 100     # Strike price\nT = 1       # Time to maturity (1 year)\nt = 0       # Current time (now)\nr = 0.05    # Risk-free interest rate (5%)\nmu = 0.1    # Drift rate (10%)\nsigma = 0.2 # Volatility (20%)\n\n# Generate data for each dependency plot\nS_values = np.linspace(50, 150, 400)\nK_values = np.linspace(50, 150, 400)\nT_values = np.linspace(0.01, 2, 400)\nr_values = np.linspace(0, 0.2, 400)\nsigma_values = np.linspace(0.01, 1, 400)\nmu_values = np.linspace(-0.1, 0.3, 400)\n\n# Calculate call prices with drift\nC_S_drift = [black_scholes_with_drift(S, K, T, t, r, mu, sigma) for S in S_values]\nC_K_drift = [black_scholes_with_drift(S_t, K, T, t, r, mu, sigma) for K in K_values]\nC_T_drift = [black_scholes_with_drift(S_t, K, T, t, r, mu, sigma) for T in T_values]\nC_r_drift = [black_scholes_with_drift(S_t, K, T, t, r, mu, sigma) for r in r_values]\nC_sigma_drift = [black_scholes_with_drift(S_t, K, T, t, r, mu, sigma) for sigma in sigma_values]\nC_mu_drift = [black_scholes_with_drift(S_t, K, T, t, r, mu, sigma) for mu in mu_values]\n\n# Plotting all dependencies in a single figure for export\n\nfig, axs = plt.subplots(3, 2, figsize=(15, 18))\n\n# Current Stock Price (S) with Drift\naxs[0, 0].plot(S_values, C_S_drift)\naxs[0, 0].set_title('Call Option Price vs Current Stock Price (S) with Drift')\naxs[0, 0].set_xlabel('Current Stock Price (S)')\naxs[0, 0].set_ylabel('Call Option Price (P)')\naxs[0, 0].grid(True)\n\n# Strike Price (K) with Drift\naxs[0, 1].plot(K_values, C_K_drift)\naxs[0, 1].set_title('Call Option Price vs Strike Price (K) with Drift')\naxs[0, 1].set_xlabel('Strike Price (K)')\naxs[0, 1].set_ylabel('Call Option Price (P)')\naxs[0, 1].grid(True)\n\n# Time to Maturity (T) with Drift\naxs[1, 0].plot(T_values, C_T_drift)\naxs[1, 0].set_title('Call Option Price vs Time to Maturity (T) with Drift')\naxs[1, 0].set_xlabel('Time to Maturity (T)')\naxs[1, 0].set_ylabel('Call Option Price (P)')\naxs[1, 0].grid(True)\n\n# Risk-Free Interest Rate (r) with Drift\naxs[1, 1].plot(r_values, C_r_drift)\naxs[1, 1].set_title('Call Option Price vs Risk-Free Interest Rate (r) with Drift')\naxs[1, 1].set_xlabel('Risk-Free Interest Rate (r)')\naxs[1, 1].set_ylabel('Call Option Price (P)')\naxs[1, 1].grid(True)\n\n# Volatility (σ) with Drift\naxs[2, 0].plot(sigma_values, C_sigma_drift)\naxs[2, 0].set_title('Call Option Price vs Volatility (σ) with Drift')\naxs[2, 0].set_xlabel('Volatility (σ)')\naxs[2, 0].set_ylabel('Call Option Price (P)')\naxs[2, 0].grid(True)\n\n# Drift (μ)\naxs[2, 1].plot(mu_values, C_mu_drift)\naxs[2, 1].set_title('Call Option Price vs Drift (μ)')\naxs[2, 1].set_xlabel('Drift (μ)')\naxs[2, 1].set_ylabel('Call Option Price (P)')\naxs[2, 1].grid(True)\n\n\n\n","type":"content","url":"/notebooks/fair-price-estimation#call-option-premium-using-the-utility-indifference-principle","position":11},{"hierarchy":{"lvl1":"Fair Price Estimation","lvl2":"The arbitrage-free theory of derivatives pricing"},"type":"lvl2","url":"/notebooks/fair-price-estimation#the-arbitrage-free-theory-of-derivatives-pricing","position":12},{"hierarchy":{"lvl1":"Fair Price Estimation","lvl2":"The arbitrage-free theory of derivatives pricing"},"content":"\n\n","type":"content","url":"/notebooks/fair-price-estimation#the-arbitrage-free-theory-of-derivatives-pricing","position":13},{"hierarchy":{"lvl1":"Fair Price Estimation","lvl3":"Call option premium using arbitrage-free theory","lvl2":"The arbitrage-free theory of derivatives pricing"},"type":"lvl3","url":"/notebooks/fair-price-estimation#call-option-premium-using-arbitrage-free-theory","position":14},{"hierarchy":{"lvl1":"Fair Price Estimation","lvl3":"Call option premium using arbitrage-free theory","lvl2":"The arbitrage-free theory of derivatives pricing"},"content":"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Black-Scholes formula with drift implementation\ndef black_scholes_with_drift(S, K, T, t, r, mu, sigma):\n    d1_mu = (np.log(S / K) + (mu + 0.5 * sigma**2) * (T - t)) / (sigma * np.sqrt(T - t))\n    d2_mu = d1_mu - sigma * np.sqrt(T - t)\n    call_price_with_drift = S * np.exp((mu - r) * (T - t)) * norm.cdf(d1_mu) - K * np.exp(-r * (T - t)) * norm.cdf(d2_mu)\n    return call_price_with_drift\n\n# Standard Black-Scholes formula (mu = r)\ndef black_scholes_standard(S, K, T, t, r, sigma):\n    return black_scholes_with_drift(S, K, T, t, r, r, sigma)\n\n# Parameters for the plots\nS_t = 100   # Current stock price\nK = 100     # Strike price\nT = 1       # Time to maturity (1 year)\nt = 0       # Current time (now)\nr = 0.05    # Risk-free interest rate (5%)\nmu = 0.1    # Drift rate (10%)\nsigma = 0.2 # Volatility (20%)\n\n# Generate data for each dependency plot\nS_values = np.linspace(50, 150, 400)\nK_values = np.linspace(50, 150, 400)\nT_values = np.linspace(0.01, 2, 400)\nr_values = np.linspace(0, 0.2, 400)\nsigma_values = np.linspace(0.01, 1, 400)\nmu_values = np.linspace(-0.1, 0.3, 400)\n\n# Calculate call prices with drift\nC_S_drift = [black_scholes_with_drift(S, K, T, t, r, mu, sigma) for S in S_values]\nC_K_drift = [black_scholes_with_drift(S_t, K, T, t, r, mu, sigma) for K in K_values]\nC_T_drift = [black_scholes_with_drift(S_t, K, T, t, r, mu, sigma) for T in T_values]\nC_r_drift = [black_scholes_with_drift(S_t, K, T, t, r, mu, sigma) for r in r_values]\nC_sigma_drift = [black_scholes_with_drift(S_t, K, T, t, r, mu, sigma) for sigma in sigma_values]\nC_mu_drift = [black_scholes_with_drift(S_t, K, T, t, r, mu, sigma) for mu in mu_values]\n\n# Calculate call prices using standard Black-Scholes formula (mu = r)\nC_S_standard = [black_scholes_standard(S, K, T, t, r, sigma) for S in S_values]\nC_K_standard = [black_scholes_standard(S_t, K, T, t, r, sigma) for K in K_values]\nC_T_standard = [black_scholes_standard(S_t, K, T, t, r, sigma) for T in T_values]\nC_r_standard = [black_scholes_standard(S_t, K, T, t, r, sigma) for r in r_values]\nC_sigma_standard = [black_scholes_standard(S_t, K, T, t, r, sigma) for sigma in sigma_values]\nC_mu_standard = [black_scholes_standard(S_t, K, T, t, r, sigma) for mu in mu_values]\n\n# Plotting all dependencies in a single figure for comparison\nfig, axs = plt.subplots(3, 2, figsize=(15, 18))\n\n# Current Stock Price (S) comparison\naxs[0, 0].plot(S_values, C_S_drift, label=\"With Drift (mu = 0.1)\")\naxs[0, 0].plot(S_values, C_S_standard, label=\"Standard (mu = r)\", linestyle=\"--\")\naxs[0, 0].set_title('Call Option Price vs Current Stock Price (S)')\naxs[0, 0].set_xlabel('Current Stock Price (S)')\naxs[0, 0].set_ylabel('Call Option Price (P)')\naxs[0, 0].legend()\naxs[0, 0].grid(True)\n\n# Strike Price (K) comparison\naxs[0, 1].plot(K_values, C_K_drift, label=\"With Drift (mu = 0.1)\")\naxs[0, 1].plot(K_values, C_K_standard, label=\"Standard (mu = r)\", linestyle=\"--\")\naxs[0, 1].set_title('Call Option Price vs Strike Price (K)')\naxs[0, 1].set_xlabel('Strike Price (K)')\naxs[0, 1].set_ylabel('Call Option Price (P)')\naxs[0, 1].legend()\naxs[0, 1].grid(True)\n\n# Time to Maturity (T) comparison\naxs[1, 0].plot(T_values, C_T_drift, label=\"With Drift (mu = 0.1)\")\naxs[1, 0].plot(T_values, C_T_standard, label=\"Standard (mu = r)\", linestyle=\"--\")\naxs[1, 0].set_title('Call Option Price vs Time to Maturity (T)')\naxs[1, 0].set_xlabel('Time to Maturity (T)')\naxs[1, 0].set_ylabel('Call Option Price (P)')\naxs[1, 0].legend()\naxs[1, 0].grid(True)\n\n# Risk-Free Interest Rate (r) comparison\naxs[1, 1].plot(r_values, C_r_drift, label=\"With Drift (mu = 0.1)\")\naxs[1, 1].plot(r_values, C_r_standard, label=\"Standard (mu = r)\", linestyle=\"--\")\naxs[1, 1].set_title('Call Option Price vs Risk-Free Interest Rate (r)')\naxs[1, 1].set_xlabel('Risk-Free Interest Rate (r)')\naxs[1, 1].set_ylabel('Call Option Price (P)')\naxs[1, 1].legend()\naxs[1, 1].grid(True)\n\n# Volatility (σ) comparison\naxs[2, 0].plot(sigma_values, C_sigma_drift, label=\"With Drift (mu = 0.1)\")\naxs[2, 0].plot(sigma_values, C_sigma_standard, label=\"Standard (mu = r)\", linestyle=\"--\")\naxs[2, 0].set_title('Call Option Price vs Volatility (σ)')\naxs[2, 0].set_xlabel('Volatility (σ)')\naxs[2, 0].set_ylabel('Call Option Price (P)')\naxs[2, 0].legend()\naxs[2, 0].grid(True)\n\n# Drift (μ) comparison\naxs[2, 1].plot(mu_values, C_mu_drift, label=\"With Drift (mu = varying)\")\naxs[2, 1].plot(mu_values, C_mu_standard, label=\"Standard (mu = r)\", linestyle=\"--\")\naxs[2, 1].set_title('Call Option Price vs Drift (μ)')\naxs[2, 1].set_xlabel('Drift (μ)')\naxs[2, 1].set_ylabel('Call Option Price (P)')\naxs[2, 1].legend()\naxs[2, 1].grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n","type":"content","url":"/notebooks/fair-price-estimation#call-option-premium-using-arbitrage-free-theory","position":15},{"hierarchy":{"lvl1":"Fair Price Estimation","lvl3":"Using the BSM framework in practice","lvl2":"The arbitrage-free theory of derivatives pricing"},"type":"lvl3","url":"/notebooks/fair-price-estimation#using-the-bsm-framework-in-practice","position":16},{"hierarchy":{"lvl1":"Fair Price Estimation","lvl3":"Using the BSM framework in practice","lvl2":"The arbitrage-free theory of derivatives pricing"},"content":"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nimport pandas as pd\n\ndef black_scholes_price(S, K, T, r, sigma, option_type='call'):\n    \"\"\"\n    Calculate Black-Scholes option price.\n    \"\"\"\n    d1 = (np.log(S / K) + (r + 0.5 * sigma **2 ) * T ) / (sigma * np.sqrt(T))\n    d2 = d1 - sigma * np.sqrt(T)\n    \n    if option_type == 'call':\n        price = S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n    elif option_type == 'put':\n        price = K * np.exp(-r * T) * norm.cdf(-d2) - S * norm.cdf(-d1)\n    else:\n        raise ValueError(\"option_type must be 'call' or 'put'\")\n    return price\n\ndef black_scholes_delta(S, K, T, r, sigma, option_type='call'):\n    \"\"\"\n    Calculate Black-Scholes option delta.\n    \"\"\"\n    d1 = (np.log(S / K) + (r + 0.5 * sigma **2 ) * T ) / (sigma * np.sqrt(T))\n    \n    if option_type == 'call':\n        delta = norm.cdf(d1)\n    elif option_type == 'put':\n        delta = norm.cdf(d1) - 1\n    else:\n        raise ValueError(\"option_type must be 'call' or 'put'\")\n    return delta\n\ndef simulate_gbm_paths(S0, mu, sigma, T, dt, n_paths, \n                      stochastic_vol=False, kappa=2.0, theta=0.04, xi=0.2, rho=-0.7):\n    \"\"\"\n    Simulate GBM paths with optional stochastic volatility based on the Heston model.\n    \n    Parameters:\n    - S0: initial stock price\n    - mu: drift\n    - sigma: initial volatility\n    - T: maturity\n    - dt: time step\n    - n_paths: number of simulation paths\n    - stochastic_vol: if True, use the Heston stochastic volatility model\n    - kappa: rate of mean reversion of variance (only if stochastic_vol=True)\n    - theta: long-term variance mean (only if stochastic_vol=True)\n    - xi: volatility of variance (only if stochastic_vol=True)\n    - rho: correlation between stock and variance (only if stochastic_vol=True)\n    \n    Returns:\n    - paths: array of shape (n_steps +1, n_paths)\n    \"\"\"\n    n_steps = int(T / dt)\n    paths = np.zeros((n_steps +1, n_paths))\n    paths[0] = S0\n    \n    if stochastic_vol:\n        # Initialize variance\n        V0 = sigma ** 2\n        variances = np.zeros((n_steps +1, n_paths))\n        variances[0] = V0\n        \n        # Precompute correlation matrix and Cholesky decomposition\n        cov_matrix = np.array([[1.0, rho],\n                               [rho, 1.0]])\n        L = np.linalg.cholesky(cov_matrix)\n        \n        for t in range(1, n_steps +1):\n            # Simulate two correlated random variables\n            Z = np.random.standard_normal((2, n_paths))\n            correlated_Z = L @ Z\n            Z_S = correlated_Z[0]\n            Z_V = correlated_Z[1]\n            \n            # Update variance using CIR process\n            V_prev = variances[t-1]\n            V = V_prev + kappa * (theta - V_prev) * dt + xi * np.sqrt(np.maximum(V_prev, 0)) * np.sqrt(dt) * Z_V\n            V = np.maximum(V, 0)  # Ensure variance is non-negative\n            variances[t] = V\n            \n            # Update stock price\n            S_prev = paths[t-1]\n            S = S_prev * np.exp( (mu - 0.5 * V_prev) * dt + np.sqrt(V_prev) * np.sqrt(dt) * Z_S )\n            paths[t] = S\n    else:\n        for t in range(1, n_steps +1):\n            Z = np.random.standard_normal(n_paths)\n            paths[t] = paths[t-1] * np.exp( (mu - 0.5 * sigma **2 ) * dt + sigma * np.sqrt(dt) * Z )\n    \n    return paths\n\ndef dynamic_hedging(paths, S0, K, T, r, sigma_bs, option_type='call', dt=1/252,\n                   rehedge_freq=1, half_spread=0.0, sigma_sim=None):\n    \"\"\"\n    Implement dynamic hedging strategy.\n    \n    Parameters:\n    - paths: simulated stock price paths (array of shape (n_steps+1, n_paths))\n    - S0: initial stock price\n    - K: strike price\n    - T: maturity\n    - r: risk-free rate\n    - sigma_bs: volatility used in BS model\n    - option_type: 'call' or 'put'\n    - dt: time step size\n    - rehedge_freq: frequency of rehedging in terms of number of steps. If 1, rebalance every step.\n    - half_spread: transaction cost as a fraction of price (half spread for buying and selling)\n    - sigma_sim: volatility used in simulation (if different from sigma_bs)\n    \n    Returns:\n    - differences: array of portfolio - payoff for each path\n    \"\"\"\n    n_steps, n_paths = paths.shape[0]-1, paths.shape[1]\n    # Calculate option price and initial delta\n    option_price = black_scholes_price(S0, K, T, r, sigma_bs, option_type)\n    option_delta = black_scholes_delta(S0, K, T, r, sigma_bs, option_type)\n    \n    # Initialize portfolio\n    portfolio = np.full(n_paths, option_price)\n    stock_position = np.full(n_paths, option_delta)\n    cash_position = portfolio - stock_position * S0\n    \n    # Time steps\n    times = np.linspace(0, T, n_steps +1)\n    \n    # Rehedge steps\n    rehedge_steps = rehedge_freq\n    \n    for t in range(1, n_steps +1):\n        tau = T - times[t]\n        if tau <= 0:\n            tau = 1e-10  # Avoid division by zero\n        \n        # Determine if rebalancing is needed\n        if t % rehedge_steps == 0:\n            # Compute delta using BS formula\n            current_S = paths[t]\n            current_delta = black_scholes_delta(current_S, K, tau, r, sigma_bs, option_type)\n            \n            # Calculate change in delta\n            delta_change = current_delta - stock_position\n            # Calculate transaction cost\n            transaction_cost = half_spread * np.abs(delta_change * current_S)\n            \n            # Update cash position\n            cash_position = cash_position * np.exp(r * dt) - delta_change * current_S - transaction_cost\n            \n            # Update stock position\n            stock_position = current_delta\n        else:\n            # Just grow the cash position\n            cash_position = cash_position * np.exp(r * dt)\n        \n        # Update portfolio value\n        portfolio = cash_position + stock_position * paths[t]\n    \n    # Option payoff\n    if option_type == 'call':\n        payoff = np.maximum(paths[-1] - K, 0)\n    elif option_type == 'put':\n        payoff = np.maximum(K - paths[-1], 0)\n    else:\n        raise ValueError(\"option_type must be 'call' or 'put'\")\n    \n    differences = portfolio - payoff\n    return differences\n\ndef run_simulation():\n    # Parameters\n    S0 = 100          # Initial stock price\n    K = 100           # Strike price\n    T = 1.0           # 1 year\n    r = 0.05          # 5% risk-free rate\n    sigma_bs = 0.20   # 20% volatility used in Black-Scholes model\n    mu = 0.1          # 10% drift\n    option_type = 'call'\n    n_paths = 10000   # Number of simulation paths\n    dt = 1/10000      # Fixed small time step for accurate approximation\n    \n    # Perfect setup with varying rehedging frequency\n    rehedge_freq_values = [1, 10, 100]  # Rebalance every 1, 10, 100 steps\n    differences_rehedge_freq = {}\n    \n    # Simulate perfect hedging with varying rehedging frequencies\n    for freq in rehedge_freq_values:\n        print(f\"Simulating Perfect Setup with Rehedging Frequency: Every {freq} steps\")\n        paths_perfect = simulate_gbm_paths(S0, mu, sigma_bs, T, dt, n_paths, stochastic_vol=False)\n        differences = dynamic_hedging(paths_perfect, S0, K, T, r, sigma_bs, option_type, dt, \n                                     rehedge_freq=freq, half_spread=0.0)\n        differences_rehedge_freq[freq] = differences\n    \n    # Baseline residuals (Rebalance every step)\n    residuals_perfect = differences_rehedge_freq[1]\n    \n    # Violations\n    # 1. Different volatility for simulation (sigma_sim = 0.19 and 0.21)\n    sigma_sim_values = [0.19, 0.21]\n    differences_sigma_sim = {}\n    for sigma_sim in sigma_sim_values:\n        print(f\"Simulating Different Volatility in Simulation: sigma_sim = {sigma_sim}\")\n        paths_diff_vol = simulate_gbm_paths(S0, mu, sigma_sim, T, dt, n_paths, stochastic_vol=False)\n        differences = dynamic_hedging(paths_diff_vol, S0, K, T, r, sigma_bs, option_type, dt, \n                                     rehedge_freq=1, half_spread=0.0)\n        differences_sigma_sim[sigma_sim] = differences\n    \n    # 2. Stochastic Volatility (Heston Model)\n    print(\"Simulating Stochastic Volatility (Heston Model)\")\n    paths_stoch_vol = simulate_gbm_paths(S0, mu, sigma_bs, T, dt, n_paths, stochastic_vol=True,\n                                        kappa=20.0, theta=0.04, xi=0.2, rho=-0.7)\n    differences_stoch_vol = dynamic_hedging(paths_stoch_vol, S0, K, T, r, sigma_bs, option_type, dt, \n                                           rehedge_freq=1, half_spread=0.0)\n    \n    # 3. Transaction Costs (Half Spread = 0.005%)\n    half_spread = 0.00005  # 0.005%\n    print(\"Simulating With Transaction Costs (Half Spread = 0.005%)\")\n    paths_half_spread = simulate_gbm_paths(S0, mu, sigma_bs, T, dt, n_paths, stochastic_vol=False)\n    differences_half_spread = dynamic_hedging(paths_half_spread, S0, K, T, r, sigma_bs, option_type, dt, \n                                             rehedge_freq=1, half_spread=half_spread)\n    \n    # Plot histograms in a 2x2 grid\n    plt.figure(figsize=(18, 14))\n    \n    # Subplot 1: Effect of Re-Hedging Frequency on Hedging Residuals\n    plt.subplot(2, 2, 1)\n    colors = ['blue', 'green', 'red', 'purple']\n    labels = [f'Rebalance Every {freq} Steps' for freq in rehedge_freq_values]\n    \n    for i, freq in enumerate(rehedge_freq_values):\n        plt.hist(differences_rehedge_freq[freq], bins=100, alpha=0.5, \n                 color=colors[i], label=labels[i], density=True)\n    \n    plt.title('Effect of Re-Hedging Frequency on Hedging Residuals')\n    plt.xlabel('Portfolio - Payoff')\n    plt.ylabel('Density')\n    plt.legend()\n    plt.grid(True)\n    \n    # Subplot 2: Different Volatility in Simulation\n    plt.subplot(2, 2, 2)\n    colors_sigma = ['orange', 'cyan']\n    labels_sigma = [f'sigma_sim = {sigma_sim:.2f}' for sigma_sim in sigma_sim_values]\n    \n    for i, sigma_sim in enumerate(sigma_sim_values):\n        plt.hist(differences_sigma_sim[sigma_sim], bins=100, alpha=0.5, \n                 color=colors_sigma[i], label=labels_sigma[i], density=True)\n    \n    # Add baseline\n    plt.hist(residuals_perfect, bins=100, alpha=0.3, color='black', label='Perfect Replication', density=True)\n    \n    plt.title('Different Volatility in Simulation')\n    plt.xlabel('Portfolio - Payoff')\n    plt.ylabel('Density')\n    plt.legend()\n    plt.grid(True)\n    \n    # Subplot 3: Stochastic Volatility (Heston Model)\n    plt.subplot(2, 2, 3)\n    plt.hist(differences_stoch_vol, bins=100, alpha=0.7, color='purple', edgecolor='black', density=True, label='Stochastic Volatility')\n    \n    # Add baseline\n    plt.hist(residuals_perfect, bins=100, alpha=0.3, color='black', label='Perfect Replication', density=True)\n    \n    plt.title('Stochastic Volatility (Heston Model)')\n    plt.xlabel('Portfolio - Payoff')\n    plt.ylabel('Density')\n    plt.legend()\n    plt.grid(True)\n    \n    # Subplot 4: Transaction Costs (Half Spread)\n    plt.subplot(2, 2, 4)\n    plt.hist(differences_half_spread, bins=100, alpha=0.7, color='green', edgecolor='black', density=True, label='Half Spread = 0.005%')\n    \n    # Add baseline\n    plt.hist(residuals_perfect, bins=100, alpha=0.3, color='black', label='Perfect Replication', density=True)\n    \n    plt.title('Transaction Costs (Half Spread = 0.005%)')\n    plt.xlabel('Portfolio - Payoff')\n    plt.ylabel('Density')\n    plt.legend()\n    plt.grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Summary statistics for Perfect Setup with varied rehedging frequency\n    print(\"Summary Statistics for Perfect Setup with Varied Re-Hedging Frequency:\")\n    for freq, diff in differences_rehedge_freq.items():\n        print(f\"Rebalance Every {freq} Steps: Mean = {np.mean(diff):.6f}, Std Dev = {np.std(diff):.6f}\")\n    print(\"-\"*60)\n    \n    # Summary statistics for Violations\n    print(\"Summary Statistics for Violations:\")\n    \n    # 1. Different Volatility in Simulation\n    for sigma_sim, diff in differences_sigma_sim.items():\n        print(f\"Different Volatility in Simulation (sigma_sim = {sigma_sim:.2f}):\")\n        print(f\"  Mean difference: {np.mean(diff):.6f}\")\n        print(f\"  Std Dev of difference: {np.std(diff):.6f}\")\n    \n    # 2. Stochastic Volatility (Heston Model)\n    print(\"Stochastic Volatility (Heston Model):\")\n    print(f\"  Mean difference: {np.mean(differences_stoch_vol):.6f}\")\n    print(f\"  Std Dev of difference: {np.std(differences_stoch_vol):.6f}\")\n    \n    # 3. Transaction Costs (Half Spread)\n    print(\"Transaction Costs (Half Spread = 0.005%):\")\n    print(f\"  Mean difference: {np.mean(differences_half_spread):.6f}\")\n    print(f\"  Std Dev of difference: {np.std(differences_half_spread):.6f}\")\n    print(\"-\"*40)\n\nif __name__ == \"__main__\":\n    run_simulation()\n\n\n\n\n\n\n","type":"content","url":"/notebooks/fair-price-estimation#using-the-bsm-framework-in-practice","position":17},{"hierarchy":{"lvl1":"Market Microstructure"},"type":"lvl1","url":"/notebooks/market-microstructure","position":0},{"hierarchy":{"lvl1":"Market Microstructure"},"content":"","type":"content","url":"/notebooks/market-microstructure","position":1},{"hierarchy":{"lvl1":"Market Microstructure","lvl2":"The Central Limit Order Book"},"type":"lvl2","url":"/notebooks/market-microstructure#the-central-limit-order-book","position":2},{"hierarchy":{"lvl1":"Market Microstructure","lvl2":"The Central Limit Order Book"},"content":"","type":"content","url":"/notebooks/market-microstructure#the-central-limit-order-book","position":3},{"hierarchy":{"lvl1":"Market Microstructure","lvl3":"Visualization of the order book","lvl2":"The Central Limit Order Book"},"type":"lvl3","url":"/notebooks/market-microstructure#visualization-of-the-order-book","position":4},{"hierarchy":{"lvl1":"Market Microstructure","lvl3":"Visualization of the order book","lvl2":"The Central Limit Order Book"},"content":"\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define bid and ask prices\nbid_prices = np.array([94, 95, 96, 97, 98, 99])  # Bid price levels\nask_prices = np.array([101, 102, 103, 104, 105, 106])  # Ask price levels\n\n# Randomize volumes for bids and asks\nnp.random.seed(45)  # For reproducibility\nbid_volumes = np.random.randint(50, 400, size=len(bid_prices))  # Random volumes for bids\nask_volumes = np.random.randint(50, 400, size=len(ask_prices))  # Random volumes for asks\n\n# Empty volumes for levels close to the mid-price (bid at 99 and ask at 101)\nbid_volumes[5] = 0  # Bid close to mid (price 99)\nask_volumes[0] = 0  # Ask close to mid (price 101)\n\n# Calculate the best bid and ask prices and their volumes\n# Best bid: highest bid price with non-zero volume\nnonzero_bid_indices = np.where(bid_volumes > 0)[0]\nif len(nonzero_bid_indices) > 0:\n    best_bid_price = bid_prices[nonzero_bid_indices].max()\n    best_bid_index = np.where(bid_prices == best_bid_price)[0][0]\n    best_bid_volume = bid_volumes[best_bid_index]\nelse:\n    best_bid_price = None\n    best_bid_volume = 0\n\n# Best ask: lowest ask price with non-zero volume\nnonzero_ask_indices = np.where(ask_volumes > 0)[0]\nif len(nonzero_ask_indices) > 0:\n    best_ask_price = ask_prices[nonzero_ask_indices].min()\n    best_ask_index = np.where(ask_prices == best_ask_price)[0][0]\n    best_ask_volume = ask_volumes[best_ask_index]\nelse:\n    best_ask_price = None\n    best_ask_volume = 0\n\n# Calculate mid-price and spread\nif best_bid_price is not None and best_ask_price is not None:\n    mid_price = (best_bid_price + best_ask_price) / 2\n    spread = best_ask_price - best_bid_price\nelse:\n    mid_price = None\n    spread = None\n\n# Calculate imbalance between best bid and ask volumes\ntotal_volume = best_bid_volume + best_ask_volume\nif total_volume > 0:\n    imbalance = (best_bid_volume - best_ask_volume) / total_volume\nelse:\n    imbalance = None\n\n# Create figure and axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot bid bars (leave a gap near the mid-price)\nax.bar(bid_prices, bid_volumes, color='green', align='center', label='Bid Orders')\n\n# Plot ask bars (leave a gap near the mid-price)\nax.bar(ask_prices, ask_volumes, color='red', align='center', label='Ask Orders')\n\n# Mid-price line\nif mid_price is not None:\n    ax.axvline(mid_price, color='black', linestyle='--', label=f'Mid Price')\nelse:\n    ax.axvline(100, color='black', linestyle='--', label='Mid Price')\n\n# Adding x-axis ticks for every integer price level from min to max price\nall_price_levels = np.arange(min(bid_prices.min(), ask_prices.min()), max(bid_prices.max(), ask_prices.max()) + 1)\nax.set_xticks(all_price_levels)\nax.set_xticklabels(all_price_levels)\n\n# Labels and title\nax.set_ylabel('Volume')\nax.set_xlabel('Price Levels')\nax.set_title('Central Limit Order Book (Market Depth)')\nax.legend()\n\n# Show the plot\nplt.show()\n\n# Print the calculated values\nprint(\"Calculated Market Metrics:\")\nif best_bid_price is not None:\n    print(f\"Best Bid Price: {best_bid_price}, Volume: {best_bid_volume}\")\nelse:\n    print(\"No bids available.\")\n\nif best_ask_price is not None:\n    print(f\"Best Ask Price: {best_ask_price}, Volume: {best_ask_volume}\")\nelse:\n    print(\"No asks available.\")\n\nif mid_price is not None and spread is not None:\n    print(f\"Mid-Price: {mid_price:.2f}\")\n    print(f\"Spread: {spread:.2f}\")\nelse:\n    print(\"Mid-Price and Spread are not available.\")\n\nif imbalance is not None:\n    print(f\"Imbalance between best bid and ask: {imbalance:.2f}\")\nelse:\n    print(\"Imbalance is not available due to zero total volume at best bid and ask.\")\n\n\n\n\n\n\n","type":"content","url":"/notebooks/market-microstructure#visualization-of-the-order-book","position":5},{"hierarchy":{"lvl1":"Market Microstructure","lvl3":"Effect of a market order in the CLOB","lvl2":"The Central Limit Order Book"},"type":"lvl3","url":"/notebooks/market-microstructure#effect-of-a-market-order-in-the-clob","position":6},{"hierarchy":{"lvl1":"Market Microstructure","lvl3":"Effect of a market order in the CLOB","lvl2":"The Central Limit Order Book"},"content":"\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define bid and ask prices\nbid_prices = np.array([94, 95, 96, 97, 98, 99])  # Bid price levels\nask_prices = np.array([101, 102, 103, 104, 105, 106])  # Ask price levels\n\n# Randomize volumes for bids and asks\nnp.random.seed(45)  # For reproducibility\nbid_volumes = np.random.randint(50, 400, size=len(bid_prices))  # Random volumes for bids\nask_volumes = np.random.randint(50, 400, size=len(ask_prices))  # Random volumes for asks\n\n# Empty volumes for levels close to the mid-price (bid at 99 and ask at 101)\nbid_volumes[5] = 0  # Bid close to mid\nask_volumes[0] = 0  # Ask close to mid\n\n# Arrays to track consumed volumes (for visualization)\nconsumed_bid_volumes = np.zeros(len(bid_volumes))\nconsumed_ask_volumes = np.zeros(len(ask_volumes))\n\nmid_price = 100  # Initial mid-price before market order execution\n\n# Simulate market order input\nmarket_order_size = 500  # For example, 500\nmarket_order_side = \"buy\"  # For example, 'buy'\n\n# Function to simulate the market order\ndef execute_market_order(order_size, order_side):\n    global bid_volumes, ask_volumes, consumed_bid_volumes, consumed_ask_volumes\n    executed_volume = 0\n    executed_value = 0\n\n    if order_side == 'buy':  # Market order to buy shares\n        remaining_size = order_size\n        for i in range(len(ask_prices)):\n            if ask_volumes[i] > 0:\n                tradable_volume = min(ask_volumes[i], remaining_size)\n                executed_volume += tradable_volume\n                executed_value += tradable_volume * ask_prices[i]\n                consumed_ask_volumes[i] += tradable_volume  # Track consumed volume\n                ask_volumes[i] -= tradable_volume\n                remaining_size -= tradable_volume\n                if remaining_size <= 0:\n                    break\n    elif order_side == 'sell':  # Market order to sell shares\n        remaining_size = order_size\n        for i in range(len(bid_prices)-1, -1, -1):  # Start from highest bid\n            if bid_volumes[i] > 0:\n                tradable_volume = min(bid_volumes[i], remaining_size)\n                executed_volume += tradable_volume\n                executed_value += tradable_volume * bid_prices[i]\n                consumed_bid_volumes[i] += tradable_volume  # Track consumed volume\n                bid_volumes[i] -= tradable_volume\n                remaining_size -= tradable_volume\n                if remaining_size <= 0:\n                    break\n\n    # Calculate the average execution price\n    execution_price = executed_value / executed_volume if executed_volume > 0 else 0\n    return execution_price\n\n# Function to recalculate the mid-price based on the updated order book\ndef recalculate_mid_price():\n    global bid_prices, bid_volumes, ask_prices, ask_volumes\n    # Get the best bid price (highest bid price with non-zero volume)\n    if np.any(bid_volumes > 0):\n        best_bid = np.max(bid_prices[bid_volumes > 0])\n    else:\n        best_bid = np.nan  # No bids in the book\n\n    # Get the best ask price (lowest ask price with non-zero volume)\n    if np.any(ask_volumes > 0):\n        best_ask = np.min(ask_prices[ask_volumes > 0])\n    else:\n        best_ask = np.nan  # No asks in the book\n\n    # Recalculate mid-price\n    if not np.isnan(best_bid) and not np.isnan(best_ask):\n        new_mid_price = (best_bid + best_ask) / 2\n    elif not np.isnan(best_bid):\n        new_mid_price = best_bid  # Only bids are available\n    elif not np.isnan(best_ask):\n        new_mid_price = best_ask  # Only asks are available\n    else:\n        new_mid_price = np.nan  # No bids or asks in the book\n\n    return new_mid_price\n\n# Updated function to visualize the order book\ndef plot_order_book(order_side, mid_price):\n    fig, ax = plt.subplots(figsize=(10, 6))\n\n    # Plot remaining bid orders in green\n    ax.bar(bid_prices, bid_volumes, color='green', align='center', label='Remaining Bid Orders')\n\n    # Plot remaining ask orders in red\n    ax.bar(ask_prices, ask_volumes, color='red', align='center', label='Remaining Ask Orders')\n\n    # Plot consumed liquidity based on the order side\n    if order_side == 'buy':\n        # Plot consumed ask liquidity on top with a hatch pattern\n        if np.any(consumed_ask_volumes > 0):\n            ax.bar(ask_prices, consumed_ask_volumes, bottom=ask_volumes, color='red', align='center',\n                   label='Consumed Ask Liquidity', hatch='\\\\\\\\', edgecolor='red', alpha=0.7)\n    elif order_side == 'sell':\n        # Plot consumed bid liquidity on top with a hatch pattern\n        if np.any(consumed_bid_volumes > 0):\n            ax.bar(bid_prices, consumed_bid_volumes, bottom=bid_volumes, color='green', align='center',\n                   label='Consumed Bid Liquidity', hatch='//', edgecolor='green', alpha=0.7)\n\n    # Mid-price line\n    if not np.isnan(mid_price):\n        ax.axvline(mid_price, color='black', linestyle='--', label=f'Mid Price')\n    else:\n        ax.axvline(mid_price, color='black', linestyle='--', label='Mid Price Unavailable')\n\n    # Adding x-axis ticks for every integer price level from min to max price\n    all_price_levels = np.arange(min(bid_prices.min(), ask_prices.min()), max(bid_prices.max(), ask_prices.max()) + 1)\n    ax.set_xticks(all_price_levels)\n    ax.set_xticklabels(all_price_levels)\n\n    # Labels and title\n    ax.set_ylabel('Volume')\n    ax.set_xlabel('Price Levels')\n    ax.set_title('Effect of a Market Order in the CLOB')\n\n    # Custom legend to include only relevant labels\n    handles, labels = ax.get_legend_handles_labels()\n    by_label = dict(zip(labels, handles))\n\n    # Filter the legend labels based on the order side\n    if order_side == 'buy':\n        by_label.pop('Consumed Bid Liquidity', None)\n    elif order_side == 'sell':\n        by_label.pop('Consumed Ask Liquidity', None)\n\n    ax.legend(by_label.values(), by_label.keys())\n\n    plt.show()\n\n# Execute the market order and update the order book\nexecution_price = execute_market_order(market_order_size, market_order_side)\n\n# Recalculate the mid-price based on the updated order book\nnew_mid_price = recalculate_mid_price()\n\n# Plot the updated order book with the new mid-price\nplot_order_book(market_order_side, new_mid_price)\n\n# Output the execution price and the new mid-price\nprint(f\"The average execution price for the market order is: {execution_price:.2f}\")\nif not np.isnan(new_mid_price):\n    print(f\"The new mid-price after the market order execution is: {new_mid_price:.2f}\")\nelse:\n    print(\"The mid-price is unavailable due to no bids or asks in the book.\")\n\n\n\n\n\n\n","type":"content","url":"/notebooks/market-microstructure#effect-of-a-market-order-in-the-clob","position":7},{"hierarchy":{"lvl1":"Market Microstructure","lvl3":"Effect of a limit order in the CLOB","lvl2":"The Central Limit Order Book"},"type":"lvl3","url":"/notebooks/market-microstructure#effect-of-a-limit-order-in-the-clob","position":8},{"hierarchy":{"lvl1":"Market Microstructure","lvl3":"Effect of a limit order in the CLOB","lvl2":"The Central Limit Order Book"},"content":"\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define bid and ask prices\nbid_prices = np.array([94, 95, 96, 97, 98, 99])  # Bid price levels\nask_prices = np.array([101, 102, 103, 104, 105, 106])  # Ask price levels\n\n# Randomize volumes for bids and asks\nnp.random.seed(45)  # For reproducibility\nbid_volumes = np.random.randint(50, 400, size=len(bid_prices))  # Random volumes for bids\nask_volumes = np.random.randint(50, 400, size=len(ask_prices))  # Random volumes for asks\n\n# Empty volumes for levels close to the mid-price (bid at 99 and ask at 101)\nbid_volumes[5] = 0  # Bid close to mid\nask_volumes[0] = 0  # Ask close to mid\n\n# Arrays to track consumed volumes and new limit orders for visualization\nconsumed_bid_volumes = np.zeros(len(bid_volumes))\nconsumed_ask_volumes = np.zeros(len(ask_volumes))\nnew_bid_volumes = np.zeros(len(bid_volumes))\nnew_ask_volumes = np.zeros(len(ask_volumes))\n\nmid_price = 100  # Initial mid-price before limit order execution\n\n# Function to simulate an aggressive limit order\ndef execute_limit_order(order_price, order_volume, order_side):\n    global bid_volumes, ask_volumes, consumed_bid_volumes, consumed_ask_volumes, new_bid_volumes, new_ask_volumes\n\n    executed_volume = 0\n    executed_value = 0\n    remaining_volume = order_volume\n\n    if order_side == 'buy':\n        best_ask = min(ask_prices[ask_volumes > 0]) if np.any(ask_volumes > 0) else np.inf\n        if order_price < best_ask:\n            # The buy limit order remains unexecuted and gets added to the book below existing orders\n            insert_order_into_book(order_price, remaining_volume, 'buy', priority='low')\n            return 0, remaining_volume  # No execution, entire order sits in the book\n        else:\n            # The buy limit order consumes liquidity like a market order up to the limit price\n            for i in range(len(ask_prices)):\n                if ask_prices[i] <= order_price and ask_volumes[i] > 0:\n                    if ask_volumes[i] >= remaining_volume:\n                        executed_volume += remaining_volume\n                        executed_value += remaining_volume * ask_prices[i]\n                        consumed_ask_volumes[i] += remaining_volume  # Track consumed volume\n                        ask_volumes[i] -= remaining_volume\n                        remaining_volume = 0\n                        break\n                    else:\n                        executed_volume += ask_volumes[i]\n                        executed_value += ask_volumes[i] * ask_prices[i]\n                        consumed_ask_volumes[i] += ask_volumes[i]  # Track consumed volume\n                        remaining_volume -= ask_volumes[i]\n                        ask_volumes[i] = 0\n            # Any remaining volume sits in the book at the limit price\n            if remaining_volume > 0:\n                insert_order_into_book(order_price, remaining_volume, 'buy', priority='low')\n\n    elif order_side == 'sell':\n        best_bid = max(bid_prices[bid_volumes > 0]) if np.any(bid_volumes > 0) else -np.inf\n        if order_price > best_bid:\n            # The sell limit order remains unexecuted and gets added to the book above existing orders\n            insert_order_into_book(order_price, remaining_volume, 'sell', priority='low')\n            return 0, remaining_volume  # No execution, entire order sits in the book\n        else:\n            # The sell limit order consumes liquidity like a market order up to the limit price\n            for i in range(len(bid_prices)-1, -1, -1):  # Start from highest bid\n                if bid_prices[i] >= order_price and bid_volumes[i] > 0:\n                    if bid_volumes[i] >= remaining_volume:\n                        executed_volume += remaining_volume\n                        executed_value += remaining_volume * bid_prices[i]\n                        consumed_bid_volumes[i] += remaining_volume  # Track consumed volume\n                        bid_volumes[i] -= remaining_volume\n                        remaining_volume = 0\n                        break\n                    else:\n                        executed_volume += bid_volumes[i]\n                        executed_value += bid_volumes[i] * bid_prices[i]\n                        consumed_bid_volumes[i] += bid_volumes[i]  # Track consumed volume\n                        remaining_volume -= bid_volumes[i]\n                        bid_volumes[i] = 0\n            # Any remaining volume sits in the book at the limit price\n            if remaining_volume > 0:\n                insert_order_into_book(order_price, remaining_volume, 'sell', priority='low')\n\n    # Calculate the average execution price\n    execution_price = executed_value / executed_volume if executed_volume > 0 else 0\n    return execution_price, remaining_volume\n\n# Function to insert the remaining limit order into the order book\ndef insert_order_into_book(order_price, remaining_volume, order_side, priority):\n    global bid_prices, ask_prices, bid_volumes, ask_volumes, new_bid_volumes, new_ask_volumes, consumed_bid_volumes, consumed_ask_volumes\n\n    if order_side == 'buy':  # Remaining buy order stays as a bid\n        # Check if the price level exists\n        if order_price in bid_prices:\n            index = np.where(bid_prices == order_price)[0][0]\n            if priority == 'low':\n                new_bid_volumes[index] += remaining_volume  # Lower priority new limit order\n            else:\n                bid_volumes[index] += remaining_volume  # Higher priority remaining volume\n        else:\n            # Insert a new price level\n            bid_prices = np.append(bid_prices, order_price)\n            bid_volumes = np.append(bid_volumes, 0 if priority == 'low' else remaining_volume)\n            new_bid_volumes = np.append(new_bid_volumes, remaining_volume if priority == 'low' else 0)\n            consumed_bid_volumes = np.append(consumed_bid_volumes, 0)\n            # Sort by price\n            sort_indices = np.argsort(bid_prices)\n            bid_prices = bid_prices[sort_indices]\n            bid_volumes = bid_volumes[sort_indices]\n            new_bid_volumes = new_bid_volumes[sort_indices]\n            consumed_bid_volumes = consumed_bid_volumes[sort_indices]\n\n    elif order_side == 'sell':  # Remaining sell order stays as an ask\n        # Check if the price level exists\n        if order_price in ask_prices:\n            index = np.where(ask_prices == order_price)[0][0]\n            if priority == 'low':\n                new_ask_volumes[index] += remaining_volume  # Lower priority new limit order\n            else:\n                ask_volumes[index] += remaining_volume  # Higher priority remaining volume\n        else:\n            # Insert a new price level\n            ask_prices = np.append(ask_prices, order_price)\n            ask_volumes = np.append(ask_volumes, 0 if priority == 'low' else remaining_volume)\n            new_ask_volumes = np.append(new_ask_volumes, remaining_volume if priority == 'low' else 0)\n            consumed_ask_volumes = np.append(consumed_ask_volumes, 0)\n            # Sort by price\n            sort_indices = np.argsort(ask_prices)\n            ask_prices = ask_prices[sort_indices]\n            ask_volumes = ask_volumes[sort_indices]\n            new_ask_volumes = new_ask_volumes[sort_indices]\n            consumed_ask_volumes = consumed_ask_volumes[sort_indices]\n\n# Function to recalculate the mid-price based on the updated order book\ndef recalculate_mid_price():\n    global bid_prices, bid_volumes, ask_prices, ask_volumes, new_bid_volumes, new_ask_volumes\n    # Calculate total bid volumes at each price level\n    total_bid_volumes = bid_volumes + new_bid_volumes\n    # Get the best bid price (highest bid price with non-zero volume)\n    if np.any(total_bid_volumes > 0):\n        best_bid = np.max(bid_prices[total_bid_volumes > 0])\n    else:\n        best_bid = np.nan  # No bids in the book\n\n    # Calculate total ask volumes at each price level\n    total_ask_volumes = ask_volumes + new_ask_volumes\n    # Get the best ask price (lowest ask price with non-zero volume)\n    if np.any(total_ask_volumes > 0):\n        best_ask = np.min(ask_prices[total_ask_volumes > 0])\n    else:\n        best_ask = np.nan  # No asks in the book\n\n    # Recalculate mid-price\n    if not np.isnan(best_bid) and not np.isnan(best_ask):\n        new_mid_price = (best_bid + best_ask) / 2\n    elif not np.isnan(best_bid):\n        new_mid_price = best_bid  # Only bids are available\n    elif not np.isnan(best_ask):\n        new_mid_price = best_ask  # Only asks are available\n    else:\n        new_mid_price = np.nan  # No bids or asks in the book\n\n    return new_mid_price\n\n# Updated function to visualize the order book\ndef plot_order_book(order_side, mid_price):\n    fig, ax = plt.subplots(figsize=(10, 6))\n\n    # For bids\n    # Total volumes for stacking\n    bid_total_volumes = new_bid_volumes + bid_volumes + consumed_bid_volumes\n\n    # Plot new bid limit orders at the bottom\n    ax.bar(bid_prices, new_bid_volumes, color='darkgreen', align='center',\n           label='New Bid Limit Orders')\n\n    # Plot existing bid orders on top of new limit orders\n    bid_existing_bottom = new_bid_volumes\n    ax.bar(bid_prices, bid_volumes, bottom=bid_existing_bottom, color='green', align='center',\n           label='Remaining Bid Orders')\n\n    # Plot consumed bid liquidity on top of existing orders\n    bid_consumed_bottom = bid_existing_bottom + bid_volumes\n    if np.any(consumed_bid_volumes > 0):\n        ax.bar(bid_prices, consumed_bid_volumes, bottom=bid_consumed_bottom, color='green', align='center',\n               label='Consumed Liquidity', hatch='//', edgecolor='green', alpha=0.7)\n\n    # For asks\n    # Total volumes for stacking\n    ask_total_volumes = new_ask_volumes + ask_volumes + consumed_ask_volumes\n\n    # Plot new ask limit orders at the bottom\n    ax.bar(ask_prices, new_ask_volumes, color='darkred', align='center',\n           label='New Ask Limit Orders')\n\n    # Plot existing ask orders on top of new limit orders\n    ask_existing_bottom = new_ask_volumes\n    ax.bar(ask_prices, ask_volumes, bottom=ask_existing_bottom, color='red', align='center',\n           label='Remaining Ask Orders')\n\n    # Plot consumed ask liquidity on top of existing orders\n    ask_consumed_bottom = ask_existing_bottom + ask_volumes\n    if np.any(consumed_ask_volumes > 0):\n        ax.bar(ask_prices, consumed_ask_volumes, bottom=ask_consumed_bottom, color='red', align='center',\n               label='Consumed Liquidity', hatch='\\\\\\\\', edgecolor='red', alpha=0.7)\n\n    # Mid-price line\n    if not np.isnan(mid_price):\n        ax.axvline(mid_price, color='black', linestyle='--', label=f'Mid Price')\n    else:\n        ax.axvline(mid_price, color='black', linestyle='--', label='Mid Price Unavailable')\n\n    # Adding x-axis ticks\n    all_price_levels = np.arange(min(bid_prices.min(), ask_prices.min()), max(bid_prices.max(), ask_prices.max()) + 1)\n    ax.set_xticks(all_price_levels)\n    ax.set_xticklabels(all_price_levels)\n\n    # Labels and title\n    ax.set_ylabel('Volume')\n    ax.set_xlabel('Price Levels')\n    ax.set_title('Effect of a limit order in the CLOB')\n\n    # Custom legend\n    handles, labels = ax.get_legend_handles_labels()\n    by_label = dict(zip(labels, handles))\n    ax.legend(by_label.values(), by_label.keys())\n\n    plt.show()\n\n# Example of a buy limit order at price 102 for 700 shares\nlimit_order_price = 98\nlimit_order_volume = 500\nlimit_order_side = 'buy'\n\n# Execute the limit order\nexecution_price, remaining_volume = execute_limit_order(limit_order_price, limit_order_volume, limit_order_side)\n\n# Recalculate the mid-price based on the updated order book\nnew_mid_price = recalculate_mid_price()\n\n# Plot the updated order book with the new mid-price\nplot_order_book(limit_order_side, new_mid_price)\n\n# Output the execution price and remaining order details\nprint(f\"The average execution price for the limit order is: {execution_price:.2f}\")\nif remaining_volume > 0:\n    print(f\"Remaining order of {remaining_volume} shares sits in the book at the limit price of {limit_order_price}.\")\nif not np.isnan(new_mid_price):\n    print(f\"The new mid-price after the limit order execution is: {new_mid_price:.2f}\")\nelse:\n    print(\"The mid-price is unavailable due to no bids or asks in the book.\")\n\n\n\n\n\n\n","type":"content","url":"/notebooks/market-microstructure#effect-of-a-limit-order-in-the-clob","position":9},{"hierarchy":{"lvl1":"Market Microstructure","lvl2":"The RfQ process in MD2C platforms"},"type":"lvl2","url":"/notebooks/market-microstructure#the-rfq-process-in-md2c-platforms","position":10},{"hierarchy":{"lvl1":"Market Microstructure","lvl2":"The RfQ process in MD2C platforms"},"content":"","type":"content","url":"/notebooks/market-microstructure#the-rfq-process-in-md2c-platforms","position":11},{"hierarchy":{"lvl1":"Market Microstructure","lvl3":"Visualization of RfQ process","lvl2":"The RfQ process in MD2C platforms"},"type":"lvl3","url":"/notebooks/market-microstructure#visualization-of-rfq-process","position":12},{"hierarchy":{"lvl1":"Market Microstructure","lvl3":"Visualization of RfQ process","lvl2":"The RfQ process in MD2C platforms"},"content":"\n\nfrom graphviz import Digraph\n\n# Create a new directed graph\ndot = Digraph(comment='RfQ Protocol')\n\n# Graph-level attributes for font\ndot.attr(fontname='Helvetica', fontsize='12')\n\n# Add nodes with custom fonts\ndot.node('Client', 'Client', shape='ellipse', style='filled', color='lightblue', fontname='Helvetica', fontsize='12')\ndot.node('Platform', 'Platform', shape='box', style='filled', color='lightgrey', fontname='Helvetica', fontsize='12')\n\n# Add dealers with custom fonts\nfor i in range(1, 5):  # Adjust the number of dealers as needed\n    dot.node(f'Dealer{i}', f'Dealer {i}', shape='ellipse', style='filled', color='lightcoral', fontname='Helvetica-Italic', fontsize='12')\n\n# Add edges with custom fonts\ndot.edge('Client', 'Platform', label='RfQ', fontname='Courier', fontsize='10', fontcolor='darkblue')\nfor i in range(1, 5):\n    dot.edge('Platform', f'Dealer{i}', label=f'RfQ', fontname='Courier', fontsize='10', fontcolor='darkblue')\n    dot.edge(f'Dealer{i}', 'Platform', label=f'Quote{i}', fontname='Courier', fontsize='10', fontcolor='darkred')\ndot.edge('Platform', 'Client', label='Quotes', fontname='Courier', fontsize='10', fontcolor='darkred')\n\n# Render and display the graph\n# dot.render(\"/Users/javier/Documents/aaat/markdown/rfq\", format='png', cleanup=True)\ndot\n\n\n\n\n\n","type":"content","url":"/notebooks/market-microstructure#visualization-of-rfq-process","position":13},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets"},"type":"lvl1","url":"/notebooks/rfq-models","position":0},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets"},"content":"","type":"content","url":"/notebooks/rfq-models","position":1},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets","lvl2":"Generative models for the request for quote activity"},"type":"lvl2","url":"/notebooks/rfq-models#generative-models-for-the-request-for-quote-activity","position":2},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets","lvl2":"Generative models for the request for quote activity"},"content":"","type":"content","url":"/notebooks/rfq-models#generative-models-for-the-request-for-quote-activity","position":3},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets","lvl3":"Simulation of RfQs arrival and client attrition","lvl2":"Generative models for the request for quote activity"},"type":"lvl3","url":"/notebooks/rfq-models#simulation-of-rfqs-arrival-and-client-attrition","position":4},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets","lvl3":"Simulation of RfQs arrival and client attrition","lvl2":"Generative models for the request for quote activity"},"content":"\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import poisson, norm\nfrom collections import deque\n\ndef run_simulation(N_clients=50,\n                   T=200,\n                   reservation_price_mean=100,\n                   reservation_price_std_pct=0.1,\n                   lambda_mean=1,\n                   lambda_std=0.05,\n                   prior_mean=60,\n                   prior_std=10,\n                   res_price_noise_std_pct=0.2,\n                   hit_rate_target=0.4,\n                   window_size=10,\n                   attrition_threshold=0.1):\n\n    \"\"\"\n    Simulate RFQ interactions with Bayesian quantile pricing and client attrition.\n    Clients stop trading if their moving-average hit rate over `window_size` days falls below `attrition_threshold`.\n\n    Returns:\n        rfq_df: DataFrame of RfQ events\n        active_history: list of active client counts per day\n        activity_matrix: binary DataFrame of daily activity (clients x days)\n    \"\"\"\n    np.random.seed(42)\n    # Generate distribution of reservation prices for the segment of clients\n    reservation_prices = np.random.normal(reservation_price_mean,\n                                  reservation_price_mean * reservation_price_std_pct,\n                                  N_clients)\n    # Reservation prices are noisy given potential changing market conditions\n    res_price_noise_std = res_price_noise_std_pct * reservation_price_mean\n    res_price_noise_var = res_price_noise_std**2\n    # Generate distribution of RfQ intensities for the segment of clients\n    lambdas = np.abs(np.random.normal(lambda_mean, lambda_std, N_clients))\n    clients_posterior = {i: {'mean': prior_mean, 'var': prior_std**2,\n                             'n': 0, 'sum_obs': 0.0}\n                         for i in range(N_clients)}\n    z = norm.ppf(1 - hit_rate_target)\n    active = np.ones(N_clients, dtype=bool)\n    # Track per-client active status at end of each day\n    active_flags = np.zeros((T, N_clients), dtype=bool)\n    daily_history = [deque(maxlen=window_size) for _ in range(N_clients)]\n    active_history = []\n    records = []\n    Y = np.zeros((T, N_clients), dtype=int)\n    for t in range(T):\n        daily_hits = np.zeros(N_clients, dtype=int)\n        daily_reqs = np.zeros(N_clients, dtype=int)\n        for i in range(N_clients):\n            if not active[i]: continue\n            n_rfq = poisson.rvs(lambdas[i])\n            if n_rfq > 0: Y[t,i] = 1\n            for _ in range(n_rfq):\n                post = clients_posterior[i]\n                price = max(0.0, post['mean'] + np.sqrt(post['var'] + res_price_noise_var) * z)\n                r = norm.rvs(reservation_prices[i], res_price_noise_std)\n                # Trading happens when price offered is lower than the reservation price\n                hit = price <= r\n                daily_reqs[i] += 1; daily_hits[i] += int(hit)\n                # The dealer updates the estimation of the reservation price of the client\n                post['n'] += 1; post['sum_obs'] += r\n                post_var = 1/(1/prior_std**2 + post['n']/res_price_noise_var)\n                post['var'] = post_var\n                post['mean'] = post_var*(prior_mean/prior_std**2 + post['sum_obs']/res_price_noise_var)\n                records.append({'time': t, 'client_id': i, 'price': price, 'hit': hit})\n        for i in range(N_clients):\n            if not active[i] or daily_reqs[i]==0: continue\n            rate = daily_hits[i]/daily_reqs[i]\n            daily_history[i].append(rate)\n            # A client stops sending RfQs to the dealer if the hit & miss is too low\n            if len(daily_history[i])==window_size and np.mean(daily_history[i])<attrition_threshold:\n                active[i] = False\n        active_history.append(active.sum())\n        active_flags[t, :] = active.copy()\n    rfq_df = pd.DataFrame(records)\n    activity = pd.DataFrame(Y, columns=[f'client_{i}' for i in range(N_clients)])\n    active_df = pd.DataFrame(active_flags, columns=[f'client_{i}' for i in range(N_clients)])\n    return rfq_df, active_history, activity, active_df\n\n\n\n\nfrom scipy.special import betaln\nfrom scipy.optimize import minimize\n\n# ----------------------\n# Segment-level Model\n# ----------------------\ndef estimate_segment_params(activity_matrix):\n    def log_marginal_likelihood(D, alpha, beta, gamma, delta):\n        n, x = len(D), D.sum()\n        last = np.where(D==1)[0]\n        r = n - (last[-1]+1) if last.size>0 else n\n        logA = (betaln(alpha+x, beta+n-x)-betaln(alpha,beta)\n                +betaln(gamma, delta+n)-betaln(gamma,delta))\n        logB = [(betaln(alpha+x, beta+n-x-i)-betaln(alpha,beta)\n                 +betaln(gamma+1, delta+n-i)-betaln(gamma,delta))\n                for i in range(1, r+1)]\n        mags = [logA] + logB; m_max = max(mags)\n        return m_max + np.log(sum(np.exp(m - m_max) for m in mags))\n    def neg_ll(params):\n        alpha, beta, gamma, delta = np.exp(params)\n        return -sum(log_marginal_likelihood(activity_matrix.iloc[:end, j].values,\n                                             alpha, beta, gamma, delta)\n                    for j in range(activity_matrix.shape[1])\n                    for end in [activity_matrix.iloc[:,:].values.shape[0]])\n    res = minimize(neg_ll, np.log([1,1,1,1]), method='L-BFGS-B', bounds=[(-5,5)]*4)\n    return np.exp(res.x)\n\ndef attrition_probability(D, alpha, beta, gamma, delta):\n    n, x = len(D), D.sum()\n    last = np.where(D==1)[0]\n    r = n - (last[-1]+1) if last.size>0 else n\n    logA = (betaln(alpha+x, beta+n-x)-betaln(alpha,beta)\n            +betaln(gamma, delta+n)-betaln(gamma,delta))\n    logB = [(betaln(alpha+x, beta+n-x-i)-betaln(alpha,beta)\n             +betaln(gamma+1, delta+n-i)-betaln(gamma,delta))\n            for i in range(1, r+1)]\n    mags = [logA] + logB; m_max = max(mags)\n    logL = m_max + np.log(sum(np.exp(m - m_max) for m in mags))\n    P_active = np.exp(logA - logL)\n    return 1 - P_active\n\n\n\n\n\nfrom sklearn.metrics import (\n    confusion_matrix, accuracy_score,\n    precision_score, recall_score,\n    roc_curve, auc\n)\n\n# ----------------------\n# Workflow: Simulate, Train, Test\n# ----------------------\nT_train, T_test = 100, 100\nT_tot = T_train + T_test\nN_clients = 50\nrfq_all, active_all, Y_all, active_df = run_simulation(N_clients = N_clients, T=T_train+T_test)\n\nY_train = Y_all.iloc[:T_train].reset_index(drop=True)\nY_test  = Y_all.iloc[T_train:].reset_index(drop=True)\nrfq_test = rfq_all[rfq_all['time']>=T_train].copy()\nrfq_test['time'] -= T_train\nactive_test = active_all[T_train:]\n\nalpha, beta, gamma, delta = estimate_segment_params(Y_train)\nprint(f\"Params: α={alpha:.2f}, β={beta:.2f}, γ={gamma:.2f}, δ={delta:.2f}\")\n\n# ----------------------\n# Risk Scoring\n# ----------------------\nrisk_records = []\nfor j in range(Y_test.shape[1]):\n    hist = []\n    for t in range(T_test):\n        hist.append(Y_test.iloc[t, j])\n        D = np.array(hist, dtype=int)\n        p_inact = attrition_probability(D, alpha, beta, gamma, delta)\n        risk_records.append({\n            'time': t,\n            'client_id': j,\n            'p_inactive': p_inact,\n            'alert': p_inact > 0.5  # thresholded prediction\n        })\n\nrisk_df = pd.DataFrame(risk_records)\n\n# Label inactivity\nrisk_df['inactive'] = risk_df.apply(\n    lambda r: not active_df.loc[r['time'] + T_train, f'client_{r[\"client_id\"]}'],\n    axis=1\n)\n\n# ----------------------\n# Evaluation Metrics\n# ----------------------\n\n# Ground truth and predictions\ny_true   = risk_df['inactive']\ny_pred   = risk_df['alert']           # at 0.5 threshold\ny_scores = risk_df['p_inactive']      # raw probabilities\n\n# Confusion matrix\ntn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\nprint(\"Confusion Matrix:\")\nprint(f\"  TN={tn}, FP={fp}, FN={fn}, TP={tp}\")\n\n# Accuracy, Precision, Recall at 0.5\naccuracy  = accuracy_score(y_true, y_pred)\nprecision = precision_score(y_true, y_pred)\nrecall    = recall_score(y_true, y_pred)\nprint(f\"Accuracy : {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall   : {recall:.4f}\")\n\n# ROC curve & AUC\nfpr, tpr, thresholds = roc_curve(y_true, y_scores)\nroc_auc = auc(fpr, tpr)\nprint(f\"AUC      : {roc_auc:.4f}\")\n\n# Plot ROC\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], 'k--', label='Random')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc='lower right')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n# ----------------------\n# Plots: Hit Rate & Attrition Over Full Simulation\n# ----------------------\n# Active-client hit rate over time\nhit_rate_active = []\ndays = np.arange(T_tot)\nfor t in days:\n    act_clients = active_df.iloc[t]\n    active_ids = [int(col.split('_')[1]) for col, flag in act_clients.items() if flag]\n    daily = rfq_all[rfq_all['time'] == t]\n    rates = []\n    for cid in active_ids:\n        cr = daily[daily['client_id'] == cid]\n        if not cr.empty:\n            rates.append(cr['hit'].mean())\n    hit_rate_active.append(np.mean(rates) if rates else np.nan)\n\nplt.figure(figsize=(10,5))\nplt.plot(days, hit_rate_active, label='Avg Hit Rate', color='blue')\nplt.title('Average Hit Rate Over Time')\nplt.xlabel('Day'); plt.ylabel('Hit Rate'); plt.grid(True); plt.legend()\n\n# Attrition rate over time\nattr_rate = [100 * (Y_all.shape[1] - x) / Y_all.shape[1] for x in active_all]\nplt.figure(figsize=(10,5))\nplt.plot(days, attr_rate, label='% Inactive', color='red')\nplt.title('Cumulative Attrition Rate Over Time')\nplt.xlabel('Day'); plt.ylabel('% Inactive'); plt.grid(True); plt.legend()\n\n\n\n\n\n\n# ----------------------\n# Model Fit Visualization\n# ----------------------\n# Compare predicted attrition vs actual on test\npred_rate = risk_df.groupby('time')['p_inactive'].mean()\nactual_rate = [100*(N_clients-x)/N_clients for x in active_test]\nplt.figure(figsize=(10,5))\nplt.plot(pred_rate.index, 100*pred_rate.values, label='Predicted % Inactive')\nplt.plot(actual_rate, label='Actual % Inactive')\nplt.title('Predicted vs Actual Attrition (Test Set)')\nplt.xlabel('Day'); plt.ylabel('% Inactive'); plt.legend(); plt.grid(True)\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# ----------------------\n# Function: Plot Clients by ID (single figure with subplots)\n# ----------------------\ndef plot_clients_subplots(client_ids):\n    \"\"\"\n    For each client in client_ids, create a subplot in one figure:\n      - Left y-axis: active (blue=active, red=inactive) and trading days (green), with numeric ticks (0/1)\n      - Right y-axis: attrition probability over time\n      - Separate legends: left for 'Active', 'Inactive', 'Traded'; right for 'P(Inactive)'\n      - Title of each subplot shows the client ID\n    \"\"\"\n    n = len(client_ids)\n    fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(12, 4*n), sharex=True)\n    if n == 1:\n        axes = [axes]\n        \n    days = np.arange(T_test)\n\n    for ax1, cid in zip(axes, client_ids):\n        # gather P(inactive) values\n        p_vals = [\n            risk_df.loc[\n                (risk_df['client_id'] == cid) & (risk_df['time'] == t),\n                'p_inactive'\n            ].item()\n            for t in days\n        ]\n        # latent active flags and trading days\n        active_flag = active_df[T_train:][f'client_{cid}'].values\n        trade_days  = Y_test.iloc[:, cid]\n\n        # Left axis: Active vs Inactive\n        idx_active   = days[active_flag]\n        idx_inactive = days[~active_flag]\n        h_active   = ax1.scatter(idx_active,   [1]*len(idx_active),   c='blue', marker='s', alpha=0.3, label='Active')\n        h_inactive = ax1.scatter(idx_inactive, [1]*len(idx_inactive), c='red',  marker='s', alpha=0.3, label='Inactive')\n        # Left axis: actual trades (0 or 1)\n        h_trade = ax1.scatter(trade_days.index, trade_days.values,\n                              c='green', marker='o', label='Traded')\n\n        # Set numeric ticks on left y-axis\n        ax1.set_ylim(-0.2, 1.2)\n        ax1.set_yticks([0, 1])\n        ax1.set_ylabel('Activity (0/1)')\n        ax1.set_title(f'Client {cid} Activity and Attrition Prob')\n\n        # Left-axis legend\n        ax1.legend(handles=[h_active, h_inactive, h_trade],\n                   labels=['Active', 'Inactive', 'Traded'],\n                   loc='upper left')\n\n        # Right axis: P(Inactive)\n        ax2 = ax1.twinx()\n        h_prob, = ax2.plot(days, p_vals, c='black', label='P(Inactive)')\n        ax2.set_ylim(0, 1)\n        ax2.set_ylabel('P(Inactive)')\n\n        # Right-axis legend\n        ax2.legend(handles=[h_prob], loc='upper right')\n\n    axes[-1].set_xlabel('Day')\n    plt.tight_layout()\n    plt.show()\n\n# Example: plot select clients in one figure\n#plot_clients_subplots([3, 8, 9, 18])\nplot_clients_subplots([3, 8, 9, 30])\n\n\n\n\n","type":"content","url":"/notebooks/rfq-models#simulation-of-rfqs-arrival-and-client-attrition","position":5},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets","lvl3":"Simulation of client abnormal behavior","lvl2":"Generative models for the request for quote activity"},"type":"lvl3","url":"/notebooks/rfq-models#simulation-of-client-abnormal-behavior","position":6},{"hierarchy":{"lvl1":"Modelling RfQs in Dealer to Client Markets","lvl3":"Simulation of client abnormal behavior","lvl2":"Generative models for the request for quote activity"},"content":"\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import poisson, norm\nfrom collections import deque\n\ndef run_simulation(N_clients=50,\n                   T=200,\n                   reservation_price_mean=100,\n                   reservation_price_std_pct=0.1,\n                   lambda_mean=1,\n                   lambda_std=0.05,\n                   prior_mean=60,\n                   prior_std=10,\n                   reservation_price_noise_std=None,\n                   discount_trigger_pct=0.35,  # trigger threshold: 0.5 means 50% lower\n                   boosted_rate_factor = 10,\n                   hit_rate_target=0.4,\n                   window_size=10,\n                   attrition_threshold=0.1,\n                   random_seed=42):\n\n    \"\"\"\n    Simulate RFQ interactions with Bayesian quantile pricing and client attrition.\n    Adds a mechanism: if a client gets a quote at least `discount_trigger_pct` below their reservation price,\n    they double their RFQ rate until the dealer quotes above their reservation price.\n    \"\"\"\n\n    np.random.seed(random_seed)\n\n    # Generate distribution of reservation prices\n    reservation_prices = np.random.normal(reservation_price_mean,\n                                          reservation_price_mean * reservation_price_std_pct,\n                                          N_clients)\n\n    # Noise in reservation prices\n    fair_price = reservation_price_mean\n    res_price_noise_std = reservation_price_noise_std or (fair_price * reservation_price_std_pct * 2)\n    res_price_noise_var = res_price_noise_std ** 2\n\n    # Base RfQ intensities\n    lambdas = np.abs(np.random.normal(lambda_mean, lambda_std, N_clients))\n    base_lambdas = lambdas.copy()  # store for reset later\n\n    # Posterior beliefs\n    clients_posterior = {i: {'mean': prior_mean, 'var': prior_std ** 2,\n                             'n': 0, 'sum_obs': 0.0}\n                         for i in range(N_clients)}\n\n    z = norm.ppf(1 - hit_rate_target)\n    active = np.ones(N_clients, dtype=bool)\n\n    # Flags for tracking \"boosted\" mode\n    boosted = np.zeros(N_clients, dtype=bool)\n\n    # Track daily active/boosted status\n    active_flags = np.zeros((T, N_clients), dtype=bool)\n    boosted_flags = np.zeros((T, N_clients), dtype=bool)\n\n    daily_history = [deque(maxlen=window_size) for _ in range(N_clients)]\n    active_history = []\n    records = []\n    Y = np.zeros((T, N_clients), dtype=int)  # binary: whether client sent at least one RFQ that day\n\n    for t in range(T):\n        daily_hits = np.zeros(N_clients, dtype=int)\n        daily_reqs = np.zeros(N_clients, dtype=int)\n\n        for i in range(N_clients):\n            if not active[i]:\n                continue\n\n            n_rfq = poisson.rvs(lambdas[i])\n            if n_rfq > 0:\n                Y[t, i] = 1\n\n            for _ in range(n_rfq):\n                post = clients_posterior[i]\n                price = max(0.0, post['mean'] + np.sqrt(post['var'] + res_price_noise_var) * z)\n                r = norm.rvs(reservation_prices[i], res_price_noise_std)\n\n                # Trading happens when price <= reservation price\n                hit = price <= r\n                daily_reqs[i] += 1\n                daily_hits[i] += int(hit)\n\n                # Check discount trigger\n                if price <= (1 - discount_trigger_pct) * r:\n                    boosted[i] = True\n                    lambdas[i] = base_lambdas[i] * boosted_rate_factor\n                elif boosted[i] and price > r / (1-discount_trigger_pct):\n                    boosted[i] = False\n                    lambdas[i] = base_lambdas[i]\n\n                # Update posterior belief\n                post['n'] += 1\n                post['sum_obs'] += r\n                post_var = 1 / (1 / (prior_std ** 2) + post['n'] / res_price_noise_var)\n                post['var'] = post_var\n                post['mean'] = post_var * (prior_mean / (prior_std ** 2) +\n                                           post['sum_obs'] / res_price_noise_var)\n\n                records.append({'time': t,\n                                'client_id': i,\n                                'price': price,\n                                'reservation': r,\n                                'hit': hit,\n                                'boosted': boosted[i]})\n\n        # Update attrition status\n        for i in range(N_clients):\n            if not active[i] or daily_reqs[i] == 0:\n                continue\n            rate = daily_hits[i] / daily_reqs[i]\n            daily_history[i].append(rate)\n            if len(daily_history[i]) == window_size and np.mean(daily_history[i]) < attrition_threshold:\n                active[i] = False\n\n        active_history.append(int(active.sum()))\n        active_flags[t, :] = active.copy()\n        boosted_flags[t, :] = boosted.copy()\n\n    rfq_df = pd.DataFrame(records)\n    activity_df = pd.DataFrame(Y, columns=[f'client_{i}' for i in range(N_clients)])\n    active_df = pd.DataFrame(active_flags, columns=[f'client_{i}' for i in range(N_clients)])\n    boosted_df = pd.DataFrame(boosted_flags, columns=[f'client_{i}' for i in range(N_clients)])\n\n    return rfq_df, active_history, activity_df, active_df, boosted_df\n\n\n\n\n# Run simulation\nrfq_df, active_history, activity_df, active_df, boosted_df = run_simulation()\n\n# Plot active and boosted clients over time\nplt.figure(figsize=(10,5))\nplt.plot(active_df.sum(axis=1), label='Active Clients')\nplt.plot(boosted_df.sum(axis=1), label='Boosted Clients', linestyle='--')\nplt.xlabel('Day')\nplt.ylabel('Number of Clients')\nplt.title('Active vs Boosted Clients Over Time')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Heatmap of boosted status\nplt.figure(figsize=(12,6))\nplt.imshow(boosted_df.T, aspect='auto', cmap='Reds')\nplt.colorbar(label='Boosted Status (1=Yes, 0=No)')\nplt.xlabel('Day')\nplt.ylabel('Client ID')\nplt.title('Boosted Clients Over Time')\nplt.show()\n\n\n\n\n\n\nfrom scipy.special import betaln, gammaln\nfrom scipy.optimize import minimize\n\n# -----------------------------\n#  Beta-Binomial mixture MLE with shared mean\n# -----------------------------\n\ndef log_beta_binom_pmf(x, n, alpha, beta):\n    # log [ C(n,x) * B(alpha+x, beta+n-x) / B(alpha,beta) ]\n    return (\n        gammaln(n + 1) - gammaln(x + 1) - gammaln(n - x + 1)\n        + betaln(alpha + x, beta + n - x)\n        - betaln(alpha, beta)\n    )\n\ndef neg_log_likelihood(params, xs, ns):\n    # params = [logit_mu, log_k_g, log_k_b, logit_qg]\n    logit_mu, log_k_g, log_k_b, logit_qg = params\n    \n    mu = 1 / (1 + np.exp(-logit_mu))  # shared mean\n    k_g = np.exp(log_k_g) + 1e-8\n    k_b = np.exp(log_k_b) + 1e-8\n    qg  = 1 / (1 + np.exp(-logit_qg))\n\n    # convert to alpha, beta\n    alpha_g, beta_g = mu * k_g, (1 - mu) * k_g\n    alpha_b, beta_b = mu * k_b, (1 - mu) * k_b\n\n    # mixture likelihood per client\n    ll = []\n    for x, n in zip(xs, ns):\n        log_p_g = log_beta_binom_pmf(x, n, alpha_g, beta_g)\n        log_p_b = log_beta_binom_pmf(x, n, alpha_b, beta_b)\n        # log-sum-exp for mixture\n        a = np.log(qg) + log_p_g\n        b = np.log(1 - qg) + log_p_b\n        m = max(a, b)\n        ll_i = m + np.log(np.exp(a - m) + np.exp(b - m))\n        ll.append(ll_i)\n    return -np.sum(ll)\n\ndef fit_beta_mixture_mle(activity_df, train_days):\n    \"\"\"\n    activity_df: (T x N) binary DataFrame\n    train_days: number of days used for training (first half)\n    Returns dict with fitted parameters.\n    \"\"\"\n    T, N = activity_df.shape\n    A = activity_df.iloc[:train_days].values  # (train_days x N)\n    xs = A.sum(axis=0)  # successes per client in training\n    ns = np.full_like(xs, fill_value=train_days)\n\n    # rough start for mean\n    p_hat = xs.mean() / train_days\n    logit_mu0 = np.log(p_hat / (1 - p_hat + 1e-8))\n\n    start = np.array([\n        logit_mu0,\n        np.log(10.0),   # k_g initial concentration\n        np.log(2.0),    # k_b initial concentration\n        0.0             # logit(0.5)\n    ], dtype=float)\n\n    res = minimize(neg_log_likelihood, start, args=(xs, ns), method='L-BFGS-B')\n    logit_mu, log_k_g, log_k_b, logit_qg = res.x\n    \n    mu  = 1 / (1 + np.exp(-logit_mu))\n    k_g = np.exp(log_k_g) + 1e-8\n    k_b = np.exp(log_k_b) + 1e-8\n    \n    params = {\n        'mu':     float(mu),\n        'alpha_g': float(mu * k_g), 'beta_g': float((1 - mu) * k_g),\n        'alpha_b': float(mu * k_b), 'beta_b': float((1 - mu) * k_b),\n        'q_g':    float(1 / (1 + np.exp(-logit_qg))),\n        'success': bool(res.success),\n        'message': res.message\n    }\n    return params\n\ndef posterior_good(x, n, params):\n    a_g = params['alpha_g']; b_g = params['beta_g']\n    a_b = params['alpha_b']; b_b = params['beta_b']\n    q_g = params['q_g']\n\n    log_p_g = log_beta_binom_pmf(x, n, a_g, b_g)\n    log_p_b = log_beta_binom_pmf(x, n, a_b, b_b)\n\n    a = np.log(q_g) + log_p_g\n    b = np.log(1 - q_g) + log_p_b\n    m = max(a, b)\n    num = np.exp(a - m)\n    den = num + np.exp(b - m)\n    return float(num / den)\n\n\n\n\n# Fit on first half, removing first 25 days until it settles\nT = activity_df.shape[0]\nmid = T // 2\nparams = fit_beta_mixture_mle(activity_df[25:], train_days=mid)\n\n# Detect abnormalities on second half using a sliding window\nn_window = 10  # window size for detection, can be adjusted\nthreshold = 0.5  # classify as abnormal if p(good|D) < threshold\n\nN = activity_df.shape[1]\nabnormal_flags = np.zeros_like(activity_df.values, dtype=bool)\n\n# For each day t in the second half, use the last n_window days (bounded within the second half)\nfor t in range(mid, T):\n    start = max(mid, t - n_window + 1)\n    end = t + 1\n    window = activity_df.iloc[start:end].values  # (w x N)\n    n = window.shape[0]\n    x = window.sum(axis=0)\n    # Posterior for each client\n    p_good = np.array([posterior_good(int(xi), int(n), params) for xi in x])\n    abnormal_flags[t, :] = p_good < threshold\n\nabnormal_df = pd.DataFrame(abnormal_flags, columns=activity_df.columns, index=activity_df.index)\n\n\n\n\nparams\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import beta\n\ndef plot_fitted_betas(params, bins=200):\n    \"\"\"\n    Plot fitted good vs bad Beta distributions.\n    params: dict returned by fit_beta_mixture_mle\n    \"\"\"\n    a_g, b_g = params['alpha_g'], params['beta_g']\n    a_b, b_b = params['alpha_b'], params['beta_b']\n    q_g = params['q_g']\n\n    # Grid\n    x = np.linspace(0, 1, bins)\n\n    # Densities\n    pdf_g = beta.pdf(x, a_g, b_g)\n    pdf_b = beta.pdf(x, a_b, b_b)\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(x, pdf_g, label=f\"Good (α={a_g:.2f}, β={b_g:.2f})\", lw=2, color=\"blue\")\n    plt.plot(x, pdf_b, label=f\"Bad  (α={a_b:.2f}, β={b_b:.2f})\", lw=2, color=\"red\")\n    plt.fill_between(x, pdf_g, alpha=0.2, color=\"blue\")\n    plt.fill_between(x, pdf_b, alpha=0.2, color=\"red\")\n\n    plt.title(\"Fitted Beta Distributions (Good vs Bad)\")\n    plt.xlabel(\"Success probability\")\n    plt.ylabel(\"Density\")\n    plt.legend()\n    plt.grid(True, ls=\"--\", alpha=0.5)\n    plt.show()\n\n\nplot_fitted_betas(params)\n\n\n\n# Use a previous window (exclude day t) when computing P(good|D)\nn_window = 10\nthreshold = 0.5\n\ndef build_pgood_panel(activity_df, boosted_df, active_df, params, mid, n_window, threshold):\n    T, N = activity_df.shape\n    rows = []\n    # Start at mid + n_window to ensure a full *previous* window exists\n    for t in range(mid + n_window, T):\n        start = t - n_window\n        end = t  # exclude day t\n        window = activity_df.iloc[start:end]  # (n_window x N)\n        x = window.sum(axis=0).values.astype(int)  # successes per client in previous window\n        n = n_window\n        # compute p_good for each client\n        p_goods = []\n        for j in range(N):\n            p = posterior_good(int(x[j]), int(n), params)\n            p_goods.append(p)\n        p_goods = np.array(p_goods)\n        # flags at time t\n        boosted_t = boosted_df.iloc[t].values.astype(bool)\n        active_t  = active_df.iloc[t].values.astype(bool)\n        inactive_t = ~active_t\n        traded_today = activity_df.iloc[t].values.astype(int)\n        for j in range(N):\n            rows.append({\n                'day': t,\n                'client_id': j,\n                'x': int(x[j]),\n                'n': int(n),\n                'p_good': float(p_goods[j]),\n                'abnormal': bool(p_goods[j] < threshold),\n                'boosted': bool(boosted_t[j]),\n                'inactive': bool(inactive_t[j]),\n                'traded_today': int(traded_today[j])\n            })\n    panel = pd.DataFrame(rows)\n    return panel\n\npanel_df = build_pgood_panel(activity_df, boosted_df, active_df, params, mid, n_window, threshold)\n\n# Correctly aligned daily summary for second half *with windowing*\ndays = sorted(panel_df['day'].unique())\ndaily_summary = panel_df.groupby('day').agg(\n    abnormal_count=('abnormal', 'sum'),\n    boosted_count=('boosted', 'sum'),\n    inactive_count=('inactive', 'sum')\n).reset_index()\n\n# Overlap summary on client-day panel\noverlap_summary = {\n    'days_evaluated': int(len(days)),\n    'clients': int(N),\n    'total_client_days': int(panel_df.shape[0]),\n    'abnormal_client_days': int(panel_df['abnormal'].sum()),\n    'boosted_client_days': int(panel_df['boosted'].sum()),\n    'inactive_client_days': int(panel_df['inactive'].sum()),\n    'abnormal_and_boosted': int((panel_df['abnormal'] & panel_df['boosted']).sum()),\n    'abnormal_and_inactive': int((panel_df['abnormal'] & panel_df['inactive']).sum()),\n    'abnormal_not_boosted_or_inactive': int((panel_df['abnormal'] & ~panel_df['boosted'] & ~panel_df['inactive']).sum())\n}\noverlap_df2 = pd.DataFrame([overlap_summary])\n\n# Heatmap of p_good by client (days x clients) for visual inspection\n# Build a matrix with rows=days, cols=clients\ndays_idx = daily_summary['day'].values\npgood_mat = np.full((len(days_idx), N), np.nan)\nfor i, t in enumerate(days_idx):\n    slice_t = panel_df[panel_df['day'] == t].sort_values('client_id')\n    pgood_mat[i, :] = slice_t['p_good'].values\n\nplt.figure(figsize=(12,6))\nplt.imshow(pgood_mat.T, aspect='auto')\nplt.colorbar(label='P(good | previous window)')\nplt.xlabel('Day index in second half')\nplt.ylabel('Client ID')\nplt.title('Per-client P(good|D) over time (previous window)')\nplt.show()\n\n   \n\n\n\nfrom matplotlib.lines import Line2D\n\ndef plot_clients_activity_prob(clients, df):\n    \"\"\"\n    One figure with one row per client.\n    On boosted days, only the 'Boosted' marker is shown (others hidden).\n    \"\"\"\n    clients = list(clients)\n    n = len(clients)\n    if n == 0:\n        return\n    \n    fig, axes = plt.subplots(n, 1, figsize=(12, 2.8 * n), sharex=True, constrained_layout=True)\n    if n == 1:\n        axes = [axes]\n    \n    # Build a single, global legend\n    legend_elems = [\n        Line2D([0],[0], marker='s', linestyle='None', color='blue',  alpha=0.3, label='Active'),\n        Line2D([0],[0], marker='x', linestyle='None', color='gray',  alpha=0.3, label='Inactive'),\n        Line2D([0],[0], marker='o', linestyle='None', color='red',   alpha=0.5, label='Traded'),\n        Line2D([0],[0], marker='^', linestyle='None', color='green', alpha=0.7, label='Boosted'),\n        Line2D([0],[0], linestyle='-', color='black', label='P(good|D)'),\n    ]\n    \n    x_min, x_max = None, None\n    for ax, cid in zip(axes, clients):\n        df_c = df[df['client_id'] == cid].sort_values('day').copy()\n        if df_c.empty:\n            ax.set_title(f\"Client {cid} (no data)\")\n            ax.set_yticks([])\n            continue\n        \n        # Masks\n        boosted_mask  = df_c['boosted']\n        active_mask   = (~df_c['inactive']) & (~boosted_mask)\n        inactive_mask = df_c['inactive'] & (~boosted_mask)\n        traded_mask   = (df_c['traded_today'] == 1) & (~boosted_mask)\n        \n        # Scatter statuses\n        ax.scatter(df_c['day'][active_mask],   [1]*active_mask.sum(),   c='blue',  marker='s', alpha=0.3)\n        ax.scatter(df_c['day'][inactive_mask], [1]*inactive_mask.sum(), c='gray',  marker='x', alpha=0.3)\n        ax.scatter(df_c['day'][traded_mask],   [1]*traded_mask.sum(),   c='red',   marker='o', alpha=0.5)\n        ax.scatter(df_c['day'][boosted_mask],  [1]*boosted_mask.sum(),  c='green', marker='^', alpha=0.7, zorder=3)\n        \n        ax.set_yticks([])\n        ax.set_ylim(0.5, 1.5)\n        \n        # p_good\n        ax2 = ax.twinx()\n        ax2.plot(df_c['day'], df_c['p_good'], color='black', lw=2)\n        ax2.set_ylim(0, 1)\n        ax2.set_ylabel(\"P(good|D)\")\n        \n        ax.set_title(f\"Client {cid}\")\n        \n        # Track x-lims\n        dmin, dmax = df_c['day'].min(), df_c['day'].max()\n        x_min = dmin if x_min is None else min(x_min, dmin)\n        x_max = dmax if x_max is None else max(x_max, dmax)\n    \n    axes[-1].set_xlabel(\"Day\")\n    if x_min is not None and x_max is not None:\n        axes[-1].set_xlim(x_min, x_max)\n    \n    fig.suptitle(\"Selected clients activity\", y=1.02)\n    \n    # Move legend below figure\n    fig.legend(handles=legend_elems, loc='lower center', ncol=5, frameon=False, bbox_to_anchor=(0.5, -0.03))\n    plt.show()\n\n\nplot_clients_activity_prob([30,31,35, 37], panel_df)\n\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\ndef _cm_df(cm, labels=(\"Pred 0\", \"Pred 1\"), index=(\"True 0\", \"True 1\")):\n    \"\"\"Pretty dataframe for a 2x2 confusion matrix.\"\"\"\n    return pd.DataFrame(cm, index=index, columns=labels)\n\ndef evaluate_confusions(panel_df, normalize=False):\n    \"\"\"\n    Evaluate confusion matrices without recomputing thresholds.\n\n    Uses:\n      - df['abnormal'] as the model's predicted abnormal label (already thresholded).\n      - df['inactive'] and df['boosted'] as operational labels.\n      - Global abnormality = (inactive OR boosted).\n\n    Returns dict with both raw arrays and pretty DataFrames.\n    Set normalize=True to return per-true-class rates instead of counts.\n    \"\"\"\n    df = panel_df.copy()\n\n    # Ensure ints\n    y_abnormal = df[\"abnormal\"].astype(int).values\n    y_inactive = df[\"inactive\"].astype(int).values\n    y_boosted  = df[\"boosted\"].astype(int).values\n\n    # Global abnormality (prediction-side for the overall CM)\n    y_global_abn = (df[\"inactive\"] | df[\"boosted\"]).astype(int).values\n\n    # Normalization mode for sklearn\n    norm_mode = \"true\" if normalize else None\n\n    results = {}\n\n    # 1) Inactive (truth) vs Abnormal (model)\n    cm_inactive = confusion_matrix(y_inactive, y_abnormal, labels=[0,1], normalize=norm_mode)\n    results[\"inactive_vs_abnormal\"] = cm_inactive\n    results[\"inactive_vs_abnormal_df\"] = _cm_df(cm_inactive)\n\n    # 2) Boosted (truth) vs Abnormal (model)\n    cm_boosted = confusion_matrix(y_boosted, y_abnormal, labels=[0,1], normalize=norm_mode)\n    results[\"boosted_vs_abnormal\"] = cm_boosted\n    results[\"boosted_vs_abnormal_df\"] = _cm_df(cm_boosted)\n\n    # 3) OVERALL: Abnormal (truth) vs Global abnormality (prediction = inactive OR boosted)\n    cm_overall = confusion_matrix(y_abnormal, y_global_abn, labels=[0,1], normalize=norm_mode)\n    results[\"overall_abnormal_vs_global\"] = cm_overall\n    results[\"overall_abnormal_vs_global_df\"] = _cm_df(cm_overall)\n\n    return results\n\n# Example usage:\ncm_results = evaluate_confusions(panel_df, normalize=False)\nprint(\"Inactive (truth) vs Abnormal (model):\\n\", cm_results[\"inactive_vs_abnormal_df\"], \"\\n\")\nprint(\"Boosted (truth) vs Abnormal (model):\\n\", cm_results[\"boosted_vs_abnormal_df\"], \"\\n\")\nprint(\"Overall: Abnormal (truth) vs Global (inactive OR boosted) prediction:\\n\", cm_results[\"overall_abnormal_vs_global_df\"])\n\n\n","type":"content","url":"/notebooks/rfq-models#simulation-of-client-abnormal-behavior","position":7},{"hierarchy":{"lvl1":"Stochastic Calculus"},"type":"lvl1","url":"/notebooks/stochastic-calculus","position":0},{"hierarchy":{"lvl1":"Stochastic Calculus"},"content":"","type":"content","url":"/notebooks/stochastic-calculus","position":1},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl2":"Deterministic dynamical systems"},"type":"lvl2","url":"/notebooks/stochastic-calculus#deterministic-dynamical-systems","position":2},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl2":"Deterministic dynamical systems"},"content":"","type":"content","url":"/notebooks/stochastic-calculus#deterministic-dynamical-systems","position":3},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Numerical solution of dynamical systems: inflation targetting","lvl2":"Deterministic dynamical systems"},"type":"lvl3","url":"/notebooks/stochastic-calculus#numerical-solution-of-dynamical-systems-inflation-targetting","position":4},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Numerical solution of dynamical systems: inflation targetting","lvl2":"Deterministic dynamical systems"},"content":"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef simulate_inflation(theta, pi_hat, pi_t0, Delta, N):\n    # Time grid\n    time = np.linspace(0, N * Delta, N + 1)\n\n    # Arrays to store the results\n    pi_forward = np.zeros(N + 1)\n    pi_backward = np.zeros(N + 1)\n    pi_analytical = np.zeros(N + 1)\n\n    # Initial condition\n    pi_forward[0] = pi_t0\n    pi_backward[0] = pi_t0\n\n    # Analytical Solution\n    pi_analytical = pi_hat + (pi_t0 - pi_hat) * np.exp(-theta * time)\n\n    # Forward Euler Scheme\n    for i in range(N):\n        pi_forward[i + 1] = pi_forward[i] + Delta * theta * (pi_hat - pi_forward[i])\n\n    # Backward Euler Scheme\n    for i in range(N):\n        pi_backward[i + 1] = (pi_backward[i] + Delta * theta * pi_hat) / (1 + Delta * theta)\n\n    return time, pi_forward, pi_backward, pi_analytical\n\n# Parameters for normal regime\ntheta_normal = 0.1\nDelta_normal = 0.1\npi_hat = 2.0\npi_t0 = 5.0\nN = 100\n\n# Parameters for challenging regime\ntheta_challenging = 10.0\nDelta_challenging = 0.1\n\n# Simulate for normal regime\ntime_normal, pi_forward_normal, pi_backward_normal, pi_analytical_normal = simulate_inflation(\n    theta_normal, pi_hat, pi_t0, Delta_normal, N)\n\n# Simulate for challenging regime\ntime_challenging, pi_forward_challenging, pi_backward_challenging, pi_analytical_challenging = simulate_inflation(\n    theta_challenging, pi_hat, pi_t0, Delta_challenging, N)\n\n# Plot the results for normal regime\nplt.figure(figsize=(12, 8))\nplt.plot(time_normal, pi_forward_normal, label='Forward Euler (Normal)', marker='o', linestyle='--')\nplt.plot(time_normal, pi_backward_normal, label='Backward Euler (Normal)', marker='x', linestyle='--')\nplt.plot(time_normal, pi_analytical_normal, label='Analytical Solution (Normal)', linestyle='-', linewidth=2)\nplt.xlabel('Time')\nplt.ylabel('Inflation Rate')\nplt.title('Inflation Targeting Model Simulation (Normal Regime)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the results for challenging regime\nplt.figure(figsize=(12, 8))\nplt.plot(time_challenging, pi_forward_challenging, label='Forward Euler (Challenging)', marker='o', linestyle='--')\nplt.plot(time_challenging, pi_backward_challenging, label='Backward Euler (Challenging)', marker='x', linestyle='--')\nplt.plot(time_challenging, pi_analytical_challenging, label='Analytical Solution (Challenging)', linestyle='-', linewidth=2)\nplt.xlabel('Time')\nplt.ylabel('Inflation Rate')\nplt.title('Inflation Targeting Model Simulation (Challenging Regime)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n","type":"content","url":"/notebooks/stochastic-calculus#numerical-solution-of-dynamical-systems-inflation-targetting","position":5},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl2":"The Wiener Process"},"type":"lvl2","url":"/notebooks/stochastic-calculus#the-wiener-process","position":6},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl2":"The Wiener Process"},"content":"","type":"content","url":"/notebooks/stochastic-calculus#the-wiener-process","position":7},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Simulation of the Wiener process (univariate)","lvl2":"The Wiener Process"},"type":"lvl3","url":"/notebooks/stochastic-calculus#simulation-of-the-wiener-process-univariate","position":8},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Simulation of the Wiener process (univariate)","lvl2":"The Wiener Process"},"content":"\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Set the parameters for the Wiener process\nT = 1.0  # total time\nn = 1000  # number of steps\ndt = T / n  # time increment\nt = np.linspace(0, T, n+1)  # time points\n\n# Number of paths to simulate\nnum_paths = 5\n\n# Set up the plot\nplt.figure(figsize=(10, 6))\n\n# Simulate multiple paths\nfor _ in range(num_paths):\n    W = np.zeros(n+1)  # Initialize the Wiener process for each path\n    for i in range(n):\n        W[i+1] = W[i] + np.sqrt(dt) * np.random.randn()\n    plt.plot(t, W, label=f'Path {_+1}')\n\n# Customize the plot\nplt.title('Sample Paths of the Wiener Process')\nplt.xlabel('Time t')\nplt.ylabel('Wiener Process W(t)')\nplt.grid(True)\n\n# Show the plot\nplt.show()\n\n\n\n\n","type":"content","url":"/notebooks/stochastic-calculus#simulation-of-the-wiener-process-univariate","position":9},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl4":"Simulation using Gaussian processes","lvl3":"Simulation of the Wiener process (univariate)","lvl2":"The Wiener Process"},"type":"lvl4","url":"/notebooks/stochastic-calculus#simulation-using-gaussian-processes","position":10},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl4":"Simulation using Gaussian processes","lvl3":"Simulation of the Wiener process (univariate)","lvl2":"The Wiener Process"},"content":"\n\n    import GPy\n    import numpy as np\n    import matplotlib.pyplot as plt\n\n    # Time points at which to sample the Wiener process\n    t = np.linspace(0, 1, 1000).reshape(-1, 1)\n\n    # Define the Brownian motion kernel\n    brownian_kernel = GPy.kern.Brownian(input_dim=1, variance=1.0)\n\n    # Create a GP model with zero mean function\n    mean_function = GPy.mappings.Constant(input_dim=1, output_dim=1, value=0)\n    model = GPy.models.GPRegression(t, np.zeros_like(t), kernel=brownian_kernel, mean_function=mean_function)\n\n    # Ensure correct model optimization\n    model.optimize()\n\n    # Sample paths from the GP model\n    num_samples = 5\n    samples = model.posterior_samples_f(t, size=num_samples)\n\n    # Plot the sampled paths\n    plt.figure(figsize=(10, 6))\n    for i in range(num_samples):\n        plt.plot(t, samples[:, :, i], label=f'Sample {i+1}')\n    plt.title('Sampled Paths from a Wiener Process using Gaussian Processes')\n    plt.xlabel('Time')\n    plt.ylabel('W(t)')\n    plt.show()\n\n\n\n\n","type":"content","url":"/notebooks/stochastic-calculus#simulation-using-gaussian-processes","position":11},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl2":"Brownian motion with drift"},"type":"lvl2","url":"/notebooks/stochastic-calculus#brownian-motion-with-drift","position":12},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl2":"Brownian motion with drift"},"content":"","type":"content","url":"/notebooks/stochastic-calculus#brownian-motion-with-drift","position":13},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Simulation using discrete paths","lvl2":"Brownian motion with drift"},"type":"lvl3","url":"/notebooks/stochastic-calculus#simulation-using-discrete-paths","position":14},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Simulation using discrete paths","lvl2":"Brownian motion with drift"},"content":"\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Set the parameters for Brownian motion with drift and volatility\nT = 1.0    # total time\nn = 1000   # number of steps\ndt = T / n  # time increment\nt = np.linspace(0, T, n+1)  # time points\n\n# Brownian motion parameters\nmu = 5    # drift\nsigma = 2.0  # volatility\n\n# Number of paths to simulate\nnum_paths = 5\n\n# Set up the plot\nplt.figure(figsize=(10, 6))\n\n# Simulate multiple paths\nfor _ in range(num_paths):\n    X = np.zeros(n+1)  # Initialize the process for each path\n    for i in range(n):\n        X[i+1] = X[i] + mu * dt + sigma * np.sqrt(dt) * np.random.randn()\n    plt.plot(t, X, label=f'Path {_+1}')\n\n# Customize the plot\nplt.title('Sample Paths of Brownian Motion with drift')\nplt.xlabel('Time t')\nplt.ylabel('X(t)')\nplt.grid(True)\n\n# Show the plot\nplt.show()\n\n\n\n\n","type":"content","url":"/notebooks/stochastic-calculus#simulation-using-discrete-paths","position":15},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Simulation using Gaussian Processes","lvl2":"Brownian motion with drift"},"type":"lvl3","url":"/notebooks/stochastic-calculus#simulation-using-gaussian-processes-1","position":16},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Simulation using Gaussian Processes","lvl2":"Brownian motion with drift"},"content":"\n\nimport GPy\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters for Brownian motion with drift\nS0 = 0.0    # Initial value\nmu = 0.5    # Drift coefficient\nsigma = 0.3  # Volatility\nT = 1.0    # Total time\nn = 1000   # Number of steps\nt = np.linspace(0, T, n+1).reshape(-1, 1)  # Time points\n\n# Define the custom kernel for Brownian motion\nclass BrownianMotionKernel(GPy.kern.Kern):\n    def __init__(self, input_dim, sigma=1.0, active_dims=None):\n        super(BrownianMotionKernel, self).__init__(input_dim, active_dims, 'Brownian')\n        self.sigma = GPy.core.Param('sigma', sigma)\n        self.link_parameters(self.sigma)\n    \n    def K(self, X, X2=None):\n        if X2 is None:\n            X2 = X\n        t1 = X\n        t2 = X2.T\n        # Covariance function of Brownian motion: sigma^2 * min(s, t)\n        cov_matrix = self.sigma**2 * np.minimum(t1, t2)\n        return cov_matrix\n    \n    def Kdiag(self, X):\n        return self.sigma**2 * X[:, 0]\n    \n    # No-op for gradient update since we are not optimizing the kernel\n    def update_gradients_full(self, dL_dK, X, X2=None):\n        pass\n    \n    # No-op for kernel gradient since we are not optimizing the kernel\n    def gradients_X(self, dL_dK, X, X2=None):\n        return np.zeros(X.shape)\n\n# Custom mean function to include drift\nclass BrownianMotionMean(GPy.core.Mapping):\n    def __init__(self, input_dim, output_dim, S0=0.0, mu=0.0):\n        super(BrownianMotionMean, self).__init__(input_dim, output_dim)\n        self.S0 = S0  # Initial value\n        self.mu = mu  # Drift coefficient\n        \n    def f(self, X):\n        # Mean function: S0 + mu * t\n        return self.S0 + self.mu * X\n    \n    def update_gradients(self, dL_df, X):\n        pass\n\n    def gradients_X(self, dL_df, X):\n        return np.zeros(X.shape)\n\n# Instantiate the Brownian motion kernel with parameters\nbm_kernel = BrownianMotionKernel(input_dim=1, sigma=sigma)\n\n# Instantiate the mean function with S0 and mu\nmean_function = BrownianMotionMean(input_dim=1, output_dim=1, S0=S0, mu=mu)\n\n# Provide the initial observation at t=0\nt_data = np.array([[0.0]])\nY_data = np.array([[S0]])\n\n# Create a GP model with the custom mean function and Brownian motion kernel\nmodel = GPy.models.GPRegression(t_data, Y_data, kernel=bm_kernel, mean_function=mean_function)\n\n# Set noise variance to a negligible value since we're simulating without observation noise\nmodel.likelihood.variance.fix(1e-10)\n\n# Sample paths from the GP model\nnum_samples = 5\nsamples = model.posterior_samples_f(t, size=num_samples)\n\n# Plot the sampled paths\nplt.figure(figsize=(10, 6))\nfor i in range(num_samples):\n    plt.plot(t, samples[:, :, i], label=f'Path {i+1}')\nplt.title(f'Sampled Paths from Brownian Motion with Drift')\nplt.xlabel('Time t')\nplt.ylabel('S(t)')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n","type":"content","url":"/notebooks/stochastic-calculus#simulation-using-gaussian-processes-1","position":17},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl2":"Arithmetic Average of the Brownian Motion"},"type":"lvl2","url":"/notebooks/stochastic-calculus#arithmetic-average-of-the-brownian-motion","position":18},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl2":"Arithmetic Average of the Brownian Motion"},"content":"\n\n","type":"content","url":"/notebooks/stochastic-calculus#arithmetic-average-of-the-brownian-motion","position":19},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Simulation using discrete paths","lvl2":"Arithmetic Average of the Brownian Motion"},"type":"lvl3","url":"/notebooks/stochastic-calculus#simulation-using-discrete-paths-1","position":20},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Simulation using discrete paths","lvl2":"Arithmetic Average of the Brownian Motion"},"content":"\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Set the parameters for Brownian motion with drift and volatility\nT = 1.0    # total time\nn = 1000   # number of steps\ndt = T / n  # time increment\nt = np.linspace(dt, T, n)  # time points starting from dt to avoid division by zero\n\n# Brownian motion parameters\nS0 = 0.0   # initial value of S_u\nmu = 0.5   # drift coefficient\nsigma = 0.3  # volatility\n\n# Number of paths to simulate\nnum_paths = 5\n\n# Initialize array to store M_t for each path\nM_t_paths = np.zeros((n, num_paths))\n\n# Set up the plot\nplt.figure(figsize=(12, 6))\n\n# Simulate multiple paths\nfor path in range(num_paths):\n    # Initialize S_u for each path\n    S = np.zeros(n + 1)  # n+1 because we include S0\n    S[0] = S0\n    # Simulate S_u using Euler-Maruyama method\n    for i in range(n):\n        dW = np.sqrt(dt) * np.random.randn()  # Brownian increment\n        S[i + 1] = S[i] + mu * dt + sigma * dW\n    # Compute cumulative sum to approximate the integral\n    cumulative_S = np.cumsum(S[:-1]) * dt  # Exclude the last S[n+1]\n    # Compute M_t at each time point\n    M_t = cumulative_S / t\n    M_t_paths[:, path] = M_t\n    # Plot the path\n    plt.plot(t, M_t, label=f'Path {path + 1}')\n\n# Customize the plot\nplt.title('Sample Paths of the arithmetic mean of a Brownian motion')\nplt.xlabel('Time t')\nplt.ylabel('M(t)')\nplt.grid(True)\nplt.legend()\n\n# Show the plot\nplt.show()\n\n\n\n\n","type":"content","url":"/notebooks/stochastic-calculus#simulation-using-discrete-paths-1","position":21},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Simulation using Gaussian Processes","lvl2":"Arithmetic Average of the Brownian Motion"},"type":"lvl3","url":"/notebooks/stochastic-calculus#simulation-using-gaussian-processes-2","position":22},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Simulation using Gaussian Processes","lvl2":"Arithmetic Average of the Brownian Motion"},"content":"\n\nimport GPy\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters for Brownian motion with drift\nS0 = 0.0    # Initial value\nmu = 0.5    # Drift coefficient\nsigma = 0.3  # Volatility\nT = 1.0    # Total time\nn = 1000   # Number of steps\nt = np.linspace(1e-6, T, n+1).reshape(-1, 1)  # Time points, avoid t=0 to prevent division by zero\n\n# Define the custom kernel based on the provided covariance function\nclass TimeAverageKernel(GPy.kern.Kern):\n    def __init__(self, input_dim, sigma=1.0, active_dims=None):\n        super(TimeAverageKernel, self).__init__(input_dim, active_dims, 'TimeAverage')\n        self.sigma = GPy.core.Param('sigma', sigma)\n        self.link_parameters(self.sigma)\n        \n    def K(self, X, X2=None):\n        if X2 is None:\n            X2 = X\n        t1 = X[:, 0].reshape(-1, 1)  # Column vector\n        t2 = X2[:, 0].reshape(1, -1)  # Row vector\n        t_min = np.minimum(t1, t2)\n        t_max = np.maximum(t1, t2)\n        # Compute the covariance matrix using the given formula\n        cov_matrix = self.sigma ** 2 * (t_min / 2) * (1 - t_min / (3 * t_max))\n        return cov_matrix\n        \n    def Kdiag(self, X):\n        t = X[:, 0]\n        # Compute the diagonal elements of the covariance matrix\n        cov_diag = self.sigma ** 2 * (t / 2) * (1 - t / (3 * t))\n        # Simplify the expression\n        cov_diag = self.sigma ** 2 * (t / 2) * (1 - 1 / 3)\n        cov_diag = self.sigma ** 2 * (t / 2) * (2 / 3)\n        cov_diag = self.sigma ** 2 * t * (1 / 3)\n        return cov_diag\n        \n    def update_gradients_full(self, dL_dK, X, X2=None):\n        pass\n        \n    def gradients_X(self, dL_dK, X, X2=None):\n        return np.zeros(X.shape)\n\n# Custom mean function to include drift\nclass TimeAverageMean(GPy.core.Mapping):\n    def __init__(self, input_dim, output_dim, S0=0.0, mu=0.0):\n        super(TimeAverageMean, self).__init__(input_dim, output_dim)\n        self.S0 = S0  # Initial value\n        self.mu = mu  # Drift coefficient\n        \n    def f(self, X):\n        t = X[:, 0]\n        mean = self.S0 + 0.5 * self.mu * t\n        return mean.reshape(-1, 1)\n        \n    def update_gradients(self, dL_df, X):\n        pass\n    \n    def gradients_X(self, dL_df, X):\n        return np.zeros(X.shape)\n\n# Instantiate the Time Average kernel with parameters\nta_kernel = TimeAverageKernel(input_dim=1, sigma=sigma)\n\n# Instantiate the mean function with S0 and mu\nmean_function = TimeAverageMean(input_dim=1, output_dim=1, S0=S0, mu=mu)\n\n# Provide the initial observation at t=1e-6 (approaching zero)\nt_data = np.array([[1e-6]])\nY_data = mean_function.f(t_data)  # Mean at t=1e-6\n\n# Create a GP model with the custom mean function and Time Average kernel\nmodel = GPy.models.GPRegression(t_data, Y_data, kernel=ta_kernel, mean_function=mean_function)\n\n# Set noise variance to a negligible value since we're simulating without observation noise\nmodel.likelihood.variance.fix(1e-10)\n\n# Sample paths from the GP model\nnum_samples = 5\nsamples = model.posterior_samples_f(t, size=num_samples)\n\n# Plot the sampled paths\nplt.figure(figsize=(12, 6))\nfor i in range(num_samples):\n    plt.plot(t, samples[:, :, i], label=f'Path {i+1}')\nplt.title('Sampled Paths of the arithmetic mean of a Brownian motion')\nplt.xlabel('Time t')\nplt.ylabel('M(t)')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n","type":"content","url":"/notebooks/stochastic-calculus#simulation-using-gaussian-processes-2","position":23},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl2":"Geometric Brownian Motion"},"type":"lvl2","url":"/notebooks/stochastic-calculus#geometric-brownian-motion","position":24},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl2":"Geometric Brownian Motion"},"content":"","type":"content","url":"/notebooks/stochastic-calculus#geometric-brownian-motion","position":25},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Simulation using discrete paths","lvl2":"Geometric Brownian Motion"},"type":"lvl3","url":"/notebooks/stochastic-calculus#simulation-using-discrete-paths-2","position":26},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Simulation using discrete paths","lvl2":"Geometric Brownian Motion"},"content":"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef simulate_gbm(S0, mu, sigma, T, N, num_paths):\n    dt = T / N\n    t = np.linspace(0, T, N)\n    paths = []\n    for _ in range(num_paths):\n        W = np.random.normal(0, np.sqrt(dt), N)\n        W = np.cumsum(W)  # cumulative sum to represent Brownian motion\n        S = S0 * np.exp((mu - 0.5 * sigma**2) * t + sigma * W)\n        paths.append(S)\n    return t, paths\n\n# Parameters\nS0 = 100  # Initial value of the asset\nmu = 0.05  # Drift\nsigma = 0.2  # Volatility\nT = 1  # Time horizon (1 year)\nN = 1000  # Number of time steps\nnum_paths = 5  # Number of paths to simulate\n\nt, paths = simulate_gbm(S0, mu, sigma, T, N, num_paths)\n\n# Plotting the simulated GBM paths\nplt.figure(figsize=(10, 6))\nfor i, S in enumerate(paths):\n    plt.plot(t, S, label=f'Path {i + 1}')\nplt.xlabel('Time')\nplt.ylabel('Price')\nplt.title('Sampled paths from Geometric Brownian Motion')\nplt.grid(True)\nplt.show()\n\n\n\n","type":"content","url":"/notebooks/stochastic-calculus#simulation-using-discrete-paths-2","position":27},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Simulation using Gaussian Processes","lvl2":"Geometric Brownian Motion"},"type":"lvl3","url":"/notebooks/stochastic-calculus#simulation-using-gaussian-processes-3","position":28},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Simulation using Gaussian Processes","lvl2":"Geometric Brownian Motion"},"content":"\n\nimport GPy\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters for Geometric Brownian Motion\nS0 = 100  # Initial value of the asset\nmu = 0.05  # Drift\nsigma = 0.2  # Volatility\nT = 1  # Time horizon (1 year)\nn = 1000   # Number of steps\nt = np.linspace(0, T, n+1).reshape(-1, 1)  # Time points\n\n# Define the custom kernel for Brownian motion\nclass GBMKernel(GPy.kern.Kern):\n    def __init__(self, input_dim, sigma=1.0, active_dims=None):\n        super(GBMKernel, self).__init__(input_dim, active_dims, 'Brownian')\n        self.sigma = GPy.core.Param('sigma', sigma)\n        self.link_parameters(self.sigma)\n    \n    def K(self, X, X2=None):\n        if X2 is None:\n            X2 = X\n        t1 = X\n        t2 = X2.T\n        # Covariance function of Brownian motion: sigma^2 * min(s, t)\n        cov_matrix = self.sigma**2 * np.minimum(t1, t2)\n        return cov_matrix\n    \n    def Kdiag(self, X):\n        return self.sigma**2 * X[:, 0]\n    \n    # No-op for gradient update since we are not optimizing the kernel\n    def update_gradients_full(self, dL_dK, X, X2=None):\n        pass\n    \n    # No-op for kernel gradient since we are not optimizing the kernel\n    def gradients_X(self, dL_dK, X, X2=None):\n        return np.zeros(X.shape)\n\n# Custom mean function to include drift\nclass GBMMean(GPy.core.Mapping):\n    def __init__(self, input_dim, output_dim, S0=0.0, mu=0.0):\n        super(GBMMean, self).__init__(input_dim, output_dim)\n        self.S0 = S0  # Initial value\n        self.mu = mu  # Drift coefficient\n        \n    def f(self, X):\n        # Mean function: S0 + mu * t\n        return self.S0 + self.mu * X\n    \n    def update_gradients(self, dL_df, X):\n        pass\n\n    def gradients_X(self, dL_df, X):\n        return np.zeros(X.shape)\n\n# Instantiate the Brownian motion kernel with parameters\nbm_kernel = GBMKernel(input_dim=1, sigma=sigma)\n\n# Instantiate the mean function with S0 and mu\nmean_function = GBMMean(input_dim=1, output_dim=1, S0=np.log(S0), mu=mu)\n\n# Provide the initial observation at t=0\nt_data = np.array([[0.0]])\nY_data = np.array([[np.log(S0)]])\n\n# Create a GP model with the custom mean function and Brownian motion kernel\nmodel = GPy.models.GPRegression(t_data, Y_data, kernel=bm_kernel, mean_function=mean_function)\n\n# Set noise variance to a negligible value since we're simulating without observation noise\nmodel.likelihood.variance.fix(1e-10)\n\n# Sample paths from the GP model\nnum_samples = 5\nsamples = model.posterior_samples_f(t, size=num_samples)\n\n# Transform samples to original GBM scale\ngbm_samples = np.exp(samples)\n\n# Plot the sampled paths\nplt.figure(figsize=(10, 6))\nfor i in range(num_samples):\n    plt.plot(t, gbm_samples[:, :, i], label=f'Path {i+1}')\nplt.title(f'Sampled Paths from Geometric Brownian Motion')\nplt.xlabel('Time t')\nplt.ylabel('S(t)')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n","type":"content","url":"/notebooks/stochastic-calculus#simulation-using-gaussian-processes-3","position":29},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl2":"Orstein - Uhlenbeck process"},"type":"lvl2","url":"/notebooks/stochastic-calculus#orstein-uhlenbeck-process","position":30},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl2":"Orstein - Uhlenbeck process"},"content":"\n\n","type":"content","url":"/notebooks/stochastic-calculus#orstein-uhlenbeck-process","position":31},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Simulation using discrete paths","lvl2":"Orstein - Uhlenbeck process"},"type":"lvl3","url":"/notebooks/stochastic-calculus#simulation-using-discrete-paths-3","position":32},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Simulation using discrete paths","lvl2":"Orstein - Uhlenbeck process"},"content":"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters for the Ornstein-Uhlenbeck process\nS0 = 5.0    # Initial value\nmu = 1.0    # Long-term mean\ntheta = 10  # Rate of mean reversion\nsigma = 1 # Volatility\nT = 1.0    # Total time\nn = 100   # Number of steps\ndt = T / n  # Time step size\nt = np.linspace(0, T, n+1)  # Time points\n\n# Number of paths to simulate\nnum_paths = 5\n\n# Set up the plot\nplt.figure(figsize=(10, 6))\n\n# Simulate multiple paths\nfor _ in range(num_paths):\n    S = np.zeros(n+1)  # Initialize the process\n    S[0] = S0  # Set initial value\n\n    for i in range(1, n+1):\n        # Compute the mean and variance for the next step\n        mean = S0 * np.exp(-theta * t[i]) + mu * (1 - np.exp(-theta * t[i]))\n        variance = (sigma**2 / (2 * theta)) * (1 - np.exp(-2 * theta * t[i]))\n        \n        # Sample from the normal distribution with the computed mean and variance\n        S[i] = np.random.normal(mean, np.sqrt(variance))\n\n    # Plot the Ornstein-Uhlenbeck path\n    plt.plot(t, S, label=f'Path {_+1}')\n\n# Customize the plot\nplt.title('Sampled Paths from the Ornstein-Uhlenbeck Process')\nplt.xlabel('Time t')\nplt.ylabel('S(t)')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n","type":"content","url":"/notebooks/stochastic-calculus#simulation-using-discrete-paths-3","position":33},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Simulation using Gaussian Processes","lvl2":"Orstein - Uhlenbeck process"},"type":"lvl3","url":"/notebooks/stochastic-calculus#simulation-using-gaussian-processes-4","position":34},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Simulation using Gaussian Processes","lvl2":"Orstein - Uhlenbeck process"},"content":"\n\nimport GPy\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters for the Ornstein-Uhlenbeck process\nS0 = 5.0    # Initial value\nmu = 1.0    # Long-term mean\ntheta = 10  # Rate of mean reversion\nsigma = 1 # Volatility\nT = 1.0    # Total time\nn = 1000   # Number of steps\nt = np.linspace(0, T, n+1).reshape(-1, 1)  # Time points\n\n# Define the custom kernel for the Ornstein-Uhlenbeck process\nclass OrnsteinUhlenbeckKernel(GPy.kern.Kern):\n    def __init__(self, input_dim, theta=1.0, sigma=1.0, active_dims=None):\n        super(OrnsteinUhlenbeckKernel, self).__init__(input_dim, active_dims, 'OU')\n        self.theta = GPy.core.Param('theta', theta)\n        self.sigma = GPy.core.Param('sigma', sigma)\n        self.link_parameters(self.theta, self.sigma)\n    \n    def K(self, X, X2=None):\n        if X2 is None:\n            X2 = X\n        t1 = X\n        t2 = X2.T\n        min_t = np.minimum(t1, t2)\n        # Covariance function of the Ornstein-Uhlenbeck process\n        cov_matrix = (self.sigma**2 / (2 * self.theta)) * np.exp(-self.theta * np.abs(t1 - t2)) * (1 - np.exp(-2 * self.theta * min_t))\n        return cov_matrix\n    \n    def Kdiag(self, X):\n        return (self.sigma**2 / (2 * self.theta)) * (1 - np.exp(-2 * self.theta * X[:, 0]))\n    \n    # No-op for gradient update since we are not optimizing the kernel\n    def update_gradients_full(self, dL_dK, X, X2=None):\n        pass\n    \n    # No-op for kernel gradient since we are not optimizing the kernel\n    def gradients_X(self, dL_dK, X, X2=None):\n        return np.zeros(X.shape)\n\n# Custom mean function to include mean-reverting drift toward mu\nclass OUProcessMean(GPy.core.Mapping):\n    def __init__(self, input_dim, output_dim, S0=0.0, mu=0.0, theta=1.0):\n        super(OUProcessMean, self).__init__(input_dim, output_dim)\n        self.S0 = S0  # Initial value\n        self.mu = mu  # Long-term mean\n        self.theta = theta  # Rate of mean reversion\n        \n    def f(self, X):\n        # Mean function: S_0 * exp(-theta * t) + mu * (1 - exp(-theta * t))\n        return self.S0 * np.exp(-self.theta * X) + self.mu * (1 - np.exp(-self.theta * X))\n        \n    def update_gradients(self, dL_df, X):\n        pass\n    \n    def gradients_X(self, dL_df, X):\n        return np.zeros(X.shape)\n\n# Instantiate the OU kernel with parameters\nou_kernel = OrnsteinUhlenbeckKernel(input_dim=1, theta=theta, sigma=sigma)\n\n# Instantiate the mean function with non-zero initial value S0 and long-term mean mu\nmean_function = OUProcessMean(input_dim=1, output_dim=1, S0=S0, mu=mu, theta=theta)\n\n# Provide the initial observation at t=0\nt_data = np.array([[0.0]])\nY_data = np.array([[S0]])\n\n# Create a GP model with the custom mean function and OU kernel\nmodel = GPy.models.GPRegression(t_data, Y_data, kernel=ou_kernel, mean_function=mean_function)\n\n# Set noise variance to a negligible value since we're simulating without observation noise\nmodel.likelihood.variance.fix(1e-10)\n\n# Sample paths from the GP model\nnum_samples = 5\nsamples = model.posterior_samples_f(t, size=num_samples)\n\n# Plot the sampled paths\nplt.figure(figsize=(10, 6))\nfor i in range(num_samples):\n    plt.plot(t, samples[:, :, i], label=f'Path {i+1}')\nplt.title(f'Sampled Paths from Ornstein-Uhlenbeck Process (S0={S0}, mu={mu}, theta={theta})')\nplt.xlabel('Time t')\nplt.ylabel('S(t)')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n","type":"content","url":"/notebooks/stochastic-calculus#simulation-using-gaussian-processes-4","position":35},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl2":"Brownian bridge"},"type":"lvl2","url":"/notebooks/stochastic-calculus#brownian-bridge","position":36},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl2":"Brownian bridge"},"content":"","type":"content","url":"/notebooks/stochastic-calculus#brownian-bridge","position":37},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Simulation using discrete paths","lvl2":"Brownian bridge"},"type":"lvl3","url":"/notebooks/stochastic-calculus#simulation-using-discrete-paths-4","position":38},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Simulation using discrete paths","lvl2":"Brownian bridge"},"content":"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters for the Brownian bridge\nT = 1.0    # Total time\nn = 100   # Number of steps\ndt = T / n  # Time step size\nt = np.linspace(0, T, n+1)  # Time points\n\n# Number of paths to simulate\nnum_paths = 5\n\n# Set up the plot\nplt.figure(figsize=(10, 6))\n\n# Simulate multiple paths\nfor _ in range(num_paths):\n    W = np.zeros(n+1)  # Initialize the Wiener process\n    for i in range(1, n+1):\n        W[i] = W[i-1] + np.sqrt(dt) * np.random.randn()  # Standard Wiener process\n\n    # Adjust Wiener process to form a Brownian bridge\n    B = W - (t / T) * W[-1]  # Brownian bridge: W(t) - (t/T) * W(T)\n\n    # Plot the Brownian bridge path\n    plt.plot(t, B, label=f'Path {_+1}')\n\n# Customize the plot\nplt.title('Sampled Paths from a Brownian Bridge (Discrete Time Simulation)')\nplt.xlabel('Time t')\nplt.ylabel('B(t)')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n","type":"content","url":"/notebooks/stochastic-calculus#simulation-using-discrete-paths-4","position":39},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Simulation using Gaussian processes","lvl2":"Brownian bridge"},"type":"lvl3","url":"/notebooks/stochastic-calculus#simulation-using-gaussian-processes-5","position":40},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Simulation using Gaussian processes","lvl2":"Brownian bridge"},"content":"\n\nimport GPy\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Time points at which to sample the Brownian bridge\nT = 1.0\nt = np.linspace(0, T, 100).reshape(-1, 1)\n\n# Define a custom kernel for the Brownian bridge\nclass BrownianBridgeKernel(GPy.kern.Kern):\n    def __init__(self, input_dim, variance=1.0, active_dims=None):\n        super(BrownianBridgeKernel, self).__init__(input_dim, active_dims, 'brownian_bridge')\n        self.variance = GPy.core.Param('variance', variance)\n        self.link_parameter(self.variance)\n    \n    def K(self, X, X2=None):\n        if X2 is None:\n            X2 = X\n        t1 = X\n        t2 = X2.T\n        T = np.max(X)\n        # Brownian bridge kernel: min(t1, t2) - (t1 * t2) / T\n        cov_matrix = np.minimum(t1, t2) - (t1 * t2) / T\n        return self.variance * cov_matrix\n    \n    def Kdiag(self, X):\n        return self.variance * (X[:, 0] - (X[:, 0]**2) / T)\n    \n    # No-op for gradient update since we are not optimizing the kernel\n    def update_gradients_full(self, dL_dK, X, X2=None):\n        pass\n    \n    # No-op for kernel gradient since we are not optimizing the kernel\n    def gradients_X(self, dL_dK, X, X2=None):\n        return np.zeros(X.shape)\n\n# Instantiate the Brownian bridge kernel with variance 1\nbrownian_bridge_kernel = BrownianBridgeKernel(input_dim=1, variance=1.0)\n\n# Create a GP model with zero mean function\nmean_function = GPy.mappings.Constant(input_dim=1, output_dim=1, value=0)\nmodel = GPy.models.GPRegression(t, np.zeros_like(t), kernel=brownian_bridge_kernel, mean_function=mean_function)\n\n# Sample paths from the GP model without optimizing (since Brownian bridge doesn't require fitting)\nnum_samples = 5\nsamples = model.posterior_samples_f(t, size=num_samples)\n\n# Plot the sampled paths\nplt.figure(figsize=(10, 6))\nfor i in range(num_samples):\n    plt.plot(t, samples[:, :, i], label=f'Sample {i+1}')\nplt.title('Sampled Paths from a Brownian Bridge using Gaussian Processes')\nplt.xlabel('Time')\nplt.ylabel('B(t)')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n","type":"content","url":"/notebooks/stochastic-calculus#simulation-using-gaussian-processes-5","position":41},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl2":"Jump processes"},"type":"lvl2","url":"/notebooks/stochastic-calculus#jump-processes","position":42},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl2":"Jump processes"},"content":"","type":"content","url":"/notebooks/stochastic-calculus#jump-processes","position":43},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Homogeneous Poisson process simulation","lvl2":"Jump processes"},"type":"lvl3","url":"/notebooks/stochastic-calculus#homogeneous-poisson-process-simulation","position":44},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Homogeneous Poisson process simulation","lvl2":"Jump processes"},"content":"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\nlambda_rate = 0.2   # The rate parameter λ of the Poisson process\nT = 50             # Total time\ndelta_t = 0.01      # Time-step Δt\nnum_steps = int(T / delta_t)  # Number of time steps\n\n# Time grid\nt = np.linspace(0, T, num_steps + 1)\n\n# Initialize N_t\nN_t = np.zeros(num_steps + 1, dtype=int)\n\n# Simulate the Poisson process\nfor i in range(1, num_steps + 1):\n    # Generate a random number between 0 and 1\n    u = np.random.rand()\n    # If u < λΔt, increment N_t\n    if u < lambda_rate * delta_t:\n        N_t[i] = N_t[i - 1] + 1\n    else:\n        N_t[i] = N_t[i - 1]\n\n# Plot the Poisson process\nplt.step(t, N_t, where='post')\nplt.xlabel('Time t')\nplt.ylabel('N(t)')\nplt.title('Simulated Poisson Process')\nplt.grid(True)\nplt.show()\n\n\n\n\n","type":"content","url":"/notebooks/stochastic-calculus#homogeneous-poisson-process-simulation","position":45},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Hawkes process","lvl2":"Jump processes"},"type":"lvl3","url":"/notebooks/stochastic-calculus#hawkes-process","position":46},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Hawkes process","lvl2":"Jump processes"},"content":"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\nmu = 0.2       # Baseline intensity (μ)\nalpha = 0.5   # Excitation parameter (α)\nbeta = 1     # Decay rate (β)\nT = 50         # Total time to simulate\ndelta_t = 0.01 # Time-step Δt\n\n# Time grid\ntime_grid = np.arange(0, T + delta_t, delta_t)\nnum_steps = len(time_grid)\n\n# Initialize N_t and event times\nN_t = np.zeros(num_steps, dtype=int)\nevent_times = []\n\n# Simulate the Hawkes process\nfor i in range(1, num_steps):\n    t_i = time_grid[i]\n    \n    # Compute lambda_t\n    lambda_t = mu\n    if event_times:\n        tau_array = np.array(event_times)\n        lambda_t += np.sum(alpha * np.exp(-beta * (t_i - tau_array)))\n    \n    # Calculate the event probability\n    p = lambda_t * delta_t\n    if p > 1.0:\n        p = 1.0  # Ensure probability does not exceed 1\n    \n    # Simulate event occurrence\n    u = np.random.rand()\n    if u < p:\n        N_t[i] = N_t[i - 1] + 1\n        event_times.append(t_i)\n    else:\n        N_t[i] = N_t[i - 1]\n\n# Plot the counting process\n#plt.figure(figsize=(12, 6))\nplt.step(time_grid, N_t, where='post')\nplt.xlabel('Time t')\nplt.ylabel('N(t)')\nplt.title('Simulated Hawkes Process')\nplt.grid(True)\nplt.show()\n\n\n\n\n","type":"content","url":"/notebooks/stochastic-calculus#hawkes-process","position":47},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Compound Poisson process","lvl2":"Jump processes"},"type":"lvl3","url":"/notebooks/stochastic-calculus#compound-poisson-process","position":48},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Compound Poisson process","lvl2":"Jump processes"},"content":"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\nlambda_rate = 0.2   # Rate parameter λ of the Poisson process\nT = 50              # Total simulation time\ndelta_t = 0.01      # Time-step Δt\nnum_steps = int(T / delta_t)\ntime_grid = np.linspace(0, T, num_steps + 1)\n\n# Jump size distribution parameters\neta = 2.5  # Rate parameter of the exponential distribution for jump sizes\n\n# Initialize arrays\nN_t = np.zeros(num_steps + 1, dtype=int)  # Counting process N(t)\nX_t = np.zeros(num_steps + 1)             # Compound Poisson process X(t)\nevent_times = []                          # Times when events occur\njump_sizes = []                           # Sizes of the jumps\n\n# Simulate the compound Poisson process\nfor i in range(1, num_steps + 1):\n    t = time_grid[i]\n    # Probability of an event in [t_i, t_i + delta_t)\n    p = lambda_rate * delta_t\n    # Ensure probability does not exceed 1\n    p = min(p, 1.0)\n    # Determine if an event occurs\n    if np.random.rand() < p:\n        N_t[i] = N_t[i - 1] + 1\n        # Sample a jump size Y_i from an exponential distribution\n        Y_i = np.random.exponential(scale=1/eta)\n        jump_sizes.append(Y_i)\n        event_times.append(t)\n        X_t[i] = X_t[i - 1] + Y_i\n    else:\n        N_t[i] = N_t[i - 1]\n        X_t[i] = X_t[i - 1]\n\n# Plot all three visualizations in one figure\nfig, ax1 = plt.subplots(figsize=(12, 7))\n\n# Plot the compound Poisson process X(t)\nax1.step(time_grid, X_t, where='post', label='Compound Poisson Process $X(t)$', color='blue')\nax1.set_xlabel('Time $t$')\nax1.set_ylabel('$X(t)$', color='blue')\nax1.tick_params(axis='y', labelcolor='blue')\nax1.grid(True)\n\n# Plot jump sizes as scatter points on X(t)\nevent_indices = [np.searchsorted(time_grid, t) for t in event_times]\nevent_values = [X_t[idx] for idx in event_indices]\nax1.scatter(event_times, jump_sizes, color='green', marker='o', label='Jump Sizes')\n\n# Create a secondary y-axis for the counting process N(t)\nax2 = ax1.twinx()\nax2.step(time_grid, N_t, where='post', label='Counting Process $N(t)$', color='red', alpha=0.6)\nax2.set_ylabel('$N(t)$', color='red')\nax2.tick_params(axis='y', labelcolor='red')\n\n# Combine legends from both axes\nlines1, labels1 = ax1.get_legend_handles_labels()\nlines2, labels2 = ax2.get_legend_handles_labels()\nax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n\nplt.title('Compound Poisson Process with Counting Process and Jump Sizes')\nplt.show()\n\n\n\n\n","type":"content","url":"/notebooks/stochastic-calculus#compound-poisson-process","position":49},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Jump diffusion processes","lvl2":"Jump processes"},"type":"lvl3","url":"/notebooks/stochastic-calculus#jump-diffusion-processes","position":50},{"hierarchy":{"lvl1":"Stochastic Calculus","lvl3":"Jump diffusion processes","lvl2":"Jump processes"},"content":"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef simulate_merton_jump_diffusion(S0, mu, sigma, lamb, mu_j, sigma_j, T, N, M):\n    \"\"\"\n    Simulate the Merton jump diffusion model.\n\n    Parameters:\n    - S0: Initial asset price.\n    - mu: Expected return (drift rate).\n    - sigma: Volatility of the continuous component.\n    - lamb: Jump intensity (average number of jumps per unit time).\n    - mu_j: Mean of the jump size distribution (of ln(J)).\n    - sigma_j: Standard deviation of the jump size distribution (of ln(J)).\n    - T: Total time horizon.\n    - N: Number of time steps.\n    - M: Number of simulation paths.\n\n    Returns:\n    - t: Array of time points.\n    - S: Matrix of simulated asset prices (M paths x N+1 time points).\n    \"\"\"\n    dt = T / N  # Time step size\n    t = np.linspace(0, T, N + 1)  # Time grid\n\n    # Precompute constants\n    drift = (mu - 0.5 * sigma**2) * dt\n    diffusion = sigma * np.sqrt(dt)\n\n    # Initialize asset price paths\n    S = np.zeros((M, N + 1))\n    S[:, 0] = S0\n\n    for m in range(M):\n        # Initialize variables for each path\n        S_m = np.zeros(N + 1)\n        S_m[0] = S0\n\n        # Generate random numbers for Brownian motion\n        Z = np.random.normal(size=N)\n\n        # Generate random numbers for jumps\n        # Poisson random numbers for number of jumps in each time step\n        Nt = np.random.poisson(lamb * dt, size=N)\n        # Total number of jumps\n        num_jumps = np.sum(Nt)\n        # Generate jump sizes only where jumps occur\n        if num_jumps > 0:\n            jump_sizes = np.random.normal(loc=mu_j, scale=sigma_j, size=num_jumps)\n        else:\n            jump_sizes = np.array([])\n\n        # Index to keep track of jump sizes\n        jump_index = 0\n\n        for i in range(1, N + 1):\n            # Continuous part\n            S_m[i] = S_m[i - 1] * np.exp(drift + diffusion * Z[i - 1])\n\n            # Jump part\n            if Nt[i - 1] > 0:\n                # Apply jumps\n                total_jump = np.sum(jump_sizes[jump_index:jump_index + Nt[i - 1]])\n                S_m[i] *= np.exp(total_jump)\n                jump_index += Nt[i - 1]\n\n        S[m, :] = S_m\n\n    return t, S\n\n# Parameters\nS0 = 100        # Initial asset price\nmu = 0.1        # Expected return\nsigma = 0.2     # Volatility\nlamb = 1     # Jump intensity λ\nk = -0.1     # Mean of ln(J) (jump sizes)\ndelta = 0.1   # Std dev of ln(J)\nT = 1.0         # Total time (e.g., 1 year)\nN = 1000        # Number of time steps\nM = 5           # Number of simulation paths\n\n# Simulate the Merton jump diffusion model\nt, S = simulate_merton_jump_diffusion(S0, mu, sigma, lamb, k, delta, T, N, M)\n\n# Plot the simulated asset price paths\nplt.figure(figsize=(12, 6))\nfor m in range(M):\n    plt.plot(t, S[m, :], label=f'Path {m + 1}')\nplt.xlabel('Time t')\nplt.ylabel('Asset Price S(t)')\nplt.title('Merton Jump Diffusion Model Simulation')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n","type":"content","url":"/notebooks/stochastic-calculus#jump-diffusion-processes","position":51},{"hierarchy":{"lvl1":"Welcome"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Welcome"},"content":"Welcome to the online book Advanced Analytics and Algorithmic Trading. This book is currently a work in progress and I will be updating it with monthly releases. The aim is to cover the proposed table of contents. Check the \n\nRelease Notes for the latest updates.\n\n","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Welcome","lvl2":"Notebooks"},"type":"lvl2","url":"/#notebooks","position":2},{"hierarchy":{"lvl1":"Welcome","lvl2":"Notebooks"},"content":"All the code used for the examples in the book is available in the final section of the book.","type":"content","url":"/#notebooks","position":3},{"hierarchy":{"lvl1":"Welcome","lvl2":"Citation"},"type":"lvl2","url":"/#citation","position":4},{"hierarchy":{"lvl1":"Welcome","lvl2":"Citation"},"content":"If you use the content of this book in your own work, please cite us using\n\nSabio Gonzalez, Javier, “Advanced Analytics and Algorithmic Trading”, 2024. URL: \n\nwww​.datasciencealgotrading​.com\n\nHere is the citation in BibTeX format@book{JSG2024,\n  title = {{Advanced Analytics and Algorithmic Trading}},\n  author = {Sabio Gonzalez, Javier},\n  year = {2024},\n  url = \"www.datasciencealgotrading.com\"\n}","type":"content","url":"/#citation","position":5},{"hierarchy":{"lvl1":"Welcome","lvl2":"See a mistake or want to make a suggestion?"},"type":"lvl2","url":"/#see-a-mistake-or-want-to-make-a-suggestion","position":6},{"hierarchy":{"lvl1":"Welcome","lvl2":"See a mistake or want to make a suggestion?"},"content":"File an \n\nissue if you notice anything broken so we can fix it. I also welcome suggestions on which contents to prioritize.","type":"content","url":"/#see-a-mistake-or-want-to-make-a-suggestion","position":7},{"hierarchy":{"lvl1":"Welcome","lvl3":"License","lvl2":"See a mistake or want to make a suggestion?"},"type":"lvl3","url":"/#license","position":8},{"hierarchy":{"lvl1":"Welcome","lvl3":"License","lvl2":"See a mistake or want to make a suggestion?"},"content":"\n\nAdvanced Analytics and Algorithmic Trading © 2024 by Javier Sabio González is licensed under a \n\nCreative Commons Attribution-NonCommercial-ShareAlike 4.0 International License, except for the code (including the code blocks and jupyter notebooks) which are licensed under the \n\nGNU GENERAL PUBLIC LICENSE Version 3.","type":"content","url":"/#license","position":9}]}