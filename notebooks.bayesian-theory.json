{"version":3,"kind":"Notebook","sha256":"184a77bc2ae5e249d9dcedcdb05de9be805a3b04f104118869e3d688bd5526eb","slug":"notebooks.bayesian-theory","location":"/notebooks/bayesian_theory.ipynb","dependencies":[],"frontmatter":{"title":"Bayesian Modelling","content_includes_title":false,"kernelspec":{"name":"python3","display_name":"py313","language":"python"},"authors":[{"nameParsed":{"literal":"Javier Sabio González","given":"Javier Sabio","family":"González"},"name":"Javier Sabio González","id":"contributors-myst-generated-uid-0"}],"github":"https://github.com/xaviweise/aaat","numbering":{"title":{"offset":1}},"source_url":"https://github.com/xaviweise/aaat/blob/main/notebooks/bayesian_theory.ipynb","edit_url":"https://github.com/xaviweise/aaat/edit/main/notebooks/bayesian_theory.ipynb","exports":[{"format":"ipynb","filename":"bayesian_theory.ipynb","url":"/build/bayesian_theory-aba50fbbd293d650d82ada41e70ae752.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Bayesian probability","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"vBEleyToKb"}],"identifier":"bayesian-probability","label":"Bayesian probability","html_id":"bayesian-probability","implicit":true,"key":"edHvNxZ0Ly"},{"type":"heading","depth":3,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Example: estimating the probability of heads in a coin toss experiment","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"UXpTChbM0m"}],"identifier":"example-estimating-the-probability-of-heads-in-a-coin-toss-experiment","label":"Example: estimating the probability of heads in a coin toss experiment","html_id":"example-estimating-the-probability-of-heads-in-a-coin-toss-experiment","implicit":true,"key":"wPMSuFulcz"}],"key":"ci0MG0YH5w"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Generate a sequence of binary random variables\nfrom scipy.stats import bernoulli, beta, uniform\np = 0.3\nr = bernoulli.rvs(p, size=400)\nr[:10]","key":"RtjZQg5rUD"},{"type":"outputs","id":"ZP_9n4am3GJ5n5pcFNQKc","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":36,"metadata":{},"data":{"text/plain":{"content":"array([0, 0, 0, 1, 0, 0, 0, 1, 0, 0])","content_type":"text/plain"}}},"children":[],"key":"z026nXkSr9"}],"key":"MK4RzVHcU8"}],"key":"maaLqn9WP3"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# prior distribution: uniform prior\nprior_alpha = 1\nprior_beta = 1\n\n# Now we plot the distribution as we add more data points\nfig, ax = plt.subplots(nrows=3, ncols=3, figsize=(17,17))\nN = np.linspace(0, len(r), 9)\nfor i in range(9):\n  n = N[i]\n  i_x = int(i/3)\n  i_y = i % 3 \n  r_trunc = r[:int(n)]  \n  p_grid = np.linspace(0, 1, 1000)\n  post_alpha = prior_alpha + np.sum(r_trunc)\n  post_beta = prior_beta + len(r_trunc)- np.sum(r_trunc)\n  mean = beta.mean(post_alpha, post_beta)\n  std = beta.std(post_alpha, post_beta)\n  ax[i_x][i_y].plot(p_grid, beta.pdf(p_grid, post_alpha, post_beta))\n  ax[i_x][i_y].set_title(\"N = \" + str(int(n))+ \", est = \" + str(np.round(mean, 3)) + \" +/- \" + str(np.round(std, 3)))","key":"lXlQ6HAeO2"},{"type":"outputs","id":"rihTTN7WLsRylBfzW4Q3n","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"b6de67c44d1ae0aac542b5ee96ee8fa6","path":"/build/b6de67c44d1ae0aac542b5ee96ee8fa6.png"},"text/plain":{"content":"<Figure size 1700x1700 with 9 Axes>","content_type":"text/plain"}}},"children":[],"key":"fI3Pnfolen"}],"key":"ZoOtdyTKqr"}],"key":"ozRCov0OFb"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# prior distribution: non - informative prior (approximation)\nprior_alpha = 0.000001\nprior_beta = 0.000001\n\n# Now we plot the distribution as we add more data points\nfig, ax = plt.subplots(nrows=3, ncols=3, figsize=(17,17))\nN = np.linspace(0, len(r), 9)\nfor i in range(9):\n  n = N[i]\n  i_x = int(i/3)\n  i_y = i % 3 \n  r_trunc = r[:int(n)]  \n  post_alpha = prior_alpha + np.sum(r_trunc)\n  post_beta = prior_beta + len(r_trunc)- np.sum(r_trunc)\n  mean = beta.mean(post_alpha, post_beta)\n  std = beta.std(post_alpha, post_beta)\n  ax[i_x][i_y].plot(p_grid, beta.pdf(p_grid, post_alpha, post_beta))\n  ax[i_x][i_y].set_title(\"N = \" + str(int(n))+ \", est = \" + str(np.round(mean, 3)) + \" +/- \" + str(np.round(std, 3)))","key":"EAfERTPsqx"},{"type":"outputs","id":"1soeDFDCJvP8cF6hKU43L","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"975e5cb1c63b9e9b70ad9ec0f6ec2025","path":"/build/975e5cb1c63b9e9b70ad9ec0f6ec2025.png"},"text/plain":{"content":"<Figure size 1700x1700 with 9 Axes>","content_type":"text/plain"}}},"children":[],"key":"SUWLiabDOk"}],"key":"p38PERYcpR"}],"key":"qmJhq1uVTk"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# prior distribution: wrong confident prior\nprior_alpha = 30\nprior_beta = 30\n\n# Now we plot the distribution as we add more data points\nfig, ax = plt.subplots(nrows=3, ncols=3, figsize=(17,17))\nN = np.linspace(0, len(r), 9)\nfor i in range(9):\n  n = N[i]\n  i_x = int(i/3)\n  i_y = i % 3 \n  r_trunc = r[:int(n)]  \n  post_alpha = prior_alpha + np.sum(r_trunc)\n  post_beta = prior_beta + len(r_trunc)- np.sum(r_trunc)\n  mean = beta.mean(post_alpha, post_beta)\n  std = beta.std(post_alpha, post_beta)\n  ax[i_x][i_y].plot(p_grid, beta.pdf(p_grid, post_alpha, post_beta))\n  ax[i_x][i_y].set_title(\"N = \" + str(int(n))+ \", est = \" + str(np.round(mean, 3)) + \" +/- \" + str(np.round(std, 3)))","key":"puv49DkBAE"},{"type":"outputs","id":"RWyMKRbHhwTewiOH8j4ZY","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"8a8ef23c0c296f74df7824856b092bf6","path":"/build/8a8ef23c0c296f74df7824856b092bf6.png"},"text/plain":{"content":"<Figure size 1700x1700 with 9 Axes>","content_type":"text/plain"}}},"children":[],"key":"huH1FDXZw1"}],"key":"cJw9Hh8dVZ"}],"key":"CNakgk2j8F"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# prior distribution: correct confident prior\nprior_alpha = 18\nprior_beta = 42\n\n# Now we plot the distribution as we add more data points\nfig, ax = plt.subplots(nrows=3, ncols=3, figsize=(17,17))\nN = np.linspace(0, len(r), 9)\nfor i in range(9):\n  n = N[i]\n  i_x = int(i/3)\n  i_y = i % 3 \n  r_trunc = r[:int(n)]  \n  post_alpha = prior_alpha + np.sum(r_trunc)\n  post_beta = prior_beta + len(r_trunc)- np.sum(r_trunc)\n  mean = beta.mean(post_alpha, post_beta)\n  std = beta.std(post_alpha, post_beta)\n  ax[i_x][i_y].plot(p_grid, beta.pdf(p_grid, post_alpha, post_beta))\n  ax[i_x][i_y].set_title(\"N = \" + str(int(n))+ \", est = \" + str(np.round(mean, 3)) + \" +/- \" + str(np.round(std, 3)))","key":"OJRoWODirQ"},{"type":"outputs","id":"rK1NEhI8ZcV2yXBuVKgu0","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"2c278ee94492a7d3c3065bca45b7fe77","path":"/build/2c278ee94492a7d3c3065bca45b7fe77.png"},"text/plain":{"content":"<Figure size 1700x1700 with 9 Axes>","content_type":"text/plain"}}},"children":[],"key":"wHb0pNinO0"}],"key":"Jof9wy88vl"}],"key":"yUyETPIpyN"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Estimation of Latent Variable Models","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"C3mlXkqNkT"}],"identifier":"estimation-of-latent-variable-models","label":"Estimation of Latent Variable Models","html_id":"estimation-of-latent-variable-models","implicit":true,"key":"SjgJtjCpHc"},{"type":"heading","depth":4,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Expectation Maximization","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"x1LwW3i2mA"}],"identifier":"expectation-maximization","label":"Expectation Maximization","html_id":"expectation-maximization","implicit":true,"key":"ZMMNuJqkqs"},{"type":"heading","depth":5,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Example 1: Gaussian Mixture Models","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"G3woIlHTTx"}],"identifier":"example-1-gaussian-mixture-models","label":"Example 1: Gaussian Mixture Models","html_id":"example-1-gaussian-mixture-models","implicit":true,"key":"RVtrMPUYWX"}],"key":"CDkL27Zi4z"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from scipy.stats import norm\nimport numpy as np\n\nclass TheGoodAndBadDataModel():\n  def __init__(self, prior_p_good, prior_mean, prior_std_good, prior_std_bad):\n    self.p_good = prior_p_good\n    self.mean = prior_mean\n    self.std_good = prior_std_good\n    self.std_bad = prior_std_bad\n    self.loglik_history = []  # store likelihood values\n\n  def predict(self, X):\n    p_x_bad_pbad = (1 - self.p_good) * norm.pdf(X, loc=self.mean, scale=self.std_bad)\n    p_x_good_pgood = self.p_good * norm.pdf(X, loc=self.mean, scale=self.std_good)\n    p_bad_x = p_x_bad_pbad / (p_x_good_pgood + p_x_bad_pbad)\n    return p_bad_x\n\n  def compute_loglik(self, X):\n    mixture_pdf = (\n        self.p_good * norm.pdf(X, loc=self.mean, scale=self.std_good) +\n        (1 - self.p_good) * norm.pdf(X, loc=self.mean, scale=self.std_bad)\n    )\n    return np.sum(np.log(mixture_pdf + 1e-12))  # add epsilon to avoid log(0)\n\n  def learn(self, X, max_iter=1000, tolerance=1e-5, print_error=False, track_likelihood=True):\n    iter = 0\n    while True:\n      iter += 1\n      # E-step\n      p_bad_s = self.predict(X)\n      p_good_s = 1 - p_bad_s\n\n      # M-step\n      p_good_sp1 = np.mean(p_good_s)\n      std_good_sp1 = np.sqrt(np.sum(p_good_s * (X - self.mean)**2) / (len(X) * p_good_sp1))\n      std_bad_sp1 = np.sqrt(np.sum(p_bad_s * (X - self.mean)**2) / (len(X) * (1 - p_good_sp1)))\n\n      # compute change (for stopping)\n      error = np.sqrt(((p_good_sp1 - self.p_good)/self.p_good)**2\n                      + ((std_good_sp1 - self.std_good)/self.std_good)**2\n                      + ((std_bad_sp1 - self.std_bad)/self.std_bad)**2)\n\n      # update parameters\n      self.p_good = p_good_sp1\n      self.std_good = std_good_sp1\n      self.std_bad = std_bad_sp1\n\n      # track log-likelihood\n      if track_likelihood:\n        ll = self.compute_loglik(X)\n        self.loglik_history.append(ll)\n        if print_error:\n          print(f\"Iter {iter}: error={error:.6f}, loglik={ll:.6f}\")\n\n      # stopping condition\n      if (error < tolerance or iter >= max_iter):\n        break\n","key":"oyBq8OVYKn"},{"type":"outputs","id":"3nPxkWLCy1u1cqlY-rflN","children":[],"key":"mzO4iinns9"}],"key":"Ntyg18qXnM"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import yfinance as yf\nimport numpy as np\n\n# Define ticker\nbbva_tkr = yf.Ticker(\"BBVA.MC\")\n\n# Get 10 years of daily adjusted close prices\nend_date = \"2025-07-31\"\nstart_date = \"2015-07-31\"  # 10 years earlier\ndata = bbva_tkr.history(start=start_date, end=end_date, interval=\"1d\")[\"Close\"]\n\ndata = bbva_tkr.history(period=\"10y\", interval=\"1d\")[\"Close\"]\n\n# Calculate daily returns\nbbva_ret = data.pct_change().dropna()\n\n# Print mean and standard deviation of returns\nprint(\"Mean return:\", np.mean(bbva_ret))\nprint(\"Std deviation:\", np.std(bbva_ret))","key":"p1K4fojwUz"},{"type":"outputs","id":"RfhCoqXmwVT8xC4hW72cq","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"Mean return: 0.0006867420397232735\nStd deviation: 0.021392342218226022\n"},"children":[],"key":"Mzc0OXzh35"}],"key":"hEOpCFX042"}],"key":"i6HoRbaI1u"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# We learn the model over the historical data. As a prior we assume there are only 10% anomalies in the dataset\n# The result is relatively robust to the choice of prior\ngbdm = TheGoodAndBadDataModel(0.99, np.mean(bbva_ret), np.std(bbva_ret), 2*np.std(bbva_ret))\ngbdm.learn(bbva_ret)\n\n# We have a look at the results: according to the model, there are 15% anomalies, with roughly a 3x standard deviation\n# This means the model detects that the distribution of returns is not accurately described by a single Gaussian\nprint(gbdm.p_good, gbdm.mean, gbdm.std_good, gbdm.std_bad)","key":"nMRyj9XlWy"},{"type":"outputs","id":"_Wt9Xg9Qx61RXnbSt2ub7","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"0.8494092741093464 0.0006867420397232735 0.01482171389384633 0.04242390635242809\n"},"children":[],"key":"twxAZBIRPe"}],"key":"uTcJ1pMOe9"}],"key":"mBMID6dYr6"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Plot the log-likelihood history\nimport matplotlib.pyplot as plt\nplt.plot(gbdm.loglik_history, marker=\"o\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Log-likelihood\")\nplt.title(\"EM Log-likelihood evolution\")\nplt.show()","key":"wAzeW9ggmc"},{"type":"outputs","id":"dOjBl_LGDN_uF0s5m7xBa","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"a16efadafc57078326b5dd6821b133b4","path":"/build/a16efadafc57078326b5dd6821b133b4.png"},"text/plain":{"content":"<Figure size 640x480 with 1 Axes>","content_type":"text/plain"}}},"children":[],"key":"RjRc2ViNiJ"}],"key":"qvHiiEpNv2"}],"key":"MFPgxeuyiv"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import matplotlib.pyplot as plt\n\nbbva_ret.hist(bins=50, figsize=(8,5))\nplt.xlabel(\"Daily Return\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram of BBVA Daily Returns (10y)\")\nplt.show()","key":"hMaw8O1Zvs"},{"type":"outputs","id":"Xl2dmUNLve8YpZMyx_OOr","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"c4c5666a0376984a880f1e75afba41d2","path":"/build/c4c5666a0376984a880f1e75afba41d2.png"},"text/plain":{"content":"<Figure size 800x500 with 1 Axes>","content_type":"text/plain"}}},"children":[],"key":"b3aDFXoiS9"}],"key":"kM20XjCcn1"}],"key":"EhzKCQ4qFw"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Let us flag anomalies as those with 50% of more probability of belonging to the \"bad data\" mixture component\nanomalies = (gbdm.predict(bbva_ret) > 0.5)\n# Add anomaly flag to DataFrame\npd_bbva_ret = bbva_ret.reset_index()\npd_bbva_ret.columns = [\"Date\", \"Returns\"]\npd_bbva_ret[\"anomaly\"] = anomalies\n\n# Plot histogram of returns, segmented by anomaly flag\npd_bbva_ret[pd_bbva_ret[\"anomaly\"] == True][\"Returns\"].hist(\n    bins=100, alpha=0.7, color=\"red\", label=\"Anomalies\")\npd_bbva_ret[pd_bbva_ret[\"anomaly\"] == False][\"Returns\"].hist(\n    bins=50, alpha=0.7, color=\"blue\", label=\"Normal\")\n\nplt.xlabel(\"Daily Return\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram of BBVA Daily Returns\")\nplt.legend()\nplt.show()\n","key":"sH4UUQxBtt"},{"type":"outputs","id":"r7m8O5d-0TSygVwRBP5-t","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"50d3c7151a6f7e2951f424560ddb4a3e","path":"/build/50d3c7151a6f7e2951f424560ddb4a3e.png"},"text/plain":{"content":"<Figure size 640x480 with 1 Axes>","content_type":"text/plain"}}},"children":[],"key":"fymbFWJhJk"}],"key":"hEXResYKtb"}],"key":"RVCEl9BuLT"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# In terms of time-series, we flat the anomalies in the following plot\ncolors = {False: \"blue\", True: \"red\"}\npd_bbva_ret.reset_index().plot.scatter(x = \"Date\", y = \"Returns\", c = pd_bbva_ret[\"anomaly\"].map(colors).values, title = \"Time Series of BBVA Daily Returns\")","key":"A5yfrR8J7r"},{"type":"outputs","id":"B2OPHdP8mLPWNkcqBjxAw","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":78,"metadata":{},"data":{"text/plain":{"content":"<Axes: title={'center': 'Time Series of BBVA Daily Returns'}, xlabel='Date', ylabel='Returns'>","content_type":"text/plain"}}},"children":[],"key":"tavwgkNE9e"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"bf13cb2de8c634b82e3d812314155b56","path":"/build/bf13cb2de8c634b82e3d812314155b56.png"},"text/plain":{"content":"<Figure size 640x480 with 1 Axes>","content_type":"text/plain"}}},"children":[],"key":"jkZhoLyDIy"}],"key":"DeTlud6Mep"}],"key":"mKvGgzeb8p"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# An interesting question is whether these anomalies tend to cluster. Looking at the auto-correlation it hints this is a possibility\n# This means it could make sense to analyse these anomalies as regime changes, i.e. use a hidden markov model\nfrom pandas.plotting import autocorrelation_plot\nax = autocorrelation_plot(pd_bbva_ret[\"anomaly\"])\nax.set_xlim([0, 100])","key":"Rb6wixezcm"},{"type":"outputs","id":"20ejqyspoW_sFP2AVzhLX","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":79,"metadata":{},"data":{"text/plain":{"content":"(0.0, 100.0)","content_type":"text/plain"}}},"children":[],"key":"VR6M25Dhm7"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"98b3126f4f5e12dbb9510d30ba7085c8","path":"/build/98b3126f4f5e12dbb9510d30ba7085c8.png"},"text/plain":{"content":"<Figure size 640x480 with 1 Axes>","content_type":"text/plain"}}},"children":[],"key":"Qgy7RHAKxW"}],"key":"b5x3I0W4az"}],"key":"VI3RsiUmzX"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Check that relevant periods of financial stress have been correctly classified\npd_bbva_ret[\"Date\"] = pd.to_datetime(pd_bbva_ret[\"Date\"]).dt.date\n\n# Define event dates\nevent_dates = [\n    pd.to_datetime(\"2020-03-16\").date(),  # COVID lockdown Spain\n    pd.to_datetime(\"2016-06-24\").date(),  # Brexit referendum (first trading day)\n    pd.to_datetime(\"2016-11-09\").date()   # Trump election (first trading day after)\n]\n\n# Filter rows that match event dates\nevents_df = pd_bbva_ret[pd_bbva_ret[\"Date\"].isin(event_dates)][[\"Date\", \"Returns\", \"anomaly\"]]\n\nprint(events_df)\n","key":"AcY1YVQ2AR"},{"type":"outputs","id":"d297CMBdN6y-Ad2s2dMj9","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"            Date   Returns  anomaly\n214   2016-06-24 -0.161792     True\n312   2016-11-09 -0.057011     True\n1166  2020-03-16 -0.133684     True\n"},"children":[],"key":"QCG7EmHqpb"}],"key":"kPL87bGqhq"}],"key":"xZciqFNqjl"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":5,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Example 3: Local Level Model","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"D6i2pmyRdV"}],"identifier":"example-3-local-level-model","label":"Example 3: Local Level Model","html_id":"example-3-local-level-model","implicit":true,"key":"W53n8dLe9I"}],"key":"LKRb41Cm25"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n%matplotlib inline","key":"iLcKYM0N3D"},{"type":"outputs","id":"yOCyy6F57ixmnJZNB_LEn","children":[],"key":"GsA9wrZwLj"}],"key":"xJTkDM0U7b"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"\ndef simulate_local_level(T=200, sigma_v2=1.0, sigma_w2=0.05, seed=42):\n    rng = np.random.default_rng(seed)\n    y = np.zeros(T)\n    x = np.zeros(T)\n    y[0] = rng.normal(0.0, np.sqrt(sigma_w2))\n    x[0] = y[0] + rng.normal(0.0, np.sqrt(sigma_v2))\n    for t in range(1, T):\n        y[t] = y[t-1] + rng.normal(0.0, np.sqrt(sigma_w2))\n        x[t] = y[t] + rng.normal(0.0, np.sqrt(sigma_v2))\n    return x, y\n\n\n\ndef kf_rts_identity_cross(x, sigma_v2, sigma_w2, m0=0.0, P0=1e6):\n    T = len(x)\n    m_pred = np.zeros(T)\n    P_pred = np.zeros(T)\n    m_filt = np.zeros(T)\n    P_filt = np.zeros(T)\n    K = np.zeros(T)\n    innov = np.zeros(T)\n    S = np.zeros(T)\n\n    m_prev, P_prev = m0, P0\n    loglik = 0.0\n    for t in range(T):\n        # predict\n        m_pred[t] = m_prev\n        P_pred[t] = P_prev + sigma_w2\n        # update\n        innov[t] = x[t] - m_pred[t]\n        S[t] = P_pred[t] + sigma_v2\n        K[t] = P_pred[t] / S[t]\n        m_filt[t] = m_pred[t] + K[t] * innov[t]\n        P_filt[t] = (1 - K[t]) * P_pred[t]\n        # log-likelihood\n        loglik += -0.5 * (np.log(2*np.pi*S[t]) + innov[t]**2 / S[t])\n        m_prev, P_prev = m_filt[t], P_filt[t]\n\n    # RTS smoother\n    m_smooth = np.zeros(T)\n    P_smooth = np.zeros(T)\n    J = np.zeros(T-1)\n    m_smooth[-1] = m_filt[-1]\n    P_smooth[-1] = P_filt[-1]\n    for t in range(T-2, -1, -1):\n        J[t] = P_filt[t] / P_pred[t+1]\n        m_smooth[t] = m_filt[t] + J[t] * (m_smooth[t+1] - m_pred[t+1])\n        P_smooth[t] = P_filt[t] + J[t]**2 * (P_smooth[t+1] - P_pred[t+1])\n\n    # lag-one smoothed covariance via identity: P_{t-1,t|T} = J_{t-1} P_{t|T}\n    P_cross = np.zeros(T-1)\n    for t in range(1, T):\n        P_cross[t-1] = J[t-1] * P_smooth[t]\n\n    return {\n        \"m_pred\": m_pred, \"P_pred\": P_pred,\n        \"m_filt\": m_filt, \"P_filt\": P_filt,\n        \"m_smooth\": m_smooth, \"P_smooth\": P_smooth,\n        \"innov\": innov, \"S\": S, \"K\": K, \"J\": J,\n        \"P_cross\": P_cross, \"loglik\": loglik\n    }\n\n\n\ndef em_local_level(x, sigma_v2_init=2.0, sigma_w2_init=0.2, m0=0.0, P0=1e6, max_iter=500, tol=1e-8):\n    sigma_v2, sigma_w2 = float(sigma_v2_init), float(sigma_w2_init)\n    ll_hist = []\n    for it in range(max_iter):\n        out = kf_rts_identity_cross(x, sigma_v2, sigma_w2, m0, P0)\n        mu, P, Pc = out[\"m_smooth\"], out[\"P_smooth\"], out[\"P_cross\"]\n        ll_hist.append(out[\"loglik\"])\n\n        # E-step expectations\n        E1 = (x - mu)**2 + P\n        diff_mu = mu[1:] - mu[:-1]\n        E2 = diff_mu**2 + P[1:] + P[:-1] - 2.0 * Pc\n\n        # M-step (MLE scaling)\n        sigma_v2_new = max(np.mean(E1), 1e-12)\n        sigma_w2_new = max(np.mean(E2), 1e-12)\n\n        # convergence\n        rel = max(abs(sigma_v2_new - sigma_v2) / (sigma_v2 + 1e-12),\n                  abs(sigma_w2_new - sigma_w2) / (sigma_w2 + 1e-12))\n        sigma_v2, sigma_w2 = sigma_v2_new, sigma_w2_new\n        if rel < tol:\n            ll_hist.append(kf_rts_identity_cross(x, sigma_v2, sigma_w2, m0, P0)[\"loglik\"])\n            break\n    return {\"sigma_v2\": sigma_v2, \"sigma_w2\": sigma_w2, \"ll_history\": np.array(ll_hist)}\n\n","key":"OobLFfbDnR"},{"type":"outputs","id":"Gtil0dSjJzEy2qvDmCYrL","children":[],"key":"Lf6pTM5oPe"}],"key":"WoWx3sFRUp"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"\n# --- Configuration ---\nT = 200\nsigma_v2_true = 1.0\nsigma_w2_true = 0.05\nseed = 42\n\n# EM seeds\nsigma_v2_init = 2.0\nsigma_w2_init = 0.2\n\n# --- Simulate ---\nx, y = simulate_local_level(T, sigma_v2_true, sigma_w2_true, seed)\n\n# --- Run EM ---\nem = em_local_level(x, sigma_v2_init=sigma_v2_init, sigma_w2_init=sigma_w2_init, max_iter=1000, tol=1e-5)\nsigma_v2_hat, sigma_w2_hat = em[\"sigma_v2\"], em[\"sigma_w2\"]\nll = em[\"ll_history\"]\n\n# --- Smooth with estimated params ---\nout = kf_rts_identity_cross(x, sigma_v2_hat, sigma_w2_hat)\n\n# --- Compare parameters ---\ndf = pd.DataFrame({\n    \"Parameter\": [r\"$\\sigma_v^2$\", r\"$\\sigma_w^2$\"],\n    \"True\": [sigma_v2_true, sigma_w2_true],\n    \"EM estimate\": [sigma_v2_hat, sigma_w2_hat],\n})\ndf\n","key":"jvMBy9YxWc"},{"type":"outputs","id":"UKjTNDcWWxz0eCi5gDOhF","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":34,"metadata":{},"data":{"text/html":{"content":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Parameter</th>\n      <th>True</th>\n      <th>EM estimate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>$\\sigma_v^2$</td>\n      <td>1.00</td>\n      <td>0.871882</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>$\\sigma_w^2$</td>\n      <td>0.05</td>\n      <td>0.054226</td>\n    </tr>\n  </tbody>\n</table>\n</div>","content_type":"text/html"},"text/plain":{"content":"      Parameter  True  EM estimate\n0  $\\sigma_v^2$  1.00     0.871882\n1  $\\sigma_w^2$  0.05     0.054226","content_type":"text/plain"}}},"children":[],"key":"g5BzN1tcFU"}],"key":"XpAsVUu5rU"}],"key":"DTYLihb9bL"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Log-likelihood (should be non-decreasing up to numerical noise)\nplt.figure()\nplt.plot(ll, marker=\"o\")\nplt.title(\"EM log-likelihood over iterations\")\nplt.xlabel(\"iteration\"); plt.ylabel(\"log-likelihood\")\nplt.tight_layout()\nplt.show()\n\n# Data and smoothed state with ±1σ band\nt = np.arange(len(x))\nm = out[\"m_smooth\"]\nstd = np.sqrt(out[\"P_smooth\"])\n\nplt.figure()\nplt.plot(x, label=\"observations $x_t$\")\nplt.plot(y, label=\"true state $y_t$\")\nplt.plot(m, label=\"smoothed mean $\\hat{y}_{t|T}$\")\nplt.fill_between(t, m - 2*std, m + 2*std, alpha=0.3, facecolor=\"tab:green\", \n                  edgecolor=\"none\", zorder=0,label=\"smoother ±2σ\")\nplt.legend()\nplt.title(\"Local level model simulation and estimation\")\nplt.xlabel(\"t\"); plt.ylabel(\"value\")\nplt.tight_layout()\nplt.show()\n","key":"UOEFTGf2bT"},{"type":"outputs","id":"CseFg97IKepKaEVKHKczY","children":[{"type":"output","jupyter_data":{"name":"stderr","output_type":"stream","text":"<>:17: SyntaxWarning: invalid escape sequence '\\h'\n<>:17: SyntaxWarning: invalid escape sequence '\\h'\n/var/folders/d5/k0x6wwx97k7_73_1cz5q38t40000gn/T/ipykernel_39762/1804985963.py:17: SyntaxWarning: invalid escape sequence '\\h'\n  plt.plot(m, label=\"smoothed mean $\\hat{y}_{t|T}$\")\n"},"children":[],"key":"jOJ7HbutHm"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"87224c0b41c58561e4bb33ea24f0564e","path":"/build/87224c0b41c58561e4bb33ea24f0564e.png"},"text/plain":{"content":"<Figure size 640x480 with 1 Axes>","content_type":"text/plain"}}},"children":[],"key":"nKhOvIEbV8"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"88c6f46936dc89d56bc8d8a87233d57b","path":"/build/88c6f46936dc89d56bc8d8a87233d57b.png"},"text/plain":{"content":"<Figure size 640x480 with 1 Axes>","content_type":"text/plain"}}},"children":[],"key":"vvPEvBtcum"}],"key":"gY0YDjr0TM"}],"key":"YZtFkMIFhk"}],"key":"NWoHMpWmT9"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"Stochastic Calculus","url":"/notebooks/stochastic-calculus","group":"Notebooks"},"next":{"title":"Market Microstructure","url":"/notebooks/market-microstructure","group":"Notebooks"}}},"domain":"http://localhost:3002"}