

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>5. Bayesian Modelling &#8212; Advanced Analytics and Algorithmic Trading</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'markdown/intro_bayesian';</script>
    <link rel="canonical" href="https://www.datasciencealgotrading.com/markdown/intro_bayesian.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="6. Causal inference" href="intro_causal.html" />
    <link rel="prev" title="4. Algorithmic Trading" href="algorithmic_trading.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../welcome.html">
  
  
  
  
  
  
    <p class="title logo__title">Advanced Analytics and Algorithmic Trading</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../welcome.html">
                    Welcome
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="releases.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="dedication.html">Dedication</a></li>
<li class="toctree-l1"><a class="reference internal" href="preface.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="symbollist.html">Symbols</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Financial Markets Fundamentals</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="intro_financial_markets.html">1. Financial Markets</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro_financial_instruments.html">2. Mechanics of Financial Instruments</a></li>
<li class="toctree-l1"><a class="reference internal" href="market_microstructure.html">3. Market microstructure</a></li>
<li class="toctree-l1"><a class="reference internal" href="algorithmic_trading.html">4. Algorithmic Trading</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Financial Modelling Fundamentals</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">5. Bayesian Modelling</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro_causal.html">6. Causal inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="stochastic_calculus.html">7. Stochastic Calculus</a></li>
<li class="toctree-l1"><a class="reference internal" href="stochastic_optimal_control.html">8. Stochastic optimal control</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_driven_methods.html">9. Data-driven methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="generative_ai.html">10. Generative Artificial Intelligence</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Execution Algorithms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="execution_fundamentals.html">11. Execution fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="lob_models.html">12. Modelling the Limit Order Book</a></li>
<li class="toctree-l1"><a class="reference internal" href="almgren_chriss.html">13. The Almgren - Chriss Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="execution_tactics.html">14. Execution tactics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Market Making Algorithms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="market_making_fundamentals.html">15. Market Making fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="fair_price_estimation.html">16. Fair price estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="liquidity_modelling.html">17. Liquidity modelling</a></li>
<li class="toctree-l1"><a class="reference internal" href="rfq_models.html">18. Modelling RfQs in Dealer to Client Markets</a></li>
<li class="toctree-l1"><a class="reference internal" href="avellaneda_stoikov.html">19. The Avellaneda and Stoikov Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="enriching_avellaneda.html">20. Enriching the Avellaneda and Stoikov Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="hedging.html">21. Hedging strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="market_making_rl.html">22. Reinforcement Learning and Market Making</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Investment Algorithms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="quant_investment_fundamentals.html">23. Quantitative investment fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="mean_reversion_strategies.html">24. Mean reversion strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="trend_following.html">25. Trend following strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="arbitrage.html">26. Arbitrage Strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="factor_investing.html">27. Factor Investing</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimal_portfolios.html">28. Optimal portfolios</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Glossary and References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="references.html">References</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Notebooks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../notebooks/stochastic_calculus.html">Stochastic Calculus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/bayesian_theory.html">Bayesian Modelling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/market_microstructure.html">Market Microstructure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/rfq_models.html">Modelling RfQs in Dealer to Client Markets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/fair_price_estimation.html">Fair Price Estimation</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/xaviweise/aaat" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/xaviweise/aaat/issues/new?title=Issue%20on%20page%20%2Fmarkdown/intro_bayesian.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/markdown/intro_bayesian.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Bayesian Modelling</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-probability">5.1. Bayesian probability</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-as-an-extension-of-logic">5.1.1. Probability as an extension of logic</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#assigning-prior-probabilities">5.1.2. Assigning prior probabilities</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-inference-and-hypothesis-testing">5.1.3. Bayesian inference and hypothesis testing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-decision-theory">5.1.4. Bayesian decision theory</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-estimating-the-probability-of-heads-in-a-coin-toss-experiment">5.1.5. Example: estimating the probability of heads in a coin toss experiment</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-machine-learning">5.2. Bayesian Machine Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-learning">5.2.1. Bayesian learning</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-online-learning">5.2.1.1. Bayesian online learning</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-prediction">5.2.2. Bayesian prediction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-coin-toss-experiment">5.2.3. Example: coin toss experiment</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-linear-regression">5.3. Bayesian Linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probabilistic-graphical-models">5.4. Probabilistic Graphical Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#latent-variable-models">5.5. Latent variable models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#partially-observable-latent-variable-models">5.5.1. Partially observable latent variable models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#full-latent-variable-models">5.5.2. Full latent variable models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-latent-variable-models">5.5.3. Examples of Latent Variable Models</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-mixture-models-gmms">5.5.3.1. Gaussian Mixture Models (GMMs)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hidden-markov-model-hmm">5.5.3.2. Hidden Markov Model (HMM)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-kalman-filter">5.5.3.3. The Kalman Filter</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#the-local-level-model">5.5.3.3.1. The local level model</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation-of-latent-variable-models">5.5.4. Estimation of Latent Variable Models</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation-mle">5.5.4.1. Maximum Likelihood Estimation (MLE)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#expectation-maximization-em">5.5.4.2. Expectation Maximization (EM)</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#example-1-gaussian-mixture-model">5.5.4.2.1. Example 1: Gaussian Mixture Model</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#example-2-hidden-markov-model-the-baum-welch-algorithm">5.5.4.2.2. Example 2: Hidden Markov Model (the Baum - Welch algorithm)</a><ul class="nav section-nav flex-column">
<li class="toc-h6 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-observation-probabilities">5.5.4.2.2.1. Gaussian observation probabilities</a></li>
</ul>
</li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#example-3-local-level-model-simple-case-of-a-kalman-filter">5.5.4.2.3. Example 3: Local Level Model (simple case of a Kalman Filter)</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="bayesian-modelling">
<span id="intro-bayesian"></span><h1><span class="section-number">5. </span>Bayesian Modelling<a class="headerlink" href="#bayesian-modelling" title="Permalink to this heading">#</a></h1>
<section id="bayesian-probability">
<h2><span class="section-number">5.1. </span>Bayesian probability<a class="headerlink" href="#bayesian-probability" title="Permalink to this heading">#</a></h2>
<p>The greek philosopher Aristotle formally introduced the field of Logic as the tool of deduction and therefore one of the the main tools of scientific progress.  Logic allows to derive conclusions based on propositions that are taken for granted. Those propositions might have been themselves derived in a logical way, or might be principles that cannot be derived themselves, the starting building blocks of any theory or discipline.</p>
<p>A fundamental principle of logic is that of the “excluded third”, for which a given proposition can only be either true or false, but there is not a third option. Therefore, logic is based on the assumption of complete certainty about the propositions, both the premises and the conclusions. But most of our knowledge (if not all, as defended by empiricists like David Hume) is uncertain, therefore we need a tool that allows us to derive conclusions from uncertain premises. Under the so-called Bayesian school of probability, such a tool is probability theory, whose rules, that where inferred for the analysis of more specific toy cases like games of chance, we will see that turn out to apply to the wider case of reasoning under uncertainty.</p>
<p>In the Bayesian interpretation, a probability is a <em>statement or our degree of belief in a proposition</em>. For instance, the proposition “Apple stock will close tomorrow at a higher price than today” is, from today’s point of view, neither true or false. The application of logic is limited to situations where we have already observed the price of Apple tomorrow, and therefore we know if the proposition is true or false. However, we might be interested in making decisions today based on our confidence in the proposition without having to wait to know its trueness: we might trigger different trading strategies whose profits are linked to Apple stock depending on the degree of confidence on the proposition. Such way of reasoning happens in all spheres of our lives, where we are always looking ahead to un uncertain future: from minor decisions like which clothes to wear on a given day, to major ones like which professional career to follow.</p>
<p>What probability theory offers is a tool to be able to derive consistently degrees of belief on certain outcomes based on the degrees of belief on the premises. In our previous example, with probability theory we could potentially compute the probability of profit from a certain trading strategy based on the probability of Apple trading higher tomorrow. Our brains are capable of doing such calculations intuitively in familiar situations, but such intuition breaks down quickly in unfamiliar domains, when probabilities are very small, or when there is a large number of assumptions or outcomes. Probability as an extension of logic can compute these probabilities in any situation.</p>
<p>As we mentioned above, the actual rules to operate consistently with probabilities were inferred initially in the analysis of specific problems like games of chance or combinatorial problems, as done by mathematicians like Bernoulli and de Moivre in the 18th century. In the same century, reverend Thomas Bayes would write about application of probability theory to more general problems of reasoning under uncertainty, hence the name Bayesian school. It is important, however, to remark that Bayes was not the first author to attach such interpretation to probability, nor the one that would really formalize it as a proper theory of inference. That would be really Laplace, who successfully showed how to use the rules of probability to assign probabilities to propositions of interest to the scientific domain, the most famous probably being his work inferring the mass of Saturn from noisy observations.</p>
<p>That we can operate with the rules of probability theory to work on degrees of belief was not, however, properly analyzed until the work of the physicist Richard Cox in the 20th century. In his work from 1946, he starts from the basic idea of quantifying our degree of belief in a proposition by associating a real number to it, without necessarily linking it to probabilities. The only condition is that the larger the number, the larger must be our degree of belief. Then, he moves onto imposing consistency rules using logical reasoning:</p>
<ul class="simple">
<li><p>If we specify how much we believe a proposition X is true, we are implicitly specifying how much we believe it is false</p></li>
<li><p>If we specify how much we believe a proposition Y is true, and then how much another proposition X is true, given that Y is true, then we are implicitly specifying how much we believe both X and Y to be true</p></li>
</ul>
<p>Then, using Boolean logic and ordinary algebra, he found that consistency constraints this real numbers associated to our degree of belief to follow the rules of probability theory:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
P(X) + P(\bar{X}) = 1 \; \textrm{(sum rule)} \\  
P(X,Y) = P(X|Y) P(Y)  \; \textrm{(product rule)}
\end{aligned}\end{split}\]</div>
<p>where ̄X denotes the proposition that X is false. Therefore, if we are bound to use the rules of logical reasoning, we can see that our degrees of belief can be rigorously mapped to probability theory.</p>
<p>These rules are just the basic building blocks from probability theory. Many other results can be derived from them, for instance the infamous Bayes’ theorem, which is a simple consequence of the product rule. Since
we can write the product rule either way in terms of X or Y:</p>
<div class="math notranslate nohighlight">
\[P(X,Y) = P(X|Y) P(Y) = P(Y|X) P(X)\]</div>
<p>Then:</p>
<div class="math notranslate nohighlight">
\[P(X|Y) = \frac{P(Y|X) P(X)}{P(Y)}\]</div>
<p>which is Bayes’s theorem. We also could derive the marginalization rule:</p>
<p>by taking the continuum limit starting from a discrete and complete set of events. We leave it as an exercise for the interested reader.</p>
<p>The work of Cox was continued by E.T. Jaynes, who would extensively work on the interpretation of probability as an extension of deductive logic, and tackle many of the controversial views on this interpretation –like the role of prior probabilities that we will discuss later on. Another typical criticism is its supposedly lack of objectivity  as a universal theory of knowledge, since degrees of belief are attached to individuals and therefore they lack the objectivity needed for consensual agreement on the validity of a proposition. As Jaynes points out, such subjectivity disappears if we introduce explicitly the set of information available to the observers: two rational observers with the same information available must agree on the degrees of belief assigned to a proposition, otherwise either they are not rational or they don’t share the same information. We can formalize this by introducing the set of information available, <span class="math notranslate nohighlight">\(I\)</span> into the definition of the probabilities, namely <span class="math notranslate nohighlight">\(P(X|I)\)</span>.</p>
<section id="probability-as-an-extension-of-logic">
<h3><span class="section-number">5.1.1. </span>Probability as an extension of logic<a class="headerlink" href="#probability-as-an-extension-of-logic" title="Permalink to this heading">#</a></h3>
<p>So far we have seen how we can map the rules of consistent reasoning based on degrees of belief to probability theory. Let us now go back to the link to deductive logic. A basic syllogism in logic is</p>
<div class="math notranslate nohighlight">
\[A \implies B\]</div>
<p>So if <span class="math notranslate nohighlight">\(A\)</span> is true, then <span class="math notranslate nohighlight">\(B\)</span> is true. Let us discuss a few ways this syllogism is affected by uncertain.</p>
<p>A first situation happens when the proposition <span class="math notranslate nohighlight">\(A \implies B\)</span> itself is true, but we only have a degree of belief on <span class="math notranslate nohighlight">\(A\)</span>, expressed by probability <span class="math notranslate nohighlight">\(P(A)\)</span>. If we are interested in the degree of belief on <span class="math notranslate nohighlight">\(B\)</span>, We can use the marginalization rule to write <span class="math notranslate nohighlight">\(P(B) = P(B|A) P(A) + P(B|\bar{A})P(\bar{A})\)</span>. Since the proposition <span class="math notranslate nohighlight">\(A \implies B\)</span> is translated into <span class="math notranslate nohighlight">\(P(B|A) = 1\)</span>, hence <span class="math notranslate nohighlight">\(P(B) = P(A) + P(B|\bar{A})P(\bar{A}) = P(A)(1- P(B|\bar{A})) + P(B|\bar{A})\)</span>. As expected, if <span class="math notranslate nohighlight">\(P(A) = 1\)</span> then <span class="math notranslate nohighlight">\(P(B) = 1\)</span>, in agreement with deductive logic. However, when <span class="math notranslate nohighlight">\(P(A) &lt; 1\)</span> the degree of belief on <span class="math notranslate nohighlight">\(B\)</span> depends on the other potential mechanisms that can trigger <span class="math notranslate nohighlight">\(B\)</span> when <span class="math notranslate nohighlight">\(A\)</span> is false, quantified by <span class="math notranslate nohighlight">\(P(B|\bar{A})\)</span></p>
<p>A second situation is when proposition <span class="math notranslate nohighlight">\(A \implies B\)</span> is true, but we observe <span class="math notranslate nohighlight">\(B\)</span> and not <span class="math notranslate nohighlight">\(A\)</span>. In this case we can use Bayes theorem:</p>
<div class="math notranslate nohighlight">
\[P(A|B) = \frac{P(B|A) P(A)}{P(B|A) P(A) + P(B|\bar{A})P(\bar{A})}\]</div>
<p>Again, since <span class="math notranslate nohighlight">\(P(B|A) = 1\)</span>, this simplifies to:</p>
<div class="math notranslate nohighlight">
\[P(A|B) = \frac{P(A)}{P(A) + P(B|\bar{A})P(\bar{A})}\]</div>
<p>In this case, the probability of <span class="math notranslate nohighlight">\(A\)</span> contingent to the observation of <span class="math notranslate nohighlight">\(B\)</span> depends, as in the previous case, on the other ways that B can be triggered when <span class="math notranslate nohighlight">\(A\)</span> is false, but also on the so-called <em>prior</em> probability of A, <span class="math notranslate nohighlight">\(P(A)\)</span>, i.e. the degree of belief on A that we had before observing B. In the same language, <span class="math notranslate nohighlight">\(P(A|B)\)</span> is called a <em>posterior</em> probability. A key consequence of this result is that <span class="math notranslate nohighlight">\(P(A|B) &gt; P(A)\)</span>, since <span class="math notranslate nohighlight">\(P(B|\bar{A}) &lt; 1\)</span> (otherwise we could not have P(B|A) = 1), and therefore the denominator <span class="math notranslate nohighlight">\(P(A) + P(B|\bar{A})P(\bar{A}) &lt; P(A) + P(\bar{A}) = 1\)</span>. Despite the uncertainty, knowledge of <span class="math notranslate nohighlight">\(B\)</span> always increases our degree of belief on <span class="math notranslate nohighlight">\(A\)</span>, i.e. we can derive useful knowledge out of the domain of deductive logic.</p>
<p>Prior and posterior probabilities play a key role in Bayesian probability theory. Bayes theorem allows us to consistently update our <em>prior</em> degrees of belief (probabilities) based on new observations, in the form of <em>posterior</em> degrees of belief. As it happens with premises in deductive logic, prior probabilities can themselves be the result of previous inferences, although at some point they might need to be provided based on different considerations than previous observations, that might not be recorded systematically. Since they are inherent to Bayesian reasoning, critics of this theory have pointed them out as a weakness, an artificial requirement that biases the conclusions that other theories could just infer from the available observations.</p>
<p>In reality, those alternative theories are the ones that make hidden assumptions that effectively set the prior probabilities to naive choices in an uncontrolled way. Bayesian probability is only mathematically stating the obvious: our degree of belief on a proposition is not only driven by a given set of observations, there is always potentially prior information that can be used to improve our estimations. And in the rare case when it is not, there are also ways to assign consistently prior probabilities, as we will see in the next section.</p>
</section>
<section id="assigning-prior-probabilities">
<h3><span class="section-number">5.1.2. </span>Assigning prior probabilities<a class="headerlink" href="#assigning-prior-probabilities" title="Permalink to this heading">#</a></h3>
<p>Theoretically, every prior probability could be written as a posterior probability conditional to previously observed data. This chain of probabilities can be exhausted in two ways: 1) because we don’t have on record any relevant data from which to compute posterior probabilities, 2) because such data does not exist, i.e. the proposition under analysis is completely agnostic to any data previously observed. Of course the latter makes for a deep philosophical discussion, which could be easily carried out to the beginning of the observed universe. In all practical terms, is case 1) the one we are faced with.</p>
<p>When there is not relevant data recorded to compute posterior probabilities that serve as priors for later computations, we need to find a different way to assign prior probabilities. The general idea is to exploit known properties or constraints of the problem analyzed.</p>
<p>A popular idea is assigning priori probabilities based on symmetries of the problem. In particular, we can use De Moivre’s computation of probability as a ratio of possible outcomes where a certain proposition is true:</p>
<div class="math notranslate nohighlight">
\[P(\textrm{A}) = \frac{\#\textrm{outcomes where A happens}}{\#\textrm{total number of outcomes}}\]</div>
<p>This definition is suitable for situations like dice games where 1) the outcomes are countable, 2) there is a symmetry in each outcome, in the sense that they can be considered equiprobable.</p>
<p>A more general principle is that of maximum entropy, which generalizes the Moivre’s rule to more complicated propositions where our prior knowledge is based on limitations or constraints on the valid outcomes. Let us consider for simplicity a discrete set of outcomes <span class="math notranslate nohighlight">\(i = 1, ..., N\)</span> (which are themselves an exhaustive set of exclusive propositions), each with an a priori probability <span class="math notranslate nohighlight">\(p_i\)</span> that we want to determine. The entropy of this probability distribution is given, using Shanon’s definition, by:</p>
<div class="math notranslate nohighlight">
\[S = -\sum_i p_i \log p_i\]</div>
<p>The motivation for the entropy function is that it can be seen as arguably the most simple functional form that  consistently quantifies the uncertainty of a probability distribution, versus other potential assignments like <span class="math notranslate nohighlight">\(\sum_i p_i^2\)</span> for instance that can be seen to run into inconsistencies.</p>
<p>If we admit not having more information about a specific problem apart from the possible outcomes, we can assign as prior probabilities those that maximize entropy, conditional of course to be a complete set of probabilities, i.e. they sum up to 1. We include this restriction by using a Lagrange multiplier, so our objective function reads:</p>
<div class="math notranslate nohighlight">
\[L = -\sum_i p_i \log p_i + \lambda (\sum_i p_i -1)\]</div>
<p>We can know maximize this Lagrangian with respect to each <span class="math notranslate nohighlight">\(p_i\)</span>, although out of symmetry we can see that any permutation <span class="math notranslate nohighlight">\(i \rightarrow j\)</span> leaves the functional invariant, so the solution must be <span class="math notranslate nohighlight">\(p_1 = ... = p_N\)</span> and using the constraint this means <span class="math notranslate nohighlight">\(p_i = 1/N\)</span> which recovers De Moivre’s rule.</p>
<p>More interesting are cases where we have extra restrictions. For instance, let us assume that our prior knowledge includes a restriction on the average of a random variable that takes values <span class="math notranslate nohighlight">\(x_1, ..., x_N\)</span> under each outcome, so we know:</p>
<div class="math notranslate nohighlight">
\[\bar{X} = \sum_i p_i x_i\]</div>
<p>Our constraint maximization problem now reads:</p>
<div class="math notranslate nohighlight">
\[L = -\sum_i p_i \log p_i + \lambda_1 (\sum_i p_i -1) + \lambda_2 (\sum_i p_i x_i - \bar{X})\]</div>
<p>Notice that the symmetry argument does no longer hold, so we take derivatives to find the extreme:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial L}{\partial p_i} = -1 - \log p_i + \lambda_1 + \lambda_2 x_i = 0 \]</div>
<p>The solution is</p>
<div class="math notranslate nohighlight">
\[p_i = \frac{e^{\lambda_2 x_i}}{Z}\]</div>
<p>where <span class="math notranslate nohighlight">\(Z\)</span> is a normalization constant, usually called the <em>partition function</em> in Statistical Physics applications. This is an exponential or Gibbs distribution for the random variable <span class="math notranslate nohighlight">\(x\)</span>. The Lagrange multiplier <span class="math notranslate nohighlight">\(\lambda_2\)</span> is related to the average <span class="math notranslate nohighlight">\(\bar{X}\)</span> by means of the constraint:</p>
<div class="math notranslate nohighlight">
\[ \sum_i x_i p_i = \frac{1}{Z} \sum_i x_i e^{\lambda_2 x_i} = \frac{d \log Z}{d \lambda_2} = \bar{X}\]</div>
<p>This also points out to a general property of the partition function in which the moments of the distribution can be computed by taking derivatives of the partition function with respect of <span class="math notranslate nohighlight">\(\lambda_2\)</span>. It is usual to rewrite this term as <span class="math notranslate nohighlight">\(\lambda_2 = 1 / T\)</span>, with T called the temperature in analogy to Statistical Physics.</p>
<p>Let us consider a final case in which we add a second restriction on the second momentum of the distribution of the random variable <span class="math notranslate nohighlight">\(x\)</span>, namely <span class="math notranslate nohighlight">\(\sum_i p_i x_i^2 = \bar{X_i^2}\)</span>. The restricted optimization target now reads:</p>
<div class="math notranslate nohighlight">
\[L = -\sum_i p_i \log p_i + \lambda_1 (\sum_i p_i -1) + \lambda_2 (\sum_i p_i x_i - \bar{X}) + \lambda_3 (\sum_i p_i x_i^2 - \bar{X^2})\]</div>
<p>Computation of the extreme yields:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial L}{\partial p_i} = -1 - \log p_i + \lambda_1 + \lambda_2 x_i + \lambda_3 x_i^2 = 0 \]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial L}{\partial p_i} = -1 - \log p_i + \lambda_1 + \lambda_2 x_i + \lambda_3 (x_i + \frac{\lambda_2}{2\lambda_3})^2 = 0 \]</div>
<p>The solution can be written as:</p>
<div class="math notranslate nohighlight">
\[p_i \propto e^{\lambda_3(x_i + \frac{\lambda_2}{2\lambda_3})^2}\]</div>
<p>which properly normalized can be quickly recognized as a Gaussian distribution. As pointed out by Jaynes, that the Gaussian distribution is the maximum entropy distribution for a problem in which we know the mean and variance (via its second order momentum), can be seen as a motivation for its pervasiveness in all sorts of domains. It is a natural prior distribution for problems where we have knowledge of the <em>scale</em> of likely values of the phenomenon under observation.</p>
<p>To close this discussion, let us touch upon the question of <em>non-informative</em> priors, which seek to describe a situation in which there is not relevant knowledge a prior about the problem. It turns out this is not a simple topic, and such situation needs to be described with more precision. At the very least, we need to specify what we are ignorant about. Take the example of a parameter we are uncertain about, and we parametrize our belief about this parameter using a distribution with the form: <span class="math notranslate nohighlight">\(f(\nu, \sigma)\)</span>, where <span class="math notranslate nohighlight">\(\nu\)</span> is a location parameter and <span class="math notranslate nohighlight">\(\sigma\)</span> is a scale parameter. This setup encompasses multiple real cases where we express our knowledge in terms of a most likely range of values. For instance, if <span class="math notranslate nohighlight">\(f\)</span> corresponds to a normal distribution, the range given by <span class="math notranslate nohighlight">\(\nu \pm \sigma\)</span> has a probability of approximately 68% of containing the true value of the parameter, according to our beliefs. What does it mean to have a non-informative prior in this case? Since we are already introducing some prior information in the structure of the problem, we can not really say that we are fully ignorant, so we need to be more precise.</p>
<p>A useful way to express such ignorance with precision is by using transformation groups and invariants of the problem. In this example, we could argue that having knowledge about the scale <span class="math notranslate nohighlight">\(\sigma\)</span> means that our prior <span class="math notranslate nohighlight">\(f(\nu, \sigma)\)</span> changes under a transformation of scale: <span class="math notranslate nohighlight">\(\sigma \rightarrow a \sigma\)</span>. As commented by Jaynes (page 379), if a scale transformation makes the problem appear different to us, then we must have some prior information about the absolute scale. The same argument can be applied to the location <span class="math notranslate nohighlight">\(\nu\)</span>: if a change of location <span class="math notranslate nohighlight">\(\nu \rightarrow \nu + b\)</span> affects our view of the problem, then we must have some prior information about the location of the parameter. If we change variables in a distribution it must hold:</p>
<div class="math notranslate nohighlight">
\[g(\nu', \sigma') d\nu' d\sigma' = f(\nu, \sigma) d\nu d\sigma\]</div>
<p>In our case, since <span class="math notranslate nohighlight">\(\nu' = \nu + b\)</span> and <span class="math notranslate nohighlight">\(\sigma' = a \sigma\)</span>, computing the Jacobian:</p>
<div class="math notranslate nohighlight">
\[g(\nu', \sigma') = a^{-1} f(\nu, \sigma)\]</div>
<p>Now we can express the conditions for a non-informative prior by stating that such change of variables shall not make any difference to our prior distribution, since otherwise we would have indeed some knowledge about the scale or location of the problem. This means:</p>
<div class="math notranslate nohighlight">
\[g(\nu, \sigma) = f(\nu, \sigma)\]</div>
<p>Therefore:</p>
<div class="math notranslate nohighlight">
\[f(\nu + b, a\sigma) = a^{-1} f(\nu, \sigma)\]</div>
<p>This equation must hold for any <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>. Let us take <span class="math notranslate nohighlight">\(a = 1\)</span>, in this case <span class="math notranslate nohighlight">\(f(\nu + b, \sigma) = f(\nu, \sigma)\)</span> which means that the function cannot depend on <span class="math notranslate nohighlight">\(\nu\)</span>, hence <span class="math notranslate nohighlight">\(f(\nu, \sigma) = \phi(\sigma)\)</span>. The resulting equation for a general <span class="math notranslate nohighlight">\(a\)</span> is <span class="math notranslate nohighlight">\(\phi(a \sigma) = a^{-1} \phi(\sigma)\)</span> whose general solution is <span class="math notranslate nohighlight">\(\phi(\sigma) =  \text{const} \,\sigma\)</span>. Therefore, the non-informative prior for this problem is:</p>
<div class="math notranslate nohighlight">
\[f(\nu, \sigma) = \frac{\text{const}}{\sigma}\]</div>
<p>which corresponds to the so-called <em>Jeffrey’s non-informative prior</em> for this particular problem. As mentioned, this prior is relevant for instance in the particular but very common case when we model our beliefs using a Gaussian distribution. Ignorance about the location or scale of the distribution of this parameter can only be consistently encoded by using the Jeffrey’s prior.</p>
</section>
<section id="bayesian-inference-and-hypothesis-testing">
<h3><span class="section-number">5.1.3. </span>Bayesian inference and hypothesis testing<a class="headerlink" href="#bayesian-inference-and-hypothesis-testing" title="Permalink to this heading">#</a></h3>
<p>Bayes’ theorem can be used as the main tool to guide a theory of rational inference under uncertainty, providing us with a way to quantify the quality of a hypothesis <span class="math notranslate nohighlight">\(H\)</span> about the world based on the evidence (observations) available <span class="math notranslate nohighlight">\(E\)</span>::</p>
<p><span class="math notranslate nohighlight">\(P(H|E) = \frac{P(E|H)}{P(E)} P(H)\)</span></p>
<p>A high score is achieved in different ways:</p>
<ul class="simple">
<li><p>When the prior probability of the hypothesis is high, meaning that the hypothesis was belief to be true even before analyzing the evidence.</p></li>
<li><p>When the ratio <span class="math notranslate nohighlight">\(\frac{P(E|H)}{P(E)}\)</span> is high, meaning that the probability of the evidence under the hypothesis is higher than the probability in general of the evidence. This means that evidence that is generally thought to be unlikely (meaning that <span class="math notranslate nohighlight">\(P(E)\)</span> was small), when explained by the hypothesis (i.e. <span class="math notranslate nohighlight">\(P(E|H)\)</span> is now high), provides a stronger score to the hypothesis than those cases in which the evidence was generally probable in any case (<span class="math notranslate nohighlight">\(P(E)\)</span> is high). Of course the reverse is also true: when a theory fails to explain evidence that is generally considered likely, then the score for this hypothesis will be lower.</p></li>
</ul>
<p>One strength of Bayesian hypothesis testing that correctly captures many attributes attached to the way scientific research is normally conducted:</p>
<ul>
<li><p>Scientists seek to test hypothesis by deriving predictions that are relatively unlikely. For the most precise case of a deterministic prediction, this means that <span class="math notranslate nohighlight">\(P(E|H) = 1\)</span>, and <span class="math notranslate nohighlight">\(P(E)\)</span> is small. This way, the ratio <span class="math notranslate nohighlight">\(P(E|H) / P(E)\)</span> is large, providing a strong support to the hypothesis in case the prediction is verified empirically.</p></li>
<li><p>Scientists that disagree on the validity of a hypothesis will score different prior probabilities to <span class="math notranslate nohighlight">\(H\)</span>. However, if predictions that are strongly attributed to this hypothesis (and weekly to alternative ones) are confirmed, their posterior probabilities will tend to converge since the scientist that attached a lower prior probability for <span class="math notranslate nohighlight">\(H\)</span> will also attach naturally a lower probability to the prediction overall, <span class="math notranslate nohighlight">\(P(E)\)</span>, by virtue of Bayes’ theorem:</p>
<div class="math notranslate nohighlight">
\[P(E) = P(E|H)P(H) + P(E|\bar{H})P(\bar{H}) = P(H) + P(E|\bar{H})P(\bar{H})\]</div>
<p>This scientist therefore will need to update her posterior probability by a factor <span class="math notranslate nohighlight">\(1/P(E)\)</span> larger than the one that originally believed in the truth of the hypothesis, since, assuming that <span class="math notranslate nohighlight">\(P(E|\bar{H}) &lt; 1\)</span>, then:</p>
<div class="math notranslate nohighlight">
\[P(H) + P(E|\bar{H})P(\bar{H}) =  P(H)(1-P(E|\bar{H})) + P(E|\bar{H})\]</div>
<p>and hence <span class="math notranslate nohighlight">\(P(E)\)</span> is smaller for the scientist who attributes a lower prior probability to <span class="math notranslate nohighlight">\(P(H)\)</span>. This implies a larger update of the posterior relative to the prior for the skeptic scientist, which implies a convergence in posteriors with the other scientist.</p>
</li>
<li><p>Scientists score higher hypotheses whose predictions have been validated multiple times independently, but the marginal increase in the score tends to decrease with the number of repetitions. In terms of Bayes’ theorem, this is reflected in the fact that as a prediction is validated thanks to evidences <span class="math notranslate nohighlight">\(E_i\)</span> in agreement with the prediction, <span class="math notranslate nohighlight">\(P(E_1) &lt; P(E_2) &lt; P(E_3) &lt;...\)</span>, and given the upper bound <span class="math notranslate nohighlight">\(P(E) \leq 1\)</span>, the convergence rate necessarily decreases when enough evidence is accumulated. The mechanics is the same as in the previous argument regarding scientists with different priors for a given hypothesis.</p></li>
<li><p>Finally, scientists tend to prefer simple global explanations that cover multiple evidences to ad-hoc ones. The Bayesian framework incorporates this idea by attaching a small prior probability to ad-hoc mechanisms. This way, if we have a general hypothesis <span class="math notranslate nohighlight">\(H\)</span> and we attach to it an ad-hoc hypothesis <span class="math notranslate nohighlight">\(H_A\)</span> to explain some specific evidence, then, since <span class="math notranslate nohighlight">\(P(H \cap H_A) \leq \min(P(H), P(H_A))\)</span>, the combined hypothesis starts with a small prior probability due to the introduction of the ad-hoc hypothesis. A classical example of ad-hoc hypotheses is the use of epicycles by Ptolemaeous in the 2nd century AD to account for the observed planet orbits under the general hypothesis that the sun and the rest of the planets were orbiting around the Earth. A Bayesian approach would directly imply a low probability for this ad-hoc extension of the theory to account for the observations.</p></li>
</ul>
<p>A typical criticism of Bayesian hypothesis testing is that theoretically, to compute <span class="math notranslate nohighlight">\(P(E)\)</span>, we would need to consider all the possible alternative hypotheses, including many we might not be aware of. However, in many practical inference scenarios, the question of interest is to score relatively conflicting pairs of hypothesis, e.g. <span class="math notranslate nohighlight">\(H_1\)</span> and <span class="math notranslate nohighlight">\(H_2\)</span>. But a relative score can be computed by dividing their posterior probabilities under the same evidence:</p>
<div class="math notranslate nohighlight">
\[\frac{P(H_1|E)}{P(H_2|E)} = \frac{P(E|H_1)}{P(E|H_2)}\frac{P(H_1)}{P(H_2)}\]</div>
<p>When using this relative score, then <span class="math notranslate nohighlight">\(P(E)\)</span> is cancelled since it is common to both scores, removing the need to consider other hypotheses.</p>
</section>
<section id="bayesian-decision-theory">
<h3><span class="section-number">5.1.4. </span>Bayesian decision theory<a class="headerlink" href="#bayesian-decision-theory" title="Permalink to this heading">#</a></h3>
<p>Bayesian theory is a tool for reasoning, but reasoning itself is a tool to make decisions. When we make inferences about the world, we are mostly trying to use this information to guide our behavior, our choices. Examples abound, ranging from the quotidian:</p>
<ul class="simple">
<li><p>We would like to predict tomorrow’s weather, because we want to decide if we will be working from home or commuting to the office</p></li>
</ul>
<p>or the academic:</p>
<ul class="simple">
<li><p>A scientist wants to assess if a new hypothesis or theory has enough support to be accepted with respect to previous ones with respect to an observed phenomenon.</p></li>
</ul>
<p>In the context of the topics covered in this book, some examples could be the following:</p>
<ul class="simple">
<li><p>A trader (human or algorithmic) wants to estimate the value of an economic indicator and compare it with the market consensus, in order to make decisions about which positions to take</p></li>
<li><p>A dealer wants to predict demand for a financial instrument in order to set prices.</p></li>
</ul>
<p>Bayesian probability as discussed so far only evaluates probabilities taking into account our state of knowledge. To use these probabilities to make decisions we need to extend our framework. Following the work of Abraham Wald, a framework for decisions consists on the triplet <span class="math notranslate nohighlight">\((\theta_i, D_j, L_{ij})\)</span> where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\theta_i\)</span> enumerates the potential states of the world that are relevant to the outcome associated with a decision. In our previous examples, for instance, the different levels of demand for the financial instrument, or if actual inflation is above or below market consensus. Within the Bayesian probability framework, we also assign posterior probabilities <span class="math notranslate nohighlight">\(P(\theta_i |I)\)</span> to each state of the world, where <span class="math notranslate nohighlight">\(I\)</span> is the set of information available at the time of making the decision.</p></li>
<li><p><span class="math notranslate nohighlight">\(D_j\)</span> enumerates the set of possible decisions that can be made. In the first example, the positions taken by the trader to benefit from the inflation forecast. In our second example, the decision is the price that will be quoted for the instrument.Notice that nothing in the framework prevents that the probabilities <span class="math notranslate nohighlight">\(P(\theta_i |I)\)</span> depend on the decision taken, since the decision is taken as well using information contained in <span class="math notranslate nohighlight">\(I\)</span>. For example, when choosing a price for an instrument, the actual demand will likely depend on the price chosen, i.e. the state of the world is affected by our decision.</p></li>
<li><p><span class="math notranslate nohighlight">\(L_{ij}\)</span> is a function that assigns a numerical loss (or gain, if negative) to making decision <span class="math notranslate nohighlight">\(j\)</span> when the state of the world turns out to be <span class="math notranslate nohighlight">\(i\)</span>. In the example of the trader, we can quantify the profit or loss associated to the positions taken after the actual inflation indicator is released and the market moves accordingly. The dealer quoting a financial instrument might want to maximize profits, i.e. price times realized demand at this price, in a full round trip in which the instrument is bought and later sold.</p></li>
</ul>
<p>In our present discussion of decision theory, we will limit ourselves to setups where we can isolate the effect of a single decision in the loss function, i.e. future decisions are considered independently. If the loss function depends not only on current decisions but also future ones, the problem becomes more complex and is the subject of <em>Stochastic Optimal Control Theory</em>, that we will address in a separate chapter. Such a problem is also the domain of <em>Reinforcement Learning Theory</em>, which we will also address later, but the approach is different in the sense that this framework skips the explicit probabilistic modelling of the environment and directly tries to find which decisions minimize losses.</p>
<p>Once we have specified the elements necessary for decision making, we need a specific criterion to evaluate the quality of the decisions. A natural one is minimization of expected loss under the posterior probability distribution:</p>
<div class="math notranslate nohighlight">
\[D^* = \text{argmin}_{D_j} \sum_i P(\theta_i|D_j, I) L_{ij}\]</div>
<p>where we have explicitly introduced the decision <span class="math notranslate nohighlight">\(D_j\)</span> in the posterior probability to reflect the possibility that our decisions influence the state of the world that will be selected.</p>
<p>This decision rule is general enough to capture a variety of real situations. Let us consider for instance the case in which we know that the state of the environment is not selected at random but there is an adversary that will select the state that maximizes our loss given our choice. In this scenario <span class="math notranslate nohighlight">\(P(\theta_i|D_j, I) = 1_{i = \text{argmax}_i L_{ij}}\)</span>. Plugging this expression into the decision rule:</p>
<div class="math notranslate nohighlight">
\[D^* = \text{argmin}_{D_j} \text{max}_i L_{ij}\]</div>
<p>This is called the minimax rule, since the optimal strategy is to choose the decision that corresponds to the lowest maximum loss across the states of the world.</p>
<p>Another typical setup is related to parameter estimation. In Bayesian theory, when making a probabilistic model for a specific problem, we model the parameters of the distribution as random variables themselves, in order to quantify our degrees of belief in the different values that these parameters can have, both a priori before we see any relevant data and a after that. Bayes theorem is again the tool to consistently update our beliefs given the new empirical evidence. At some point, though, we might need to make decisions that involve a representative point estimation of the value of the parameters. For instance, in our pricing example, client demand is a probabilistic model that links demand with prices quoted and other features. When we want to decide on an optimal price to quote, we need to decide on a specific set of parameters that describe the demand. A natural choice could be to take the mean of the parameters, meaning that our optimal prices will maximize expected demand. But such estimation does not take into account potential trade-offs when using parameters that turn out to deviate from the ones actually driving the process. The business impact of quoting a too conservative price and therefore reducing the number of clients might be asymmetric to the case where the price is too aggressive. In this context, the optimal point estimation (or <em>estimator</em>, using the jargon of statistics) depends on the cost of choosing a value that differs from the actual one, which is of course unknown. The optimal estimation problem reads then:</p>
<div class="math notranslate nohighlight">
\[\hat{\theta} = \text{argmin}_{\hat{\theta}} \int d\theta P(\theta| I) L(\theta, \hat{\theta})\]</div>
<p>where we have used the continuous version of the expression which suits better the problem of parameter estimation. Notice that in this case the decision is about the value of the parameters that we will use as estimator, and the state of the world is the the actual value of the parameter. Depending on the loss function chosen, different estimators solve this equation. Let us discuss the most typical setups:</p>
<p><strong>Quadratic error</strong>: <span class="math notranslate nohighlight">\(L(\theta, \hat{\theta}) = (\theta - \hat{\theta})^2\)</span>. This error is symmetric and penalizes more larger errors with respect to the correct value of the parameter. The penalty increases marginally for larger errors as well. The estimation equation corresponds to the mean squared error (MSE)  functional:</p>
<div class="math notranslate nohighlight">
\[\hat{\theta} = \text{argmin}_{\hat{\theta}} \int d\theta P(\theta| I)  (\theta - \hat{\theta})^2\]</div>
<p>We take the derivative with respect to <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> to find the extreme of this equation:</p>
<div class="math notranslate nohighlight">
\[\int d\theta P(\theta| I)  (-2) (\theta - \hat{\theta}) = 0 \rightarrow \hat{\theta} = \int d\theta P(\theta| I) \theta = \bar{\theta}\]</div>
<p>which can be easily proved to be a minimum. Hence, the optimal estimator for this loss function is the mean of the posterior distribution. The mean is a natural candidate as an estimator in many real-life problems. This derivation shows under which conditions it makes sense for parameter estimation, namely when estimation errors are penalized quadratically.</p>
<p><strong>Absolute error</strong>: <span class="math notranslate nohighlight">\(L(\theta, \hat{\theta}) = |\theta - \hat{\theta}|\)</span>. This loss function also penalizes more larger errors and is symmetrical, but the marginal rate of growth of the penalty is constant, not linear as in the quadratic case. The estimation equation corresponds to the mean absolute error (MAE) functional:</p>
<div class="math notranslate nohighlight">
\[\hat{\theta} = \text{argmin}_{\hat{\theta}} \int d\theta P(\theta| I)  |\theta - \hat{\theta}|\]</div>
<p>We can rewrite this equation to simplify finding the extreme:</p>
<div class="math notranslate nohighlight">
\[\int d\theta P(\theta| I)  |\theta - \hat{\theta}| = \int_{\hat{\theta}}^{\infty} P(\theta| I) (\theta - \hat{\theta})- \int_{-\infty}^{\hat{\theta}} P(\theta| I) (\theta - \hat{\theta}) \]</div>
<p>Taking the derivative with respect to <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>, we get the condition:</p>
<div class="math notranslate nohighlight">
\[\int_{\hat{\theta}}^{\infty} P(\theta| I) = \int_{-\infty}^{\hat{\theta}} P(\theta| I) \]</div>
<p>which is the definition of the median, hence <span class="math notranslate nohighlight">\(\hat{\theta} = \text{median}(P(\theta|I))\)</span>. This sheds light on the potential use of this loss function: the median is usually considered a more robust estimator against the presence of outliers than the mean. An estimator based on the quadratic error gets more influenced by the behavior of the distribution in the tails than the absolute error.</p>
<p><strong>Delta error</strong>: <span class="math notranslate nohighlight">\(L(\theta, \hat{\theta}) = -\delta(\theta - \hat{\theta})\)</span>. When using this error, we only care about being exactly right. If we are wrong, we don’t care about the magnitude of the error. The optimal estimation equation reads then:</p>
<div class="math notranslate nohighlight">
\[\hat{\theta} = \text{argmin}_{\hat{\theta}} -\int d\theta P(\theta| I) \delta(\theta - \hat{\theta})= \text{argmax}_{\hat{\theta}} P( \hat{\theta}| I) \]</div>
<p>which we recognize directly as the mode of the posterior distribution: <span class="math notranslate nohighlight">\(\hat{\theta} = \text{mode}(P(\theta|I))\)</span>. Interestingly, if we apply Bayes’ theorem and we considered a prior <span class="math notranslate nohighlight">\(P(\theta)\)</span> that is constant around the high likelihood region, we have:</p>
<div class="math notranslate nohighlight">
\[\text{argmax}_{\hat{\theta}} P( \hat{\theta}| I) \simeq \text{argmax}_{\hat{\theta}} P( I | \hat{\theta})\]</div>
<p>which is the well-known Maximum Likelihood Estimator (MLE), used extensively in most real life statistical modelling and Machine Learning problems. Its wide recognition as a useful parameter estimation technique must be acknowledged, with the disclaimer of the many assumptions that it requires to be considered an optimal estimator. Bayesian theory and decision theory help us to understand the assumptions required to consider this family of estimator an optimal choice, in contrast with other statistical frameworks which only focus on desirable mathematical properties of the estimators derived.</p>
</section>
<section id="example-estimating-the-probability-of-heads-in-a-coin-toss-experiment">
<h3><span class="section-number">5.1.5. </span>Example: estimating the probability of heads in a coin toss experiment<a class="headerlink" href="#example-estimating-the-probability-of-heads-in-a-coin-toss-experiment" title="Permalink to this heading">#</a></h3>
<p>Let us discuss the classical probability example of a coin toss experiment. First we discuss the setup, which is typically omitted in most discussions. The setup of the problem will shape our prior knowledge. Let us assume some individual will toss a coin <span class="math notranslate nohighlight">\(N\)</span> times, and our goal is to predict the probability that the next toss will yield heads (“H”) and not tails (“T”). This is written as:</p>
<div class="math notranslate nohighlight">
\[P(H|D_N)\]</div>
<p>where <span class="math notranslate nohighlight">\(D_N\)</span> is a sequence of <span class="math notranslate nohighlight">\(N\)</span> observations, heads or tails. Part of our prior knowledge assumes that the individual tossing the coin is not particularly skilled, so the initial conditions of each experiment are relatively randomized. Our object of interest can be reduced to the distribution of mass in the coin, which makes it a fair or unfair coin. Since such distribution, in the end, influences the probability that a given toss yields heads or tails, which we denote <span class="math notranslate nohighlight">\(p_H\)</span>, we can make this quantity our object of inference. Notice that given a specific coin with initial random toss conditions, we can assume that <span class="math notranslate nohighlight">\(p_H\)</span> is completely determined by the physical properties of the coin, so it can be known if those properties are observed. Of course we don’t have such information so we can only provide estimations in the form of probabilities that <span class="math notranslate nohighlight">\(p_H\)</span> have different values. Such probabilities are Bayesian, in the sense that they reflect our relative degrees of belief on each potential value of <span class="math notranslate nohighlight">\(p_H\)</span>, in the form of a probability distribution <span class="math notranslate nohighlight">\(P(p_H)\)</span>.</p>
<p>Notice that this setup is not something that might be necessarily taken for granted. As discussed by Jaynes, one could potentially prepare a setup where a robot or a highly skilled person can control the initial conditions, to the point of being able to determine the outcome each time. If this is the case, our inference is not necessary uniquely about the mass distribution of the coin, but also about the skill or setup involved in the experiment.</p>
<p>Another source of potential prior knowledge concerns the possibility that the coin has identical faces, i.e. two tails or two heads. If the individual has shown us the coin beforehand, such knowledge belongs to the prior information, e.g. if we have seen that it has tails and heads then we already know that <span class="math notranslate nohighlight">\(P(p_H = 0) = 0\)</span> and <span class="math notranslate nohighlight">\(P(p_H = 1) = 0\)</span>. If we have seen only one face, e.g. tails, we can discard <span class="math notranslate nohighlight">\(P(p_H = 1)\)</span> but not <span class="math notranslate nohighlight">\(P(p_H = 0)\)</span>. It seems clear that in many practical situations prior knowledge about the problem consists on statements about the probability at the corners, i.e. <span class="math notranslate nohighlight">\(p_H=0\)</span> or <span class="math notranslate nohighlight">\(p_H=1\)</span>. We can use then the maximum entropy principle to propose a prior distribution that incorporates this knowledge.</p>
<p>Instead of directly working with restrictions on <span class="math notranslate nohighlight">\(p_H\)</span>, let us encode them in terms of the logarithm, <span class="math notranslate nohighlight">\(\log p_H\)</span>. The rational is that it makes more natural to discuss probabilities that are infinitesimally close to <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>. Arguably, working on the logarithm of probabilities is more natural to human brains, the same as happens with other types of intensities like noise, where the use of logarithms (i.e. decibels) is standard. Human brains struggle to see the difference between <span class="math notranslate nohighlight">\(p_H = 0.001\)</span> or <span class="math notranslate nohighlight">\(p_H = 0.0001\)</span>, but in many practical applications the distinction is relevant. Jaynes actually proposes to work on logarithms of probabilities in general.</p>
<p>For consistency with the previous discussion on the maximum entropy principle, let us work in a set of discrete values for the probabilities <span class="math notranslate nohighlight">\(p_{H,i}\)</span>, <span class="math notranslate nohighlight">\(i = 1, N\)</span>,  although we could generalize to a continuous functional. The Lagrangian for the problem reads:</p>
<div class="math notranslate nohighlight">
\[L = \sum_i P(p_{H,i}) \log P(p_{H,i}) + \lambda_1 (\sum_i P(p_{H,i})-1) + \lambda_2 (\sum_i P(p_{H,i})\log p_{H,i} - ELP_0) + \lambda_3 (\sum_i P(p_{H,i}) \log (1 - p_{H,i}) - ELP_1) \]</div>
<p>where we have set the restrictions over the expected value of the log-probabilities, i.e. <span class="math notranslate nohighlight">\(E[\log p_H] = \sum_i P(p_{H,i})\log p_{H,i}\)</span> and <span class="math notranslate nohighlight">\(E[\log p_T] = \sum_i P(p_{H,i})(1-\log p_{H,i})\)</span>.</p>
<p>The extreme of this function with respect to <span class="math notranslate nohighlight">\(P(p_{H,i})\)</span>, which is the unknown, is:</p>
<div class="math notranslate nohighlight">
\[1 + \log P(p_{H,i}) + \lambda_1 + \lambda_2 \log p_{H,i}
+ \lambda_3 \log (1-p_{H,i}) = 0\]</div>
<p>whose solution is:</p>
<div class="math notranslate nohighlight">
\[P(p_{H,i}) = e^{1 + \lambda_1}p_{H,i}^{\lambda_2}
(1-p_{H,i})^{\lambda_3}\]</div>
<p>If we apply the normalization constraint and redefine <span class="math notranslate nohighlight">\(\lambda_1\)</span> and <span class="math notranslate nohighlight">\(\lambda_2\)</span> we get as a result a Beta distribution:</p>
<div class="math notranslate nohighlight">
\[P(p_{H,i}) = \frac{p_{H,i}^{\alpha-1}
(1-p_{H,i})^{\beta-1}}{B(\alpha, \beta)}\]</div>
<p>where <span class="math notranslate nohighlight">\(B(\alpha, \beta)\)</span> is the beta function. We can now go back to the continuous limit so we have:</p>
<div class="math notranslate nohighlight">
\[P(dp_{H}) = \frac{p_{H}^{\alpha-1}
(1-p_{H})^{\beta-1}}{B(\alpha, \beta)} dp_H = f(p_H) d p_H\]</div>
<p>where <span class="math notranslate nohighlight">\(f(p_H)\)</span> is the probability density function (pdf) of <span class="math notranslate nohighlight">\(p_H\)</span>. That the beta distribution is the maximum entropy prior is particularly convenient for the inference of <span class="math notranslate nohighlight">\(p_H\)</span>, since it has the interesting property of being conjugate to the likelihood of observations following a Bernoulli distribution, which is the natural model for observations of identically and independently distributed (i.i.d.) random variables, each representing a coin toss. If we have a sequence of <span class="math notranslate nohighlight">\(N\)</span> observations of coin tosses, which we denoted as <span class="math notranslate nohighlight">\(D_N\)</span> above, our best inference for <span class="math notranslate nohighlight">\(p_H\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[f(p_H|D_N) = \frac{P(D_N|p_H) f(p_H)}{P(D_N)}\]</div>
<p>where we have applied Bayes’ theorem. The likelihood function is <span class="math notranslate nohighlight">\(P(D_N|p_H)\)</span> which for a Bernoulli random variable (and a given specific sequence of coin tosses) reads:</p>
<div class="math notranslate nohighlight">
\[P(D_N|p_H) = p_H^{n_H} (1-p_H)^{N-n_H}\]</div>
<p>where <span class="math notranslate nohighlight">\(n_H\)</span> is the number of times we have observed heads <span class="math notranslate nohighlight">\(H\)</span> in the sequence. When we say that the Beta distribution is conjugated to the Bernoulli likelihood we mean that the posterior distribution is again a Beta distribution with updated parameters. The denominator in Bayes theorem can be computed using:</p>
<div class="math notranslate nohighlight">
\[p(D_N) = \int P(D_N|p_H) f(p_H) dp_H = \frac{1}{B(\alpha, \beta)}\int p_H^{n_H + \alpha - 1} (1-p_H)^{N - n_H + \beta - 1} dp_H = \frac{B(n_H + \alpha, N - n_H + \beta)}{B(\alpha, \beta)} \]</div>
<p>Therefore the posterior is:</p>
<div class="math notranslate nohighlight">
\[f(p_{H}) = \frac{p_{H}^{n_H + \alpha-1}
(1-p_{H})^{N-n_H + \beta-1}}{B(n_H + \alpha, N - n_H + \beta)}\]</div>
<p>which is Beta distribution with updated parameters. This result not only simplifies inference by a simple rule of updating the parameters of the distribution, it also provides an intuitive interpretation for the Beta prior parameters: we can interpret <span class="math notranslate nohighlight">\(\alpha\)</span> as a prior number of heads observations, and correspondingly <span class="math notranslate nohighlight">\(\beta\)</span> is a prior number of tails. This does not mean that we have really observed those, since otherwise it would not be part of the prior, but it is a convenient parametrization of the prior belief. For example, <span class="math notranslate nohighlight">\(\alpha = \beta = 1\)</span>, which in the Beta distribution corresponds to an uniform prior, would be equivalent to someone who has observed one instance of heads and one of tails. This is not a minor piece of information, since it means that both values are plausible, discarding for example the possibility that we have a two heads or two tails coin.</p>
<p>Therefore, the uniform prior, an intuitive candidate for a non-informative prior in this setup, is actually not such: it can actually be interpreted as a prior where we assume that both heads and tails are plausible. A full non-informative prior is, in this case, best parametrized by <span class="math notranslate nohighlight">\(\alpha = \beta = 0\)</span>, i.e. no observations at all, which yields a distribution of the form:</p>
<div class="math notranslate nohighlight">
\[f(p_H) \propto \frac{1}{p_H (1- p_H)}\]</div>
<p>This distribution is unfortunately ill-defined, since it cannot be normalized. It corresponds to <em>Jeffrey’s non-informative prior</em> for this setup: as shown by Jaynes (pages 382-385), it can be derived using an invariance argument describing a state of knowledge in which any evidence does not provide relevant information about <span class="math notranslate nohighlight">\(p_H\)</span>. We leave to the interested reader to check the argument in his book.</p>
<p>Let us now see the effect of the prior on the inference provided by Bayes’ theorem on the parameter <span class="math notranslate nohighlight">\(p_H\)</span>. As we have seen, when we use the Beta distribution to model the prior, the posterior distribution is also a Beta distribution with updated parameters. This simplifies considerably the calculations of the posterior probability. We could still use other priors that might suit better our modelling needs, at the price of numerical estimation of the posterior probability. We stick, though, with the Beta distribution prior. In order to have a simple description of the posterior, we use decision theory to choose the best point estimator that minimizes the square error, which as we discussed before corresponds to the mean of the posterior distribution. We could use other estimators if they suit better our particular use case. In the case of the mean, if we use the Beta prior it is given by:</p>
<div class="math notranslate nohighlight">
\[\bar{p}_H = \frac{n_H + \alpha}{N + \alpha + \beta}\]</div>
<p>which shows explicitly the influence of the prior on the estimation, via the parameters <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span>. Interestingly, the choice of <span class="math notranslate nohighlight">\(\alpha = \beta = 1\)</span> does yield:</p>
<div class="math notranslate nohighlight">
\[\bar{p}_H = \frac{n_H + 1}{N + 2}\]</div>
<p>which reinforces our interpretation of the Beta prior parameters as a number of effective prior observations of heads and tails, with the uniform prior corresponding to a single heads and tails. The non-informative prior, though, where <span class="math notranslate nohighlight">\(\alpha = \beta = 0\)</span>, corresponds to:</p>
<div class="math notranslate nohighlight">
\[\bar{p}_H = \frac{n_H}{N}\]</div>
<p>which corresponds to the widespread rule according to which probabilities shall be estimated as frequencies. In this light, such interpretation only makes sense when using a non-informative prior and the squared error as our loss function to build point estimators.</p>
<p>We generate numerically random numbers following a Bernoulli distribution with probability <span class="math notranslate nohighlight">\(p_H = 0.3\)</span>, and we see the effect on the posterior distribution of having an increasingly large number of samples available, for different choices of priors.</p>
<p>For the first simulation, we start with the uniform prior, which as we have discussed is the one that is mistakenly considered as <em>non-informative</em>. This prior corresponds to <span class="math notranslate nohighlight">\(\alpha = \beta = 1\)</span> in the Beta distribution, which plays the role of having effectively two prior <em>observations</em>, one where heads was observed, the other tails.  In the following figure, we can see that the posterior moves relatively quickly (with 50 observations) around the correct range of values and to have a relatively confident and correct estimation around 300 observations. We use the mean and the standard deviation to characterize the posterior probability distribution.</p>
<figure class="align-default" id="fig-bernoulliuniformprior">
<a class="reference internal image-reference" href="../_images/BernoulliUniformPrior.png"><img alt="../_images/BernoulliUniformPrior.png" src="../_images/BernoulliUniformPrior.png" style="width: 8in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.1 </span><span class="caption-text">Posterior probability for the probability of heads on a random simulation of a coin with underlying probability <span class="math notranslate nohighlight">\(p_H = 0.3\)</span>. Every subplot corresponds to 50 new observations. The first one corresponds to the prior, in this case a uniform one: <span class="math notranslate nohighlight">\(\alpha = \beta = 1\)</span> in the Beta distribution prior. We characterize the posterior probability using the mean and the standard deviation of the posterior probability.</span><a class="headerlink" href="#fig-bernoulliuniformprior" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>A similar result is achieved using Jaynes’ <em>non-informative</em> prior, as shown in the next figure. This is achieved by taking the limit <span class="math notranslate nohighlight">\(\alpha, \beta \rightarrow 0\)</span>, i.e. zero effective prior <em>observations</em>. Despite the strong bias towards the extreme values of the distribution, this does not prevent that the posterior incorporates quickly the information from the observations, at the same rate as the uniform prior in this numerical simulation.</p>
<figure class="align-default" id="fig-bernoullijaynesprior">
<a class="reference internal image-reference" href="../_images/BernoulliJaynesPrior.png"><img alt="../_images/BernoulliJaynesPrior.png" src="../_images/BernoulliJaynesPrior.png" style="width: 8in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.2 </span><span class="caption-text">Posterior probability for the probability of heads on a random simulation of a coin with underlying probability <span class="math notranslate nohighlight">\(p_H = 0.3\)</span>. Every subplot corresponds to 50 new observations. The first one corresponds to the prior, for which we choose a non-informative one for this simulation, which we proxy using <span class="math notranslate nohighlight">\(\alpha = \beta = 10^{-6}\)</span>. We characterize the posterior probability using the mean and the standard deviation of the posterior probability. Despite the strong biased towards the extremes, this choice does not affect the speed of convergence towards the true value.</span><a class="headerlink" href="#fig-bernoullijaynesprior" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Finally, we use a prior that contains meaningful prior information, albeit erroneous one, since the prior distribution assumes that <span class="math notranslate nohighlight">\(p_H\)</span> has a mean value of <span class="math notranslate nohighlight">\(0.5\)</span> with a standard deviation of <span class="math notranslate nohighlight">\(0.064\)</span>. This corresponds to a Beta distribution with <span class="math notranslate nohighlight">\(\alpha = \beta = 30\)</span>, which we interpret as 60 effective prior observations. In this case, a larger number of real observations is required to override the effect of the prior. For instance, with <span class="math notranslate nohighlight">\(50\)</span> observations the estimation is still half way between the prior one and the real one in this simulation. For <span class="math notranslate nohighlight">\(400\)</span> observations, the estimation is still influenced by the prior. It takes around <span class="math notranslate nohighlight">\(1000\)</span> observations to converge to the real value.</p>
<figure class="align-default" id="fig-bernoullibiasedprior">
<a class="reference internal image-reference" href="../_images/BernoulliBiasedPrior.png"><img alt="../_images/BernoulliBiasedPrior.png" src="../_images/BernoulliBiasedPrior.png" style="width: 8in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.3 </span><span class="caption-text">Posterior probability for the probability of heads on a random simulation of a coin with underlying probability <span class="math notranslate nohighlight">\(p_H = 0.3\)</span>. Every subplot corresponds to 50 new observations. The first one corresponds to the prior. In this case, we choose a prior relatively confident of a value of <span class="math notranslate nohighlight">\(0.5\)</span> for <span class="math notranslate nohighlight">\(p_H\)</span>, corresponding to a choice <span class="math notranslate nohighlight">\(\alpha = \beta = 30\)</span>. We characterize the posterior probability using the mean and the standard deviation of the posterior probability. The plots show a slow convergence to the true value due to the effect of the prior information into the estimation.</span><a class="headerlink" href="#fig-bernoullibiasedprior" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Such result could suggest that is always better to choose non-informative priors given the penalty of using wrong prior information. However, as shown in the following figure, having good prior estimation can speed up the process of learning with respect to the case of using non-informative priors. We choose in this case a prior with <span class="math notranslate nohighlight">\(\alpha = 18, \beta = 42\)</span>, which corresponds to a prior mean of <span class="math notranslate nohighlight">\(0.3\)</span> and standard deviation <span class="math notranslate nohighlight">\(0.059\)</span>. This choice corresponds to the same number of effective prior <em>observations</em> as the previous case, to provide a meaningful comparison.</p>
<figure class="align-default" id="fig-bernoulliunbiasedprior">
<a class="reference internal image-reference" href="../_images/BernoulliUnbiasedPrior.png"><img alt="../_images/BernoulliUnbiasedPrior.png" src="../_images/BernoulliUnbiasedPrior.png" style="width: 8in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.4 </span><span class="caption-text">Posterior probability for the probability of heads on a random simulation of a coin with underlying probability <span class="math notranslate nohighlight">\(p_H = 0.3\)</span>. Every subplot corresponds to 50 new observations. The first one corresponds to the prior, which in this case has a correct prior belief on the value of <span class="math notranslate nohighlight">\(p_H\)</span>. This corresponds in this case to a choice <span class="math notranslate nohighlight">\(\alpha = 18, \beta = 42\)</span>. We characterize the posterior probability using the mean and the standard deviation of the posterior probability. The plots show a quick convergence to the true value with high confidence.</span><a class="headerlink" href="#fig-bernoulliunbiasedprior" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="bayesian-machine-learning">
<h2><span class="section-number">5.2. </span>Bayesian Machine Learning<a class="headerlink" href="#bayesian-machine-learning" title="Permalink to this heading">#</a></h2>
<p>One of the main advantages of Bayesian theory is its simplicity and coherence. The toolkit presented in the previous section can be used in principle to evaluate the probability of any hypotheses conditional to the available information. In modern Machine Learning (ML), specific emphasis is placed on two families of hypotheses:</p>
<ul class="simple">
<li><p>Hypotheses on the value of parameters of a given model. This is the <em>learning</em> task in the ML jargon. From a Bayesian point of view, as we saw in the previous section, they are simply stated as posterior distributions of parameters given the available information, in particular observations in a <em>training set</em>: <span class="math notranslate nohighlight">\(P(\theta|D)\)</span>. Recall that the outcome are probability distributions. If we want to reduce parameter estimation to point estimators, Bayesian theory needs to be complemented with Decision theory.</p></li>
<li><p>Hypotheses on the future value of observations. This is the <em>prediction</em> task in the ML jargon. In Bayesian theory, this can be again written in terms of a posterior probability, in this case on the values of unobserved data <span class="math notranslate nohighlight">\(\bf{x}\)</span> given the available observed data: <span class="math notranslate nohighlight">\(P({\bf x}|D)\)</span></p></li>
</ul>
<p>Bayesian Machine Learning is therefore an application of Bayesian theory to the hypotheses of interest in Machine Learning, namely learning and prediction. Let us have a closer look to them.</p>
<section id="bayesian-learning">
<h3><span class="section-number">5.2.1. </span>Bayesian learning<a class="headerlink" href="#bayesian-learning" title="Permalink to this heading">#</a></h3>
<p>As mentioned, in Bayesian theory, learning from data is a result of the application of the Bayes’ theorem to an existing prior probability distribution. In terms of Machine Learning problems, it can be applied both to supervised  learning problems, where we model the probability of a target <span class="math notranslate nohighlight">\(t\)</span> (continuous in regression, discrete in classification) conditional to a set of features <span class="math notranslate nohighlight">\(\bf{x}\)</span> and some observed data <span class="math notranslate nohighlight">\(\it{D}\)</span>:</p>
<div class="math notranslate nohighlight">
\[p(t|\bf{x}, \it{D})\]</div>
<p>or to unsupervised machine learning problems, where we model the probability of a data point <span class="math notranslate nohighlight">\(\bf{x}\)</span> given some observed data <span class="math notranslate nohighlight">\(\it{D}\)</span>:</p>
<div class="math notranslate nohighlight">
\[p(\bf{x}| \it{D})\]</div>
<p>We start by modelling these probability distributions with a prior distribution, specified by a set of parameters <span class="math notranslate nohighlight">\(\bf{\theta}\)</span>. Recall that in the Bayesian paradigm, these parameters are also modelled with a probability distribution, whose prior is:</p>
<div class="math notranslate nohighlight">
\[P(\bf{\theta} | \bf{\alpha})\]</div>
<p>where <span class="math notranslate nohighlight">\(\bf{\alpha}\)</span> are hyper-parameters of this prior distribution.
Now we observe some data <span class="math notranslate nohighlight">\(\it{D}\)</span> which includes observations of the target and the features. Bayesian learning means updating the distribution of the parameters that define the predictive distribution after observing new data, i.e.:</p>
<div class="math notranslate nohighlight">
\[P(\bf{\theta} | \it{D}, \bf{\alpha}) = \frac{P(\it{D}| \bf{\theta}, \bf{\alpha}) P(\bf{\theta}|\bf{\alpha})}{P(\it{D}| \bf{\alpha})} \]</div>
<p>In general, the posterior distribution of the parameters will not have a closed-form, due to the difficulty of computing the denominators (the evidence). There are typically four approaches to tackle this problem:</p>
<ul class="simple">
<li><p>As we saw in the previous section, in the particular case where we use conjugate priors, the posterior can be computed analytically. For a conjugate prior, the posterior belongs to the same to the same family of distributions as the prior, with updated parameters.</p></li>
<li><p>Laplace approximation: in this case the posterior distribution is approximated by a Gaussian by using a second order expansion of the log probability of the posterior around its maximum (MAP, <em>Maximum a Posteriori</em>). We will discuss it later in this section</p></li>
<li><p>Monte Carlo methods, which aim to generate samples from the posterior distribution, despite not having a closed form. In particular, Markov Chain Monte Carlo (MCMC) methods are the most popular ones, which rely on building a Markov Chain that asymptotically converges to the posterior.</p></li>
<li><p>Variational methods, which seek to analytically approximate the posterior by finding the closest one from a more simple family of distributions (e.g. Gaussian ones). For a distance between distributions, the Kullback–Leibler divergence is typically used. A particular case of a variational methods is that of Mean Field Theory, widely used in Statistical Physics, in which the posterior distribution is approximated by the distribution of a set of independent latent factors.</p></li>
</ul>
<p>For an introduction to the most computational demanding methods like MCMC and Variational Methods, a good starting reference is [ONLINE BAYESIAN BOOK]</p>
<section id="bayesian-online-learning">
<h4><span class="section-number">5.2.1.1. </span>Bayesian online learning<a class="headerlink" href="#bayesian-online-learning" title="Permalink to this heading">#</a></h4>
<p>Bayesian learning is particularly suitable for online (or mini batch) learning. In this case, the prior distribution is the result of previous learning, and the posterior updates this prior as new data arrives. In the strictly online limit, this means every time a new observation arrives. Formally, this means:</p>
<div class="math notranslate nohighlight">
\[P(\bf{\theta} | \it{D}_t, \bf{\alpha}) = P(\bf{\theta} | \it{d}_t,\it{D}_{t-1}, \bf{\alpha}) = \frac{P(\it{d}_t| \it{D}_{t-1}, \bf{\theta}, \bf{\alpha}) P(\bf{\theta}|\it{D}_{t-1},\bf{\alpha})}{P(\it{d}_t| \it{D}_{t-1}, \bf{\alpha})} \]</div>
<p>where <span class="math notranslate nohighlight">\(\it{D}_t = \{\it{d}_t, \it{d}_{t-1}, ..., \it{d}_0\} = \{\it{d}_t, \it{D}_{t-1}\} \)</span> represents the dataset indexed by the time <span class="math notranslate nohighlight">\(t\)</span> of arrival of new information, being <span class="math notranslate nohighlight">\(\it{d}_t\)</span> the data acquired at time <span class="math notranslate nohighlight">\(t\)</span>.</p>
</section>
</section>
<section id="bayesian-prediction">
<h3><span class="section-number">5.2.2. </span>Bayesian prediction<a class="headerlink" href="#bayesian-prediction" title="Permalink to this heading">#</a></h3>
<p>After observing some data and updating the probability distribution of the parameters of the model, the model can be used to perform inferences in supervised and unsupervised problems. The peculiarity of the Bayesian Machine Learning paradigm is that we don’t have a single predictive model, but a family of them parametrized by <span class="math notranslate nohighlight">\(\bf{\theta}\)</span>. By learning we aim to narrow down which of the possible models within this family explain better the observed data. But in contrast to classical statistics, we don’t need to narrow our choice to one of these models (like the most likely one), but we use the whole posterior distribution over parameters to improve predictions.</p>
<p>In the supervised machine learning problem, this translates in the following predictive distribution for the target:</p>
<div class="math notranslate nohighlight">
\[p(t|{\bf x}, {\it D}) = \int d \theta p(t|{\bf x}, {\it D}, \theta) p(\theta|{\it D})\]</div>
<p>Similarly, in the unsupervised machine learning problem, the distribution of the target data given observations is calculated as:</p>
<div class="math notranslate nohighlight">
\[p({\bf x}| {\it D}) = \int d \theta p({\bf x}| {\it D}, \theta) p({\bf \theta}|{\it D})\]</div>
<p>These integrals are typically difficult to solve, except for some particular choices for the family of distributions. In case the integrals are intractable, one can resort either to numerical approximations or analytical ones.</p>
<p>Among the analytical approximations, the most simple one is using the MAP (*<em>Maximum a Posteriori</em>+) solution, which as discussed in the previous section means calculating the most likely value of the parameters using the posterior distribution:</p>
<div class="math notranslate nohighlight">
\[\theta_{MAP} = {\bf argmax}_{\theta}  p(\theta | {\it D})\]</div>
<p>Then we do the approximation:</p>
<div class="math notranslate nohighlight">
\[p(\theta | {\it D}) \simeq \delta (\theta - \theta_{MAP})\]</div>
<p>and therefore:</p>
<div class="math notranslate nohighlight">
\[p(t|{\bf x}, {\it D}) = p(t|{\bf x}, \theta_{MAP})\]</div>
<div class="math notranslate nohighlight">
\[p({\bf x}, {\it D}) = p({\bf x}| \theta_{MAP})\]</div>
<p>An improvement upon this approximation is using the Laplace approximation for the posterior, which essentially consists on approximating the posterior using a second order Taylor series around the MAP estimator. Instead of dealing, though, directly with the posterior, it is better to work with its logarithm <span class="math notranslate nohighlight">\(L\)</span>. For simplicity, let us consider the case of a single parameter to estimate:</p>
<div class="math notranslate nohighlight">
\[L = \log p(\theta | {\it D}) \simeq L(\theta_{MAP}) + \frac{1}{2} \frac{d L^2}{d \theta} |_{\theta_{MAP}} (\theta - \theta_{MAP})^2\]</div>
<p>where the first order term is zero since we are expanding around the maximum, and the second derivative is negative. The Laplace approximation for the posterior then reads:</p>
<div class="math notranslate nohighlight">
\[ p(\theta | {\it D}) \simeq A \exp \left[ \frac{1}{2} \frac{d L^2}{d \theta} |_{\theta_{MAP}} (\theta - \theta_{MAP})^2 \right] \]</div>
<p>which is a Gaussian distribution. The predictive distribution in this case cannot be derived generally, it will depend on the shape of the likelihood function.</p>
<p>Notice that in both approximations we are assuming that we are dealing with uni-modal distribution. In case of multi-modal distributions, these approximations no longer approximate well the posterior, and other techniques must be used.</p>
</section>
<section id="example-coin-toss-experiment">
<h3><span class="section-number">5.2.3. </span>Example: coin toss experiment<a class="headerlink" href="#example-coin-toss-experiment" title="Permalink to this heading">#</a></h3>
<p>Let us come back to our previous example of estimating the probability of a coin toss resulting in heads or tails. By using a Beta conjugate prior to the likelihood, the posterior had a closed-form which is an updated Beta distribution:</p>
<div class="math notranslate nohighlight">
\[f(p|D_N) = \frac{1}{B(\alpha + n_H, \beta + N - n_H)} p^{\alpha + n_H-1} (1-p)^{\beta + N - n_H -1} \]</div>
<p>With the information available so far, we want to predict the result of the next coin toss. The predictive distribution reads:</p>
<div class="math notranslate nohighlight">
\[\begin{split}P(t_{N+1} = H| D_N) = \int dp P(t_{N+1} = H | p, D_N) f(p |D_N) = \int dp p f(p |D_N) \\ = \int dp \frac{1}{B(\alpha + n_H, \beta + N - n_H)} p^{\alpha + n_H} (1-p)^{\beta + N - n_H -1}  = \frac{ \alpha + n_H} {\alpha + \beta + N}\end{split}\]</div>
<p>which in this case is the mean of the Beta distribution. Notice that this is not the same result that we would obtain in classical statistics using maximum likelihood even if we had a uniform prior (<span class="math notranslate nohighlight">\(\alpha = \beta = 1\)</span>), since the mean and the mode of the Beta distribution don’t coincide, the mode being <span class="math notranslate nohighlight">\(\frac{\alpha - 1}{\alpha + \beta - 2}\)</span> for <span class="math notranslate nohighlight">\(\beta &gt; 1\)</span></p>
<p>In this case, due to the choice of a conjugate prior, we have a closed-form for the posterior and the predictive distribution can also be analytically calculated. However, let us assume this is not possible and use the Laplace approximation to obtain the predictive distribution. The MAP estimator for the posterior is, by finding the maximum of the Beta distribution:</p>
<div class="math notranslate nohighlight">
\[p_{MAP} = \frac{n_H + \alpha -1}{N + \alpha + \beta - 2} \]</div>
<div class="math notranslate nohighlight">
\[q_{MAP} \equiv 1 - p_{MAP} = \frac{ N - n_H + \beta -1}{N + \alpha + \beta - 2} \]</div>
<p>The second order derivative of the log-likelihood evaluated at the maximum MAP then reads:</p>
<div class="math notranslate nohighlight">
\[\frac{d L^2}{d p^2} |_{p_{MAP}} = - \frac{N + \alpha + \beta - 2}{p_{MAP} q_{MAP}}\]</div>
<p>The posterior distribution is therefore approximated by the following Gaussian distribution:</p>
<div class="math notranslate nohighlight">
\[f(p | {\it D}) \simeq N(p_{MAP}, \sqrt{\frac{p_{MAP} q_{MAP}}{N + \alpha + \beta - 2}})\]</div>
<p>The predictive distribution is now easy to evaluate since it is just the mean of a Gaussian:</p>
<div class="math notranslate nohighlight">
\[P(t_{N+1} = H| D_N) \simeq p_{MAP} = \frac{n_H + \alpha -1}{N + \alpha + \beta - 2}\]</div>
<p>The result differs slightly from the exact one, but for <span class="math notranslate nohighlight">\(N, n_H &gt;&gt; 1\)</span> and / or <span class="math notranslate nohighlight">\(\alpha, \beta &gt;&gt; 1\)</span> they converge, as it is actually expected since in this regime the Beta distribution has the Gaussian distribution as a limit, as discussed in the previous section.</p>
<p>We can test the validity of this result numerically by plotting the actual posterior and the Laplace approximation, for different number of observations and specification of the prior:</p>
<p>CONTINUE</p>
</section>
</section>
<section id="bayesian-linear-regression">
<h2><span class="section-number">5.3. </span>Bayesian Linear Regression<a class="headerlink" href="#bayesian-linear-regression" title="Permalink to this heading">#</a></h2>
</section>
<section id="probabilistic-graphical-models">
<h2><span class="section-number">5.4. </span>Probabilistic Graphical Models<a class="headerlink" href="#probabilistic-graphical-models" title="Permalink to this heading">#</a></h2>
</section>
<section id="latent-variable-models">
<h2><span class="section-number">5.5. </span>Latent variable models<a class="headerlink" href="#latent-variable-models" title="Permalink to this heading">#</a></h2>
<p>So far we have been working with probabilistic models where all variables are observable. Latent variable models are probabilistic models where some of the variables cannot be measured or can only be partially measured, i.e. there is missing data in some of the records of the dataset.</p>
<p>In both cases, formally we define latent models as a probability distribution over two sets of variables:</p>
<div class="math notranslate nohighlight">
\[p({\rm x}, {\rm z}| \theta)\]</div>
<p>where <span class="math notranslate nohighlight">\({\rm x}\)</span> are the set of observable variables and <span class="math notranslate nohighlight">\({\rm z}\)</span> the latent ones. <span class="math notranslate nohighlight">\(\theta\)</span> are the parameters of the probability distribution, which we state explicitly for convenience later.</p>
<section id="partially-observable-latent-variable-models">
<h3><span class="section-number">5.5.1. </span>Partially observable latent variable models<a class="headerlink" href="#partially-observable-latent-variable-models" title="Permalink to this heading">#</a></h3>
<p>As mentioned above, in this case we are dealing with a problem of missing data, where some of the data points of a set of variables have not been recorded for potentially various reasons. Rubin <span id="id1">[<a class="reference internal" href="references.html#id23" title="Roderick J. A. Little and Donald B. Rubin. Statistical Analysis with Missing Data. Wiley, 2019.">1</a>]</span> distinguishes between tree different situations regarding the generative model (i.e. the full probability distribution) for missing data:</p>
<ul class="simple">
<li><p>Missing Completely at Random (MCAR): there is no pattern in the missing data. The missing data is uncorrelated with observed and unobserved data. For instance, if some trades in a booking database are missing due to corruption of the physical support.</p></li>
<li><p>Missing at Random (MAR): in this case there is pattern in the missing data that can be explained using observed data. Coming back to the example of a trades database, if the missing data is due to a particular sales person that tends to forget to register a voice operation in the systems</p></li>
<li><p>Missing Not at Random (MNAR): in this case there is also a pattern in the missing data, but can only be explained using unobserved data (e.g. full latent variables). In the example of the booking database, if the issue comes from a certain sales person as in MAR, but the booking database does not record in any case the sales person identity.</p></li>
</ul>
<p>The use of probabilistic methods for missing data is called multiple imputation and is described extensively in <span id="id2">[<a class="reference internal" href="references.html#id23" title="Roderick J. A. Little and Donald B. Rubin. Statistical Analysis with Missing Data. Wiley, 2019.">1</a>]</span>. The idea is essentially to estimate the model from the observed data using the techniques we will explore below in this section, and use it as a generative model for the unobserved variables, namely computing:</p>
<div class="math notranslate nohighlight">
\[p({\rm x_{missing}} | {\rm x_{observed}}, \theta)\]</div>
<p>Imputation of missing data can then be done using some statistic of the distribution like the mean or the median, or generating random samples from the distribution.</p>
<p>Multiple imputation works well for MCAR and reasonably (with some negligible bias) for MAR. The case of MNAR is more complicated, and can only be properly dealt with by introducing full latent variables that properly capture the MNAR mechanism, which is difficult if there is not a prior knowledge of the missing data mechanism.</p>
</section>
<section id="full-latent-variable-models">
<h3><span class="section-number">5.5.2. </span>Full latent variable models<a class="headerlink" href="#full-latent-variable-models" title="Permalink to this heading">#</a></h3>
<p>In this case, there are not available observations of the latent variables. Models with full latent variables are closely related to probabilistic graphical models, since in many situations latent variables are introduced as prior knowledge in the structure of a model about a certain process, particularly when trying to model causal relationships. For instance, a model of two variables that are correlated but we believe not to be directly causally related, can be naturally extended by introducing a latent variable, a confounder, that influences both.</p>
<p>We can distinguish between two practical cases where latent variable models naturally emerge:</p>
<ul class="simple">
<li><p>On the one hand, in problems where we know of the existence of a relevant variable, but it has not been observed and recorded in the data gathering process. An example, for instance, in the problem of demand estimation where only trades are observed, are the total interests or requests from clients to trade, that only in a fraction of cases (the hit &amp; miss) generate a trade.</p></li>
<li><p>On the other hand, a latent variable can be an abstraction that cannot be directly measured but it might represent a common influence or effective factor.</p>
<ul>
<li><p>One example is that of a market regime, which is modelled as a categorical variable with a set of states (e.g. high volatility and low volatility), and influences the price returns. Hidden Markov Models are latent variable models frequently used to model the effect of so-called regime switch of the statistics of price returns. They are typically applied as signal generators that alert of changes in market behavior.</p></li>
<li><p>Another example is the concept of mid or fair price. In real markets price information comes from trades, RfQs, limit order books, composite prices and so on. But the concept of mid / fair price of an instrument is unobservable. Kalman filters are typically used to infer the value of the mid-price from price informative observations.</p></li>
</ul>
</li>
</ul>
</section>
<section id="examples-of-latent-variable-models">
<h3><span class="section-number">5.5.3. </span>Examples of Latent Variable Models<a class="headerlink" href="#examples-of-latent-variable-models" title="Permalink to this heading">#</a></h3>
<section id="gaussian-mixture-models-gmms">
<h4><span class="section-number">5.5.3.1. </span>Gaussian Mixture Models (GMMs)<a class="headerlink" href="#gaussian-mixture-models-gmms" title="Permalink to this heading">#</a></h4>
<p>The Gaussian Mixture Model postulates that data observations <span class="math notranslate nohighlight">\(\vec{x}_n\)</span>, <span class="math notranslate nohighlight">\(n = 1, ..., N\)</span> are generated by <span class="math notranslate nohighlight">\(K\)</span> independent multivariate Gaussian distributions with unknown parameters <span class="math notranslate nohighlight">\(\vec{\mu}_k, \Sigma_k\)</span>. The membership of each data point to a particular Gaussian is determined by latent categorical random variables <span class="math notranslate nohighlight">\(z_n = {1, ..., K}\)</span> with probabilities <span class="math notranslate nohighlight">\(\pi_k\)</span>. Gaussian Mixture models are used to model unobserved subpopulations or segments from a given population. The probability distribution is therefore given by:</p>
<div class="math notranslate nohighlight">
\[p(\vec{x_n}) = \sum_{k=1}^K \pi_k {\mathcal N}(\vec{x_n}|\vec{\mu}_k, \Sigma_k)\]</div>
<p>which is a linear combination of Gaussian distributions, hence the name mixture of Gaussians. Mixture models are actually more general in that other distributions can be used to model the mixture components, namely a Binomial, Dirichlet, Poisson, Exponential, etc.</p>
<p>As a graphical model, the so-called plate notation is usually employed to simplify the representation of the <span class="math notranslate nohighlight">\(N\)</span> random variables <span class="math notranslate nohighlight">\(\vec{x_n}\)</span> and their associated <span class="math notranslate nohighlight">\(K\)</span> latent random variables <span class="math notranslate nohighlight">\(z_n\)</span> categorizing their mixture component membership. The following figure represents the mode in plate notation:</p>
<figure class="align-default" id="fig-gmm-model">
<a class="reference internal image-reference" href="../_images/gmm_model.png"><img alt="../_images/gmm_model.png" src="../_images/gmm_model.png" style="width: 3in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.5 </span><span class="caption-text">Gaussian Mixture Model in plate notation.</span><a class="headerlink" href="#fig-gmm-model" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The joint probability distribution of the model, including the latent variables, is the following:</p>
<div class="math notranslate nohighlight">
\[P(\vec{x_n}, z_n) = \sum_{k=1}^K \pi_k^{\delta_{z_n,k}} {\mathcal N}(\vec{x_n}|\vec{\mu}_k, \Sigma_k)\]</div>
<p>where <span class="math notranslate nohighlight">\(\delta_{z_n,k}\)</span> is an indicator function: <span class="math notranslate nohighlight">\(\delta_{z_n,k} = 1\)</span> if  z<span class="math notranslate nohighlight">\(_n = k\)</span>, <span class="math notranslate nohighlight">\(0\)</span> otherwise</p>
<p>Gaussian mixture models have plenty of real-life applications. The most popular probably is its use as a clustering technique, and actually the most popular clustering technique itself, K-Means, is a particular case of the Gaussian Mixture model for spherical distributions, <span class="math notranslate nohighlight">\(\Sigma_k = \epsilon I\)</span>, in the limit <span class="math notranslate nohighlight">\(\epsilon \rightarrow 0\)</span>, when it is calibrated using the Expectation Maximization (EM) algorithm (see section below).</p>
<p>Another interesting application is anomaly detection, particularly for one-dimensional data. If we have theoretical reasons to expect the distribution of data to be a Gaussian, we can use a mixture of two Gaussians
distributions with the same mean but different standard deviations to capture anomalies from <em>normality</em>:</p>
<div class="math notranslate nohighlight">
\[p(x_n) =  \pi_G {\mathcal N}(x_n|\mu, \sigma_G) + (1-\pi_G) {\mathcal N}(x_n|\mu, \sigma_B)\]</div>
<p>where <span class="math notranslate nohighlight">\(G\)</span> and <span class="math notranslate nohighlight">\(B\)</span> stand for good and bad, respectively, since the model is called the Good and Bad data model <span id="id3">[<a class="reference internal" href="references.html#id5" title="D.S. Silvia. Data Analysis: A Bayesian Tutorial. Oxford Science Publications, second edition, 2006.">2</a>]</span>. After learning the parameters of the model using our data (<span class="math notranslate nohighlight">\(\pi_G\)</span>, <span class="math notranslate nohighlight">\(\mu\)</span>, <span class="math notranslate nohighlight">\(\sigma_G\)</span>, <span class="math notranslate nohighlight">\(\sigma_G\)</span>), and making by definition <span class="math notranslate nohighlight">\(\sigma_B \geq \sigma_G\)</span>, we classify as anomalies those points in the dataset that are more likely to have been generated by the “bad” data Gaussian. This is done by inferring the probability of being “bad data” using Bayes’ theorem:</p>
<div class="math notranslate nohighlight">
\[p(\text{bad}|x_n) = \frac{p(x_n|\text{bad}) p(\text{bad})}{p(x_n)} = \frac{𝑁(𝑥_n|𝜇,𝜎_𝐵) (1-\pi_G)}{𝜋_G 𝑁(𝑥_n|𝜇,𝜎_𝐺)+(1−𝜋_𝐺)𝑁(𝑥_n|𝜇,𝜎_𝐵)}\]</div>
<p>and classifying as “bad data” or anomalies those points whose probability exceeds a given threshold, typically 0.5. This anomaly detection model works surprisingly well highlighting anomalies in data that humans could also recognize visually, making it a good candidate for automating human-driven anomaly detections.</p>
</section>
<section id="hidden-markov-model-hmm">
<h4><span class="section-number">5.5.3.2. </span>Hidden Markov Model (HMM)<a class="headerlink" href="#hidden-markov-model-hmm" title="Permalink to this heading">#</a></h4>
<p>A Hidden Markov Model explains a sequence of observations <span class="math notranslate nohighlight">\(x_1, ..., x_T\)</span> as driven by a hidden variable (the state) <span class="math notranslate nohighlight">\(y_1, ..., y_T\)</span> that has Markov dynamics:</p>
<div class="math notranslate nohighlight">
\[P(y_{t+1}|y_t, ..., y_1) =  P(y_{t+1}|y_t)\]</div>
<p>where the latter is called the transition probability, which usually is assumed stationary in the model:</p>
<div class="math notranslate nohighlight">
\[P(y_{t+s+1}|y_{t+s})=P(y_{t+1}|y_t)\]</div>
<p>The observations only depend on the state of the hidden variable at the observation time:</p>
<div class="math notranslate nohighlight">
\[P(x_t|y_t, ..., y_1, x_{t-1}, ..., x_1) = P(x_t|y_t)\]</div>
<p>The joint probability distribution of the sequence therefore simplifies to:</p>
<div class="math notranslate nohighlight">
\[P(x_1, ..., x_N, y_1, ..., y_N)=P(y_1) P(x_1|y_1)\Pi_{t=2}^T P(y_t|y_{t-1})P(x_t | y_t)\]</div>
<p>As a graphical model, the HMM has the following structure:</p>
<figure class="align-default" id="fig-hmm-model">
<a class="reference internal image-reference" href="../_images/hmm_model.png"><img alt="../_images/hmm_model.png" src="../_images/hmm_model.png" style="width: 4in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.6 </span><span class="caption-text">Graph representation of the Hidden Markov Model</span><a class="headerlink" href="#fig-hmm-model" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The hidden (latent) and observed variables can be either discrete or continuous. In many applications of HMMs, the latent variable is discrete. In this case, learning and inference is more tractable for general distributions. An example in the financial markets is explaining price returns as generated from a Gaussian noise whose volatility depends on a discrete hidden state, which is interpreted as a market or volatility regime.</p>
<p>A typical question when studying HMMs is to infer the most likely sequency of hidden states given a time-series of observations. There are specialized algorithms to do this task efficiently, the most popular one being the Viterbi algorithm <span id="id4">[<a class="reference internal" href="references.html#id19" title="Kevin P. Murphy. Machine learning : a probabilistic perspective. MIT Press, Cambridge, Mass. [u.a.], 2013. ISBN 9780262018029 0262018020.">3</a>]</span>. For learning, a special case of the Expectation Maximization algorithms (EM) is used, called the Baum - Welch algorithm.</p>
</section>
<section id="the-kalman-filter">
<h4><span class="section-number">5.5.3.3. </span>The Kalman Filter<a class="headerlink" href="#the-kalman-filter" title="Permalink to this heading">#</a></h4>
<p>A Kalman filter is a particular instance of a filtering algorithm first proposed by Rudolf E. Kalman in 1960 in his seminal paper “A New Approach to Linear Filtering and Prediction Problems” <span id="id5">[<a class="reference internal" href="references.html#id7" title="Rudolf Emil Kalman. A new approach to linear filtering and prediction problems. Journal of Basic Engineering, 82(1):35–45, 1960.">4</a>]</span>. It is a particular case of a Hidden Markov Model where both the hidden and observed variables are continuous and follow Gaussian distributions. In this simple case, inference and learning becomes tractable, making it a very efficient estimation algorithm. Actually, the term Kalman Filter applies rigorously to an estimation algorithm that takes a series of noisy observations and produces  estimations of the underlying variable by combining its previous estimate and the current observation. In order to be optimal, transition and observation noises have to be Gaussian.</p>
<p>The model in its graphical form is the same as shown in the HMM, being a particular case of this one. Mathematically, and considering for generality a vector of observations and hidden states, it can be written as:</p>
<div class="math notranslate nohighlight">
\[\vec{y}_{t+1} = \Phi \vec{y}_t + \vec{w}_t,  \vec{w}_t \sim {\mathcal N}(0, Q)\]</div>
<div class="math notranslate nohighlight">
\[\vec{x}_t = H \vec{y}_t + \vec{v}_t, \vec{v}_t \sim {\mathcal N}(0, R)\]</div>
<p>The Kalman Filter is a recursive algorithm to estimate the hidden state value given observations, using two steps: the prediction and the update. First, using the best estimate at <span class="math notranslate nohighlight">\(t-1\)</span>, we predict the value of the hidden state at <span class="math notranslate nohighlight">\(t\)</span> and its covariance <span class="math notranslate nohighlight">\(P\)</span>:</p>
<div class="math notranslate nohighlight">
\[\vec{y}_{t|t-1} = \Phi \vec{y}_{t-1|t-1}\]</div>
<div class="math notranslate nohighlight">
\[P_{t|t-1} = \Phi P_{t-1|t-1} \Phi^t + Q\]</div>
<p>where we use the suffix <span class="math notranslate nohighlight">\(\{t|t-1\}\)</span> to distinguish between the optimal estimations and the underlying random variables. Then, given an observation at <span class="math notranslate nohighlight">\(t\)</span>, we update the estimation as:</p>
<div class="math notranslate nohighlight">
\[\vec{y}_{t|t} = \vec{y}_{t|t-1} + K_t (\vec{y}_{t|t-1} - H \vec{x}_t) = (I-K_t) \vec{y}_{t|t-1} + K_t H \vec{x}_t\]</div>
<div class="math notranslate nohighlight">
\[P_{t|t} = (1-K_t H) P_{t|t-1} \]</div>
<div class="math notranslate nohighlight">
\[K_t =  P_{t|t-1} H ( H P_{t|t-1} H^t + R)^{-1}\]</div>
<p>where <span class="math notranslate nohighlight">\(K_t\)</span> is called the Kalman gain. It controls the impact of the observation on the final estimate: if <span class="math notranslate nohighlight">\(K_t = 0\)</span>, then the final estimation is purely the prediction, whereas for <span class="math notranslate nohighlight">\(K_t = I\)</span>, the estimation is fully determined by the observation.</p>
<p>These so-called filtering equations can be derived using probability theory, computing the posterior distributions under the information available. For the predict step, this means computing:</p>
<div class="math notranslate nohighlight">
\[P(\vec{y}_{t+1} | \vec{x}_{0:t})\]</div>
<p>where <span class="math notranslate nohighlight">\(\vec{x}_{0:t}\)</span> denotes observations up to <span class="math notranslate nohighlight">\(t\)</span>. For the update state, we compute:</p>
<div class="math notranslate nohighlight">
\[P(\vec{y}_{t+1} | \vec{x}_{0:t+1})\]</div>
<p>i.e. now we include the observation at time <span class="math notranslate nohighlight">\(t+1\)</span> to update the estimation of the latent variables.</p>
<p>The filtering equations are central to Kalman filtering, as in most applications the goal is to estimate the most recent values of the latent variables given all available information up to the current time. However, there are situations where we may want to refine estimates of latent variables at earlier times by incorporating <em>all</em> available information, including observations collected after the time of interest. In such cases, the quantity of interest is</p>
<div class="math notranslate nohighlight">
\[P(\vec{y}_{t} \mid \vec{x}_{0:T})\]</div>
<p>where information up to time <span class="math notranslate nohighlight">\(T\)</span> is taken into account. The equations derived for this inference are known as <em>smoothing equations</em>, since the refinement produces a smoother estimate of the latent variables across time.</p>
<p>In the next subsection, we will derive both the filtering and the smoothing equations computing these probability distributions for a simple instance of the Kalman filter: the local level model.</p>
<p>Kalman filters have many applications in multiple domains. In financial markets, they can be used to combine different noisy observations of spot prices to infer a consistent estimate of a fair or mid price, to be used typically in the market-making of relatively illiquid instruments like bonds or some derivatives <span id="id6">[<a class="reference internal" href="references.html#id20" title="E. Synclair. Option Trading: Pricing and Volatility Strategies and Techniques. Wiley, 1st edition, 2010.">5</a>]</span>. Another use case is to infer prices of instruments when markets are closed, which can be useful for hedging indexes that have components with different time zones. In reference <span id="id7">[<a class="reference internal" href="references.html#id21" title="A. Javaheri, D. Lautier, and A. E. Galli. Filtering in finance. Wilmott Magazine, 2003.">6</a>]</span>, they have been applied to the estimation of term structure models. And in the context of investment strategies, they have a relatively popular application to improve pairs trading strategies <span id="id8">[<a class="reference internal" href="references.html#id22" title="Ernest P. Chan. Algorithmic Trading: Winning Strategies and Their Rationale. Wiley Trading. John Wiley &amp; Sons, 2013.">7</a>]</span>. We will delve into some of these applications later in this book.</p>
<section id="the-local-level-model">
<h5><span class="section-number">5.5.3.3.1. </span>The local level model<a class="headerlink" href="#the-local-level-model" title="Permalink to this heading">#</a></h5>
<p>A simple tractable model that illustrates well the general theory of the Kalman filter is the local level model, which despite its simplicity can have relevant applications, for instance in pricing. The model has the structure:</p>
<div class="math notranslate nohighlight">
\[y_{t+1} = y_t + w_t, w_t \sim {\mathcal N}(0, \sigma_w^2)\]</div>
<div class="math notranslate nohighlight">
\[x_t = y_t + v_t, v_t \sim {\mathcal N}(0, \sigma_v^2)\]</div>
<p><strong>Derivation of the forward filtering equations: predict and update</strong></p>
<p>The predict equation can be derived by computing the  distribution of <span class="math notranslate nohighlight">\(y_{t+1}\)</span> conditional to the previous observations, which we denote <span class="math notranslate nohighlight">\(x_{0:t}\)</span>:</p>
<div class="math notranslate nohighlight">
\[p(y_{t+1}|x_{0:t})\]</div>
<p>We can compute this probability by using the marginalization rule over the latent variable <span class="math notranslate nohighlight">\(y_t\)</span>:</p>
<div class="math notranslate nohighlight">
\[p(y_{t+1}|x_{0:t}) = \int dy_t p(y_t|x_{0:t})p(y_{t+1}|y_t)\]</div>
<p>where we have exploited the graphical structure of the model to simplify <span class="math notranslate nohighlight">\(p(y_{t+1}|y_t, x_{0:t}) = p(y_{t+1}|y_t) = {\mathcal N}(y_{t+1}| y_t, \sigma_w^2)\)</span>, since <span class="math notranslate nohighlight">\(y_t\)</span> is the only parent of <span class="math notranslate nohighlight">\(y_{t+1}\)</span>. The proof is based on induction: let us assume that <span class="math notranslate nohighlight">\(p(y_t|x_{0:t}) \sim {\mathcal N}(\hat{y}_{t|t}, \sigma_{t|t}^2)\)</span>, i.e. it is Gaussian with mean <span class="math notranslate nohighlight">\(\hat{y}_{t|t}\)</span> and standard deviation <span class="math notranslate nohighlight">\(\sigma_{t|t}\)</span>. <span class="math notranslate nohighlight">\(p(y_{t+1}|x_{0:t})\)</span> is therefore the result of the convolution of two Gaussian distributions, which is itself a Gaussian distribution with:</p>
<div class="math notranslate nohighlight">
\[p(y_{t+1}|x_{0:t}) = {\mathcal N}(y_{t+1}|\hat{y}_{t|t}, \sigma_{t|t}^2 +\sigma_w^2)\]</div>
<p>This completes the derivation of the predict equations, given by:</p>
<div class="math notranslate nohighlight">
\[\hat{y}_{t+1|t} = \hat{y}_{t|t}\]</div>
<div class="math notranslate nohighlight">
\[\sigma_{t+1|t}^2 = \sigma_{t|t}^2 +\sigma_w^2\]</div>
<p>where, as a reminder to the reader, we use the notation <span class="math notranslate nohighlight">\(t_1 | t_2\)</span> to mean a variable at time <span class="math notranslate nohighlight">\(t_1\)</span> conditional to information up to <span class="math notranslate nohighlight">\(t_2\)</span>. We have not yet, though, completed the induction proof, since we have not proven why <span class="math notranslate nohighlight">\(p(y_t|x_{0:t})\)</span> follows a Gaussian distribution.</p>
<p>This is directly linked to the derivation of the update equation, which corresponds to the following inference:</p>
<div class="math notranslate nohighlight">
\[p(y_{t+1}|x_{0:t+1})\]</div>
<p>We can write <span class="math notranslate nohighlight">\(p(y_{t+1}|x_{0:t+1}) = p(y_{t+1}|x_{0:t}, x_{t+1})\)</span> and apply Bayes’ theorem on <span class="math notranslate nohighlight">\(x_{t+1}\)</span>:</p>
<div class="math notranslate nohighlight">
\[p(y_{t+1}|x_{0:t+1}) = \frac{p(x_{t+1}|y_{t+1}) p(y_{t+1}|x_{0:t})}{p(x_{t+1}|x_{0:t})}\]</div>
<p>We readily recognize the probability distribution derived in the predict step:</p>
<div class="math notranslate nohighlight">
\[p(y_{t+1}|x_{0:t}) = {\mathcal N}(y_{t+1}|\hat{y}_{t|t}, \sigma_{t|t}^2 +\sigma_w^2) \equiv {\mathcal N}(y_{t+1}|\hat{y}_{t+1|t}, \sigma_{t+1|t}^2)\]</div>
<p>Moreover, from the model definition we have:</p>
<div class="math notranslate nohighlight">
\[p(x_{t+1}|y_{t+1}) = {\mathcal N}(x_{t+1}|y_{t+1}, \sigma_v^2)\]</div>
<p>This is the normalized product of two Gaussian density functions, which is also a Gaussian density function, since we can complete the square:</p>
<div class="math notranslate nohighlight">
\[\frac{1}{2\sigma_{t+1|t}^2}(y_{t+1} - \hat{y}_{t+1|t})^2 + \frac{1}{2\sigma_v^2}(x_{t+1} - y_{t+1})^2 =
\frac{1}{2\sigma_{t+1|t+1}^2}(y_{t+1} - \hat{y}_{t+1|t+1})^2 + ... \]</div>
<p>where we have omitted terms that are constant with respect to <span class="math notranslate nohighlight">\(y_{t+1}\)</span> and we have defined:</p>
<div class="math notranslate nohighlight">
\[\hat{y}_{t+1|t+1} = \sigma_{t+1|t+1}^2 (\frac{\hat{y}_{t+1|t}}{\sigma_{t+1|t}^2} + \frac{x_{t+1}}{\sigma_v^2})\]</div>
<div class="math notranslate nohighlight">
\[\sigma_{t+1|t+1}^2 = \frac{\sigma_v^2 \sigma_{t+1|t}^2}{\sigma_v^2 + \sigma_{t+1|t}^2}\]</div>
<p>If we introduce the Kalman gain as:</p>
<div class="math notranslate nohighlight">
\[K_{t+1} =  \frac{\sigma_{t+1|t}^2}{\sigma_v^2 + \sigma_{t+1|t}^2} \]</div>
<p>we can write these equations as:</p>
<div class="math notranslate nohighlight">
\[\hat{y}_{t+1|t+1} = \hat{y}_{t+1|t} + K_{t+1}(x_{t+1} - \hat{y}_{t+1|t})\]</div>
<div class="math notranslate nohighlight">
\[ \sigma_{t+1|t+1}^2 =  (1 - K_{t+1} )\sigma_{t+1|t}^2\]</div>
<p>which are the update equations of the Kalman filter, corresponding to the mean and variance of <span class="math notranslate nohighlight">\(y_{t+1}\)</span> conditional to observations up to <span class="math notranslate nohighlight">\(t+1\)</span>, which follows a Gaussian distribution:</p>
<div class="math notranslate nohighlight">
\[p(y_{t+1}|x_{0:t+1}) = {\mathcal N}(y_{t+1}|\hat{y}_{t+1|t+1}, \sigma_{t+1|t+1}^2)\]</div>
<p>As a consequence, we have proven by induction that <span class="math notranslate nohighlight">\(p(y_t|x_{0:t})\)</span> follows a Gaussian distribution, as long as the initial condition <span class="math notranslate nohighlight">\(p(y_0)\)</span> is also Gaussian.</p>
<p><strong>Derivation of the smoothing equations</strong></p>
<p>The derivation of the smoothing equations follows the same lines as the forward filtering equations. In this case, we are interested in computing:</p>
<div class="math notranslate nohighlight">
\[p(y_t|x_{0:T})\]</div>
<p>i.e., we seek to infer the distribution of the latent variable at <span class="math notranslate nohighlight">\(0 \leq t \leq T\)</span>. We can exploit the structure of the graphical model to compute this expression by conditioning on <span class="math notranslate nohighlight">\(y_{t+1}\)</span>:</p>
<div class="math notranslate nohighlight">
\[p(y_t|x_{0:T}) = \int d y_{t+1} p(y_t|y_{t+1}, x_{0:T}) p(y_{t+1}|x_{0:T}) = \int d y_{t+1} p(y_t|y_{t+1}, x_{0:t}) p(y_{t+1}|x_{0:T})\]</div>
<p>Where we have used, in the second step, the fact that conditioning on <span class="math notranslate nohighlight">\(y_{t+1}\)</span> removes the dependence on <span class="math notranslate nohighlight">\(x_{t+1:T}\)</span>.</p>
<p>We assume that we have already done the forward filter path, so we have computed the distributions <span class="math notranslate nohighlight">\(p(y_t|x_{0:t}) \sim {\mathcal N}(\hat{y}_{t|t}, \sigma^2_{t|t})\)</span> and <span class="math notranslate nohighlight">\(p(y_{t+1}|x_{0:t}) \sim {\mathcal N}(\hat{y}_{t+1|t}, \sigma^2_{t+1|t})\)</span>. In particular, notice that for <span class="math notranslate nohighlight">\(t = T\)</span>, the filtering and smoothing inferences are the same, given by <span class="math notranslate nohighlight">\(p(y_{T}|x_{0:T})\)</span>.</p>
<p>Therefore, we will derive the smoothing equations backwards starting from <span class="math notranslate nohighlight">\(p(y_{T}|x_{0:T}) \sim {\mathcal N}(\hat{y}_{T|T}, \sigma^2_{T|T})\)</span> as the initial condition. The key idea is to assume that we already have computed:</p>
<div class="math notranslate nohighlight">
\[p(y_{t+1}|x_{0:T}) \sim {\mathcal N}(\hat{y}_{t+1|T}, \sigma^2_{t+1|T})\]</div>
<p>and use it to compute <span class="math notranslate nohighlight">\(p(y_t|x_{0:T})\)</span>. This means to compute the previous integral which depends on <span class="math notranslate nohighlight">\(p(y_{t+1}|x_{0:T})\)</span> which we have already computed, and <span class="math notranslate nohighlight">\(p(y_t|y_{t+1}, x_{0:t})\)</span>, which we need to derive. We use the standard trick of computing the join probability <span class="math notranslate nohighlight">\(p(y_t, y_{t+1}|x_{0:t})\)</span>. Since <span class="math notranslate nohighlight">\(y_{t+1} = y_t + w_t\)</span>, where <span class="math notranslate nohighlight">\(y_t\)</span> and <span class="math notranslate nohighlight">\(w_t\)</span> are uncorrelated, this is simply:</p>
<div class="math notranslate nohighlight">
\[\begin{split}p(y_t, y_{t+1}|x_{0:t}) \sim {\mathcal N} \left( \left[ \begin{matrix} \hat{y}_{t|t} \\ \hat{y}_{t+1|t} \end{matrix} \right], \left[ \begin{matrix} \sigma^2_{t|t} &amp; \sigma^2_{t|t} \\ \sigma^2_{t|t} &amp; \sigma^2_{t+1|t} \end{matrix} \right] \right)\end{split}\]</div>
<p>where we have used that <span class="math notranslate nohighlight">\(cov(y_t, y_{t+1}) = var(y_t)\)</span> in this model. Now we can derive the conditional distribution using:</p>
<div class="math notranslate nohighlight">
\[p(y_t|y_{t+1}, x_{0:t}) = \frac{p(y_t, y_{t+1}|x_{0:t})}{p(y_{t+1}|x_{0:t})}\]</div>
<p>Computing this expression we get:</p>
<div class="math notranslate nohighlight">
\[p(y_t|y_{t+1}, x_{0:t}) \sim {\mathcal N}\left( \hat{y}_{t|t}+ \frac{\sigma^2_{t|t}}{ \sigma^2_{t+1|t}}(y_t - \hat{y}_{t+1|t}), \sigma^2_{t|t}(1 - \frac{\sigma^2_{t|t}}{\sigma^2_{t+1|t}})\right)\]</div>
<p>Now we have the expression for the two probabilities involved in the calculation of <span class="math notranslate nohighlight">\(p(y_t|x_{0:T})\)</span>, which can be carried by completing the squares on <span class="math notranslate nohighlight">\(y_{t+1}\)</span> and integrating it away. The final result is, unsurprisingly, a Gaussian distribution with the following backward iterative equations for the mean and variance, which conform the Rauch - Tung - Striebel smoothing equations:</p>
<div class="math notranslate nohighlight">
\[\hat{y}_{t|T} = \hat{y}_{t|t} + \frac{\sigma^2_{t|t}}{ \sigma^2_{t+1|t}}\left(\hat{y}_{t+1|T} - \hat{y}_{t+1|t}\right)\]</div>
<div class="math notranslate nohighlight">
\[\sigma^2_{t|T} = \sigma^2_{t|t} + (\frac{\sigma^2_{t|t}}{ \sigma^2_{t+1|t}})^2(\sigma^2_{t+1|T} - \sigma^2_{t+1|t})\]</div>
<p>As mentioned, we compute first the forward filtering equations until <span class="math notranslate nohighlight">\(T\)</span>, which are used to initialize the smoothing equations and proceed backwards. The smoothing equations provide a better estimation than the filtering ones, since they use the full information set up to <span class="math notranslate nohighlight">\(T\)</span>. This translates into smaller estimation variances. They cannot be used in an online context, though, which is where Kalman filters find a large number of applications. Their main application is parameter estimation, since they are required to compute the likelihood of the data when using maximum likelihood estimation.</p>
</section>
</section>
</section>
<section id="estimation-of-latent-variable-models">
<h3><span class="section-number">5.5.4. </span>Estimation of Latent Variable Models<a class="headerlink" href="#estimation-of-latent-variable-models" title="Permalink to this heading">#</a></h3>
<p>Given observations and a latent variables model  typically represented as a probabilistic graphical model, we would like to estimate the parameters of the model. The difficulty, here, comes from the latent variables, for which we don’t have observations. There are different ways to estimate parameters in this case, but we will focus on the most popular ones: maximum likelihood estimation (MLE) and expectation maximization (EM), which computes an approximation of the MLE solution.</p>
<section id="maximum-likelihood-estimation-mle">
<h4><span class="section-number">5.5.4.1. </span>Maximum Likelihood Estimation (MLE)<a class="headerlink" href="#maximum-likelihood-estimation-mle" title="Permalink to this heading">#</a></h4>
<p>The presence of latent variables does not preclude the use of MLE. Our goal is still to find the parameters that maximize the probability of the observations given the parameters (or likelihood). If we could observe the latent variables, that would mean maximizing the so-called (in the context of latent variable models) complete likelihood:</p>
<div class="math notranslate nohighlight">
\[\Pi_{n=1}^N P({\rm x}_n, {\rm z}_n| \theta)\]</div>
<p>Sine the latter are not observed, we need to integrate them out:</p>
<div class="math notranslate nohighlight">
\[\sum_{z_1, ..., z_N} \Pi_{n=1}^N P({\rm x}_n, {\rm z}_n| \theta)\]</div>
<p>where we have assumed discrete distributions for the latent variables. For continuous ones, the sums have to be replaced by integrals. The latter is called the incomplete likelihood.</p>
<p>Whereas maximizing incomplete likelihoods with respect to the parameters is in theory possible, in practice it is usually a computationally expensive method. Therefore the need to develop more efficient estimation methods like Expectation Maximization.</p>
</section>
<section id="expectation-maximization-em">
<h4><span class="section-number">5.5.4.2. </span>Expectation Maximization (EM)<a class="headerlink" href="#expectation-maximization-em" title="Permalink to this heading">#</a></h4>
<p>If we were to have observations of the latent variables, we could then maximize the complete likelihood and get maximum likelihood estimators for the parameters <span class="math notranslate nohighlight">\(\theta\)</span>. However, we don’t have those observations and we need to maximize the incomplete likelihood, which is a harder problem.</p>
<p>The intuition behind the Expectation Maximization (EM) method lies in this idea of maximizing the complete likelihood. Since we don’t have observations of the latent variables, EM seeks to maximize the incomplete likelihood breaking the problem in two easier steps:</p>
<ul class="simple">
<li><p>E-step: use observations and current best estimates of the parameters to infer values for the latent variables</p></li>
<li><p>M-step: Plug these values into the complete likelihood and maximize it to get new estimations for the parameters
These steps are repeated until convergence.</p></li>
</ul>
<p>Mathematically, EM does not guarantee to find a global optimum for the incomplete likelihood, but under certain general conditions it can be shown that the EM solution is a lower bound for the global maximum.</p>
<p>Let us now look at the algorithm in more detail. We start with the <em>E-step</em>. The best estimate for the hidden variables given the latest estimation of parameters <span class="math notranslate nohighlight">\(\theta^{(s)}\)</span> and observations is given by the following conditional distribution:</p>
<div class="math notranslate nohighlight">
\[P({\rm z}|\{{\rm x_n}\},\theta^{s})\]</div>
<p>It is called E-step because it computes the expectation of the complete (log)likelihood (the log to make it easier to compute) using this distribution, where the complete log-likelihood has yet unknown parameters <span class="math notranslate nohighlight">\(\theta\)</span>, which we denote:</p>
<div class="math notranslate nohighlight">
\[Q(\theta|\theta^{(s)}) \equiv \mathbb{E}_s[\log Π^𝑁_{𝑛=1}𝑃(x_𝑛,z_𝑛|𝜃)] \]</div>
<div class="math notranslate nohighlight">
\[=  \sum_n \sum_{z_n} P({\rm z_n}|{\rm x_n},\theta_{t})\log 𝑃(x_𝑛,z_𝑛|𝜃) \]</div>
<p>where we have introduced the compact notation <span class="math notranslate nohighlight">\(\mathbb{E}_s[...] = \mathbb{E}[...|\{{\rm x_n}\},\theta^{s}]\)</span>.</p>
<p>The <em>M-step</em> then maximizes this function with respect to <span class="math notranslate nohighlight">\(\theta\)</span>, which become <span class="math notranslate nohighlight">\(\theta^{(s+1)}\)</span> in the iterative algorithm.</p>
<p>The sketch to prove that the EM solution is a lower bound of the maximum incomplete likelihood is relatively easy, so we do it in the follow. We start from the incomplete log-likelihood:</p>
<div class="math notranslate nohighlight">
\[\log \Pi_{n=1}^N P({\rm x}_n| \theta) = \sum_{n=1}^N \log \sum_{z_n}  P({\rm x}_n, {\rm z}_n| \theta) = \sum_{n=1}^N  \log \sum_{z_n} q({\rm z_n}|{\rm x}_n) \frac{P({\rm x}_n, {\rm z}_n| \theta)}{q({\rm z_n}|{\rm x}_n)}\]</div>
<p>where <span class="math notranslate nohighlight">\(q(z_n|x_n)\)</span> is so far a generic distribution. Now since <span class="math notranslate nohighlight">\(log\)</span> is a concave function, we can use Jensen’s inequality, whose general form is:</p>
<div class="math notranslate nohighlight">
\[f(\sum_n \lambda_n x_n) \geq \sum_n \lambda_n f(x_n)\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda_n \geq 0\)</span> and <span class="math notranslate nohighlight">\(\sum_n \lambda_n = 1\)</span>. Applied to our case, where <span class="math notranslate nohighlight">\(\lambda_n = q({\rm z_n}|{\rm x}_n)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\sum_{n=1}^N  \log \sum_{z_n} q({\rm z_n}|{\rm x}_n) \frac{P({\rm x}_n, {\rm z}_n| \theta)}{q({\rm z_n}|{\rm x}_n)} \geq \sum_{n=1}^N \sum_{z_n} q({\rm z_n}|{\rm x}_n) \log \frac{P({\rm x}_n, {\rm z}_n| \theta)}{q({\rm z_n}|{\rm x}_n)} \equiv F(q,\theta) \]</div>
<p>with <span class="math notranslate nohighlight">\(F(q,\theta)\)</span> is called the free energy in analogy of Statistical Mechanics. The free energy is therefore a lower bound of the incomplete likelihood:</p>
<div class="math notranslate nohighlight">
\[\log \Pi_{n=1}^N P({\rm x}_n| \theta)  \geq F(q,\theta) \]</div>
<p>If we maximized the free energy we are optimizing a lower bound of the incomplete likelihood, which is essentially what we claim EM does. EM maximizes the free energy using coordinate gradient ascent, i.e. iteratively optimizing <span class="math notranslate nohighlight">\(F\)</span> in <span class="math notranslate nohighlight">\(q\)</span> and <span class="math notranslate nohighlight">\(\theta\)</span>:</p>
<ul class="simple">
<li><p>E-step: <span class="math notranslate nohighlight">\(q_{s+1} = {\rm argmax}_q F(q, \theta^{(s)})\)</span></p></li>
<li><p>M-step: <span class="math notranslate nohighlight">\(\theta_{s+1} = {\rm argmax}_\theta F(q_{t+1}, \theta)\)</span></p></li>
</ul>
<p>To finish the proof, we only need to find the solution for the E-step. A simple proof consists in first noticing that the inequality also holds for <span class="math notranslate nohighlight">\(\theta = \theta^{(s)}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\log \Pi_{n=1}^N P({\rm x}_n| \theta^{(s)})  \geq F(q,\theta^{(s)}) \]</div>
<p>Now let us take <span class="math notranslate nohighlight">\(q_{s+1} = P(z_n | x_n, \theta^{(s)})\)</span>. Plugging it in into the free energy:</p>
<div class="math notranslate nohighlight">
\[ \sum_{n=1}^N \sum_{z_n} P(z_n | x_n, \theta^{(s)}) \log \frac{P({\rm x}_n, {\rm z}_n| \theta^{(s)})}{P(z_n | x_n, \theta^{(s)})} \]</div>
<div class="math notranslate nohighlight">
\[= \sum_{n=1}^N (\sum_{z_n} P(z_n | x_n, \theta^{(s)})) \log P({\rm x}_n | \theta^{(s)}) \]</div>
<div class="math notranslate nohighlight">
\[= \sum_{n=1}^N \log P({\rm x}_n | \theta^{(s)}) = \log \Pi_{n=1}^N P({\rm x}_n| \theta^{(s)}) \]</div>
<p>which makes the inequality an equality, therefore maximizing the free energy!</p>
<p>For the M-step, we just need to sustitute <span class="math notranslate nohighlight">\(q_{s+1}\)</span> into the free energy:</p>
<div class="math notranslate nohighlight">
\[ F(q_{s+1}, \theta) = \sum_{n=1}^N \sum_{z_n} P(z_n | x_n, \theta^{(s)}) \log \frac{P({\rm x}_n, {\rm z}_n| \theta)}{P(z_n | x_n, \theta^{(s)})} \]</div>
<div class="math notranslate nohighlight">
\[  =\sum_{n=1}^N \sum_{z_n} P(z_n | x_n, \theta^{(s)}) \log P({\rm x}_n, {\rm z}_n| \theta) - \sum_{n=1}^N \sum_{z_n} P(z_n | x_n, \theta^{(s)}) \log P(z_n | x_n, \theta^{(s)}) \]</div>
<div class="math notranslate nohighlight">
\[= Q(\theta | \theta^{(s)}) + H_{q_{s+1}}\]</div>
<p>where the second term, <span class="math notranslate nohighlight">\(H_{q_{s+1}}\)</span>, the entropy of <span class="math notranslate nohighlight">\(q_{s+1}\)</span>, does not depend on <span class="math notranslate nohighlight">\(\theta\)</span>. Therefore the maximum of this free energy wrt to <span class="math notranslate nohighlight">\(\theta\)</span> is the same as the maximum of <span class="math notranslate nohighlight">\(Q\)</span>:</p>
<div class="math notranslate nohighlight">
\[\theta_{s+1} = {\rm argmax}_\theta Q(\theta|\theta^{(s)})\]</div>
<p>completing the proof.</p>
<p>The algorithm is run until some form of convergence is reached (or a maximum number of iterations). Two different criteria are used typically to evaluate convergence:</p>
<ul class="simple">
<li><p>Convergence in the incomplete log-likelihood <span class="math notranslate nohighlight">\(l(\theta^{(s)})\)</span>, which we now from the previous proof that has to increase always on each EM iteration. The only issue is that potentially might be computationally expensive to evaluate, since it requires the integration over the latent variables.</p></li>
<li><p>Convergence in parameters, meaning that two subsequent set of parameters are close enough using some distance metric, for instance Euclidean distance:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[||\theta^{(s+1)} - \theta^(s)||_2 &lt;= \text{threshold}\]</div>
<p>In the following examples we will see examples of a general rule when using EM, namely that the estimators of the parameters <span class="math notranslate nohighlight">\(\theta\)</span> obtained in the M-step are the ones that we would get by optimizing the incomplete likelihood but with the values of the latent variables replaced by their expectations under <span class="math notranslate nohighlight">\(P(z_n | x_n, \theta^{(s)})\)</span></p>
<section id="example-1-gaussian-mixture-model">
<h5><span class="section-number">5.5.4.2.1. </span>Example 1: Gaussian Mixture Model<a class="headerlink" href="#example-1-gaussian-mixture-model" title="Permalink to this heading">#</a></h5>
<p>As a first example we derive EM for the Gaussian Mixture Model (GMM). The complete log-likelihood for this model reads:</p>
<div class="math notranslate nohighlight">
\[\log \Pi_n P(\vec{x_n}, z_n|\theta) = \log \Pi_n \Pi_{k=1}^K \left(\pi_k {\mathcal N}(\vec{x_n}|\vec{\mu}_k, \Sigma_k)\right)^{\delta_{z_n,k}} =  \sum_n \sum_{k=1}^K  \delta_{z_n,k} \left( \log \pi_k + \log {\mathcal N}(\vec{x_n}|\vec{\mu}_k, \Sigma_k)\right) \]</div>
<p>The set of parameters <span class="math notranslate nohighlight">\(\theta\)</span> of the model are the segment probabilities <span class="math notranslate nohighlight">\(\pi_k\)</span> and Gaussian parameters <span class="math notranslate nohighlight">\(\vec{\mu}_k, \Sigma_k\)</span>. Given observations <span class="math notranslate nohighlight">\(\vec{x_n}\)</span> and a current estimation of parameters <span class="math notranslate nohighlight">\(\theta^{(s)}\)</span>, where <span class="math notranslate nohighlight">\(s\)</span> is the index for the current EM iteration, we infer the distribution of the latent variables (E-step):</p>
<div class="math notranslate nohighlight">
\[\gamma_{n,k}^{(s)} \equiv P(z_n=k|\vec{x}_n, \theta^{(s)}) = \frac{\pi_k^s {\mathcal N}(\vec{x_n}|\vec{\mu}_k^s, \Sigma_k^s)}{\sum_{k'=1}^K \pi_{k'}^s {\mathcal N}(\vec{x_n}|\vec{\mu}_{k'}^s, \Sigma_{k'}^s)}\]</div>
<p>where we have used Bayes theorem. Now we write the expected log-likelihood with respect to this distribution, which again for simplicity of notation we denote as <span class="math notranslate nohighlight">\(\mathbb{E}_s[...]\)</span>:</p>
<div class="math notranslate nohighlight">
\[Q(\theta | \theta^{(s)}) = \mathbb{E}_s\left[\log \Pi_n P(\vec{x_n}, z_n|\theta)\right] =  \sum_n \sum_{k=1}^K \mathbb{E}_s\left[\delta_{z_n,k}\right] \left( \log \pi_k + \log {\mathcal N}(\vec{x_n}|\vec{\mu}_k, \Sigma_k)\right)\]</div>
<div class="math notranslate nohighlight">
\[=\sum_n \sum_{k=1}^K \gamma_{n,k}^{(s)} \left( \log \pi_k + \log {\mathcal N}(\vec{x_n}|\vec{\mu}_k, \Sigma_k)\right)\]</div>
<p>Now we apply the M-step, finding the estimators for the parameters that maximize the expected log-likelihood. For the case of <span class="math notranslate nohighlight">\(\pi_k\)</span> we need to introduce the constraint <span class="math notranslate nohighlight">\(\sum_k \pi_k = 1\)</span>:</p>
<div class="math notranslate nohighlight">
\[\max_{\pi_k} Q(\theta|\theta^{(s)}) + \lambda (\sum_{k'} \pi_{k'} -1) \rightarrow \pi_k^{s+1} = \frac{1}{N} \sum_{n=1}^N \gamma_{n,k}^{(s)}\]</div>
<p>Similarly we can find the estimators for the Gaussian parameters:</p>
<div class="math notranslate nohighlight">
\[\vec{\mu}_k^{s+1} = \frac{1}{N \pi_k^{s+1}} \sum_{n=1}^N \gamma_{n,k}^{(s)} \vec{x_n}\]</div>
<div class="math notranslate nohighlight">
\[\Sigma_k^{s+1} = \frac{1}{N \pi_k^{s+1}} \sum_{n=1}^N \gamma_{n,k}^{(s)} (\vec{x_n} - \vec{\mu}_k^{s+1})(\vec{x_n} - \vec{\mu}_k^{s+1})^t\]</div>
<p>From these estimators, we can easily verify that indeed they are of the form of the estimators we would obtain for the complete log-likelihood, but with the hidden variables replaced by their expectations under the posterior <span class="math notranslate nohighlight">\(P(z_n=k|\vec{x}_n, \theta^{(s)})\)</span>. Another interesting point to remark is how the EM algorithm for the GMM is essentially a “soft” version of K-means, where instead of having hard assignments to the cluster with the nearest centroid, we have soft assignments to each of the Gaussian distributions. We left to the student the task, as an exercise, to show how EM converges to K-means in the particular case of spherical distributions <span class="math notranslate nohighlight">\(\Sigma_k = \epsilon I\)</span> in the limit <span class="math notranslate nohighlight">\(\epsilon \rightarrow 0\)</span>.</p>
<p>Let us now illustrate the algorithm in practice. We apply the <em>Good and Bad Data Model</em> (GBDM) to detect anomalies in the daily returns of BBVA stock.</p>
<p>The intuition behind GBDM is that, under normal market conditions, price returns approximately follow a Gaussian distribution, consistent with the Geometric Brownian Motion (GBM) model for stock prices first proposed by Samuelson. Yet, it is well established that financial markets can experience episodes of extreme stress in which returns deviate sharply from the predictions of GBM—for example, during the COVID-19 confinement, the Brexit referendum, or the first election of Donald Trump. The GBDM captures these empirical features by combining two components: a Gaussian distribution representing the <em>good</em> regime of normal market behavior, and a second Gaussian distribution accounting for the <em>bad</em> regime of stressed conditions. In this way, the model provides a simple but powerful generative framework for distinguishing between typical fluctuations and anomalous events in financial time series.</p>
<p>Specifically, we consider a fixed 10-year window, compute daily returns, and fit the model to this time series using the Expectation–Maximization (EM) algorithm. For initialization, we set the mean of the historical returns as the common mean of both the good and bad data components. The standard deviation of the good data distribution is initialized with the historical standard deviation of returns, while the bad data distribution is initialized with twice this value. As a prior probability, we assume that 95% of observations belong to the good regime. Importantly, the final results are quite robust to these initialization choices, which suggests that the model reliably captures the distinction between normal and anomalous returns.</p>
<p>We check that the EM algorithm behaves as expected, i.e. the incomplete log-likelihood always increases with each iteration of the algorithm, and it reaches convergence. This is shown in the following figure:</p>
<figure class="align-default" id="fig-gbdm-loglikelihood">
<a class="reference internal image-reference" href="../_images/GBDM_loglikelihood.png"><img alt="../_images/GBDM_loglikelihood.png" src="../_images/GBDM_loglikelihood.png" style="width: 8in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.7 </span><span class="caption-text">Log-likelihood of the GBDM calculated with the parameters obtained at each iteration. As expected from the theoretical formulation of the algorithm, the log-likelihood cannot decrease at any iteration of EM.</span><a class="headerlink" href="#fig-gbdm-loglikelihood" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>To classify anomalies, we use a <span class="math notranslate nohighlight">\(P(\text{bad}|D) &gt; 0.5\)</span> rule to classify data as abnormal. In the following two figures we can see the results of the model in both the histogram and time-series of daily returns of BBVA. The GBDM partitions the data in two clusters: one that captures normal financial conditions, and a second one that flags periods of stress as well as exuberance (abnormally high negative and positive returns, respectively). By inspection of the data in the accompanying notebook we can see that the model correctly classifies the aforementioned events, since it flags the 2020-03-16 (the first lock-down day in Spain), the 2016-06-24 (the first trading day after the Brexit referendum) and the 2016-11-09 (the first trading day after Trump’s election).</p>
<figure class="align-default" id="fig-gbdm-returns-histogram">
<a class="reference internal image-reference" href="../_images/GBDM_returns_histogram.png"><img alt="../_images/GBDM_returns_histogram.png" src="../_images/GBDM_returns_histogram.png" style="width: 8in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.8 </span><span class="caption-text">Histogram of daily returns of BBVA’s stock over 10 years from 2015 to 2025. The GBDM model has been used to flag anomalous returns, which are depicted in red.</span><a class="headerlink" href="#fig-gbdm-returns-histogram" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="fig-gbdm-returns-timeseries">
<a class="reference internal image-reference" href="../_images/GBDM_returns_timeseries.png"><img alt="../_images/GBDM_returns_timeseries.png" src="../_images/GBDM_returns_timeseries.png" style="width: 8in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.9 </span><span class="caption-text">Time-series of daily returns of BBVA’s stock over 10 years. Again, red points depict days that have been classified as having abnormally high and low returns.</span><a class="headerlink" href="#fig-gbdm-returns-timeseries" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="example-2-hidden-markov-model-the-baum-welch-algorithm">
<h5><span class="section-number">5.5.4.2.2. </span>Example 2: Hidden Markov Model (the Baum - Welch algorithm)<a class="headerlink" href="#example-2-hidden-markov-model-the-baum-welch-algorithm" title="Permalink to this heading">#</a></h5>
<p>Let us turn now to Hidden Markov Models (HMMs). The complete log-likelihood looks:</p>
<div class="math notranslate nohighlight">
\[\log P(y_{1}) P(x_{1}|y_{1})\Pi_{t=2}^T P(y_{t}|y_{t-1})P(x_{t} | y_{t}) = \left(\log P(y_{1}) + \log P(x_{1}|y_{1}) + \sum_{t=2}  \log \left(P(y_{t}|y_{t-1}) + \log P(x_{t} | y_{t})\right) \right)\]</div>
<p>where <span class="math notranslate nohighlight">\(t\)</span> is the index for the time series observations. The parameters of the model <span class="math notranslate nohighlight">\(\theta\)</span> are the ones defining the transition and observation probabilities. These are time-independent since the distributions are stationary. Given an estimation of the parameters <span class="math notranslate nohighlight">\(\theta^{(s)}\)</span> (we use <span class="math notranslate nohighlight">\(s\)</span> for the EM iterations in this case), we can infer the latent variables of the model given the observations as:</p>
<div class="math notranslate nohighlight">
\[P(y_{1}, ..., y_{T}| x_{1}, ..., x_{T}, \theta^{(s)})\]</div>
<p>Let us consider discrete variables and introduce some convenient notation:</p>
<div class="math notranslate nohighlight">
\[a_{i,j} = P(y_{t} = Y_j|y_{t-1} = Y_i)\]</div>
<div class="math notranslate nohighlight">
\[\pi_i = P(y_1 = Y_i)\]</div>
<div class="math notranslate nohighlight">
\[b_{k,i} = P(x_t = X_k|y_t = Y_i)\]</div>
<p>Using indicator functions:</p>
<div class="math notranslate nohighlight">
\[P(y_{1}) = \sum_i \pi_i^{\delta_{y_{1}, Y_i}}\]</div>
<div class="math notranslate nohighlight">
\[P(y_{t}|y_{t-1}) = \sum_{i,j} a_{i,j}^{\delta_{y_{t}, Y_i} \delta_{y_{t-1}, Y_j}}\]</div>
<div class="math notranslate nohighlight">
\[P(x_{t} | y_{t}) = \sum_{k,i} b_{k,i}^{\delta_{x_{t}, X_k}\delta_{y_{t}, Y_i}}\]</div>
<p>The complete log-likelihood can be then written as:</p>
<div class="math notranslate nohighlight">
\[\sum_i \delta_{y_{1}, Y_i} \log \pi_i + \sum_{k,i} \delta_{x_{1}, X_k}\delta_{y_{1}, Y_i} \log b_{k,i} + \sum_{t=2} \left(\sum_{i,j} \delta_{y_{t}, Y_i} \delta_{y_{t-1}, Y_i} \log a_{i,j} + \sum_{k,i} \delta_{x_{t}, X_k}\delta_{y_{t}, Y_i} \log b_{k,i}\right)\]</div>
<p>We calculate the expected log-likelihood:</p>
<div class="math notranslate nohighlight">
\[Q(\theta|\theta^{(s)}) = \sum_i \mathbb{E}_s[\delta_{y_{1}, Y_i}]\log \pi_i + \sum_{k,i} \delta_{x_{1}, X_k}\mathbb{E}_s[\delta_{y_{1}, Y_i}] \log b_{k,i} + \sum_{t=2} \left(\sum_{i,j} \mathbb{E}_s[\delta_{y_{t}, Y_i} \delta_{y_{t-1}, Y_j}] \log a_{i,j} + \sum_{k,i} \delta_{x_{t}, X_k} \mathbb{E}_s[\delta_{y_{t}, Y_i}]\log b_{k,i}\right)\]</div>
<p>where <span class="math notranslate nohighlight">\(E_s\)</span> is the expectation with respect to the distribution of latent variables at iteration <span class="math notranslate nohighlight">\(s\)</span>, shown above. We define:</p>
<div class="math notranslate nohighlight">
\[\gamma_{t}^{s,i} \equiv  E_s[\delta_{y_{t}, Y_i}] = P(y_{t} = Y_i| x_{1}, ..., x_{T},\theta^{(s)})\]</div>
<div class="math notranslate nohighlight">
\[\chi_{t}^{s,i,j} \equiv E_s[\delta_{y_{t}, Y_i} \delta_{y_{t-1}, Y_j}] = P(y_{t}= Y_i, y_{t-1}= Y_j| x_{1}, ..., x_{T},\theta^{(s)})\]</div>
<p>Substituting it into the expected log-likelihood:</p>
<div class="math notranslate nohighlight">
\[Q(\theta|\theta^{(s)}) = \sum_n \left(\sum_i \gamma_{1,n}^{s,i} \log \pi_i + \sum_{k,j} \delta_{x_{1,n}, X_k}\gamma_{t,n}^{s,i} \log b_{k,i} + \sum_{t=2} \left(\sum_{i,j} \chi_{t,n}^{s,i,j}  \log a_{i,j} + \sum_{k,i} \delta_{x_{t,n}, X_k}\gamma_{t,n}^{s,i}\log b_{k,i}\right)\right)\]</div>
<p>Now we can maximize the function with respect to parameters <span class="math notranslate nohighlight">\(\pi_i, a_{i,j}, b_{k,i}\)</span> to find the estimators at iteration <span class="math notranslate nohighlight">\(s+1\)</span>:</p>
<div class="math notranslate nohighlight">
\[\pi^{s+1}_i =  \gamma_{1}^{s,i} \]</div>
<div class="math notranslate nohighlight">
\[a_{i,j}^{s+1} = \frac{\sum_{t=1}^{T-1}\chi_{t}^{s,i,j}}{\sum_{t=1}^{T-1}\gamma_{t}^{s,i}}\]</div>
<div class="math notranslate nohighlight">
\[b_{i,k}^{s+1} = \frac{\sum_{t=1}^T \delta_{x_{t}, X_k} \gamma_{t}^{s,i}}{\sum_{t=1}^{T-1}\gamma_{t}^{s,i}}\]</div>
<p>It remains to show how to calculate at each iteration the expectations from the E-step. This can be done by introducing a set of two helper probabilities. The forward probability:</p>
<div class="math notranslate nohighlight">
\[\alpha_{i}^s(t) \equiv P(x_{1} = X_{k_1}, ..., x_{t} = X_{k_t}, y_{t} = Y_i| \theta^{(s)})\]</div>
<p>which can be calculated recursively:</p>
<div class="math notranslate nohighlight">
\[\alpha_{i}^s(1) = P(x_{1} = X_{k_1}, y_{1} = Y_i| \theta^{(s)})\]</div>
<div class="math notranslate nohighlight">
\[= P(x_{1} = X_{k_1}| y_{1}=Y_i,  \theta^{(s)}) P (y_{1} = Y_i| \theta^{(s)}) = b_{k_1,i}^s \pi_i^s\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\alpha_{i}^s(t+1) = P(x_{1} = X_{k_1}, ..., x_{t+1} = X_{k_{t+1}}, y_{t+1} = Y_i| \theta^{(s)}) \]</div>
<div class="math notranslate nohighlight">
\[= \sum_{i'} P(x_{1} = X_{k_1}, ..., x_{t+1} = X_{k_{t+1}}, y_{t} = Y_{i'}, y_{t+1} = Y_i| \theta^{(s)})\]</div>
<div class="math notranslate nohighlight">
\[ = \sum_{i'} P(x_{t+1} = X_{k_{t+1}}, y_{t+1} = Y_i| x_{1} = X_{k_1}, ..., x_{t} = X_{k_{t}}, y_{t} = Y_{i'}, \theta^{(s)}) P(x_{1} = X_{k_1}, ..., x_{t} = X_{k_{t}}, y_{t} = Y_{i'}| \theta^{(s)}) \]</div>
<div class="math notranslate nohighlight">
\[= \sum_{i'} P(x_{t+1} = X_{k_{t+1}}, y_{t+1} = Y_i| y_{t} = Y_{i'}, \theta^{(s)}) \alpha_{i'}^s(t) \]</div>
<div class="math notranslate nohighlight">
\[= \sum_{i'} P(x_{t+1} = X_{k_{t+1}}| y_{t+1} = Y_i, \theta^{(s)})P(y_{t+1} = Y_i | y_{t} = Y_{i'}, \theta^{(s)}) \alpha_{i'}^s(t) \]</div>
<div class="math notranslate nohighlight">
\[= b_{k_{t+1},i}^s \sum_{i'} a_{i,i'}^s \alpha_{i',n}^s(t)\]</div>
<p>And the backward probability:</p>
<div class="math notranslate nohighlight">
\[\beta_{i}^s(t) \equiv P(x_{t+1} = X_{k_{t+1}}, ..., x_{T} = X_{k_T}| y_{t} = Y_i, \theta^{(s)})\]</div>
<p>As an exercise, we let the student to derive the recursive formulae for its calculation:</p>
<div class="math notranslate nohighlight">
\[\beta_{i}^s(T) = 1\]</div>
<div class="math notranslate nohighlight">
\[\beta_{i}^s(t) = \sum_{i'} \beta_{i'}^s(t+1) a_{i',i}^s b_{k_{t+1},i'}^s\]</div>
<p>We can now write the probabilities of the latent variables as a function of these helper probabilities using Bayes’ theorem:</p>
<div class="math notranslate nohighlight">
\[\gamma_{t}^{s,i}  = P(y_{t} = Y_i| x_{1}, ..., x_{T},\theta^{(s)}) = \frac{P(y_{t} = Y_i, x_{1}, ..., x_{T}|\theta^{(s)})}{P(y_{t} = Y_i|\theta^{(s)})} \]</div>
<div class="math notranslate nohighlight">
\[= \frac{P(x_{t+1}, ..., x_{T}|y_{t} = Y_i,x_{1}, ..., x_{t}, \theta^{(s)}) P(y_{t} = Y_i,x_{1}, ..., x_{t}| \theta^{(s)})}{\sum_{i'}P(y_{t} = Y_{i'}, x_{1}, ..., x_{T}|\theta^{(s)})} \]</div>
<div class="math notranslate nohighlight">
\[= \frac{\alpha_{i}^s(t) \beta_{i}^s(t)}{\sum_{i'}\alpha_{i'}^s(t) \beta_{i'}^s(t)}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\chi_{t}^{s,i,j} = P(y_{t}= Y_i, y_{t-1}= Y_j| x_{1}, ..., x_{T},\theta^{(s)}) = \frac{P(y_{t}= Y_i, y_{t-1}= Y_j, x_{1}, ..., x_{T}|\theta^{(s)})}{P(x_{1}, ..., x_{T}|\theta^{(s)})} \]</div>
<div class="math notranslate nohighlight">
\[= \frac{P(x_{t}, ..., x_{T}| y_{t}= Y_i,  y_{t-1}= Y_j, x_{1}, ..., x_{t-1},\theta^{(s)}) P(y_{t}= Y_i, y_{t-1}= Y_j, x_{1}, ..., x_{t-1}|\theta^{(s)}}{P(x_{1}, ..., x_{T}|\theta^{(s)})} \]</div>
<div class="math notranslate nohighlight">
\[= \frac{P(x_{t}, ..., x_{T}| y_{t}= Y_i,\theta^{(s)}) P(y_{t}= Y_i| y_{t-1}= Y_j, x_{1}, ..., x_{t-1},\theta^{(s)})P(y_{t-1}= Y_j, x_{1}, ..., x_{t-1},\theta^{(s)})}{P(x_{1}, ..., x_{T}|\theta^{(s)}) } \]</div>
<div class="math notranslate nohighlight">
\[=\frac{P(x_{t}|y_{t}= Y_i,\theta^{(s)}) P(x_{t+1}, ..., x_{T}, y_{t}= Y_i,\theta^{(s)}) a_{i,j}^s \alpha_{j}^s(t-1)}{P(x_{1}, ..., x_{T}|\theta^{(s)}) } \]</div>
<div class="math notranslate nohighlight">
\[= \frac{b_{k_t, i}^s \beta_{i}^s(t) a_{i,j}^s \alpha_{j}^s(t-1)}{\sum_{i',j'} b_{k_t, i'}^s \beta_{i'}^s(t) a_{i',j'}^s \alpha_{j'}^s(t-1)}\]</div>
<p>This completes all the ingredients need to perform EM in the Baum-Welch algorithm. Given the set of parameters <span class="math notranslate nohighlight">\(\theta^{(s)}\)</span> we use the forward and backward probabilities to update the estimations of the latent variables (E-step) which then are plugged in the estimators of <span class="math notranslate nohighlight">\(\theta^{(s+1)}\)</span> (M-step) and we iterate until convergence. The only thing needed are initial estimates of the parameters, <span class="math notranslate nohighlight">\(\theta^{(0)}\)</span>, which could be random numbers or based on prior knowledge. A good choice of initialization can help to increase the speed of convergence.</p>
<section id="gaussian-observation-probabilities">
<h6><span class="section-number">5.5.4.2.2.1. </span>Gaussian observation probabilities<a class="headerlink" href="#gaussian-observation-probabilities" title="Permalink to this heading">#</a></h6>
<p>So far we have considered only discrete transition and observation probabilities. We can consider different probability models within the HMM framework and Baum - Welch EM learning. If the observations are continuous variables, we can model their conditional distributions as Gaussian:</p>
<div class="math notranslate nohighlight">
\[b_{i}(x_t) = P(x_t|y_t = Y_i) = {\mathcal N}(x|\mu_i, \sigma_i)\]</div>
<p>The extension to multi-variate Gaussian should be easy to derive. The expected log-likelihood now reads:</p>
<div class="math notranslate nohighlight">
\[Q(\theta|\theta^{(s)}) = \left(\sum_i \gamma_{1}^{s,i} \log \pi_i -\frac{1}{2} \sum_{j} \gamma_{t}^{s,i} (\frac{(x_{1} - \mu_i)^2}{\sigma_i^2} + \log( 2\pi \sigma_i^2)) + \sum_{t=2} \left(\sum_{i,j} \chi_{t}^{s,i,j}  \log a_{i,j} -\frac{1}{2} \sum_{i} \gamma_{t}^{s,i}(\frac{(x_{t} - \mu_i)^2}{\sigma_i^2} + \log( 2\pi \sigma_i^2)) \right)\right)\]</div>
<p>We leave as an exercise to work the expressions for the EM estimators of the parameters of the Gaussian (the rest of estimators don’t change):</p>
<div class="math notranslate nohighlight">
\[\mu_i^{(s+1)} = \frac{ \sum_{t=1}^T \gamma_{t}^{s,i} x_{t}}{\sum_{t=1}^{T}\gamma_{t}^{s,i}}\]</div>
<div class="math notranslate nohighlight">
\[(\sigma_i^{(s+1)})^2 = \frac{\sum_{t=1}^T \gamma_{t}^{s,i} (x_{t} - \mu_i^{(s+1)})^2}{\sum_{t=1}^{T}\gamma_{t}^{s,i}}\]</div>
<p>which are similar to the GMM ones –in fact HMM models can be interpreted as time-series Gaussian Mixture Models.</p>
</section>
</section>
<section id="example-3-local-level-model-simple-case-of-a-kalman-filter">
<h5><span class="section-number">5.5.4.2.3. </span>Example 3: Local Level Model (simple case of a Kalman Filter)<a class="headerlink" href="#example-3-local-level-model-simple-case-of-a-kalman-filter" title="Permalink to this heading">#</a></h5>
<p>Recall that the local level model equations are given by:</p>
<div class="math notranslate nohighlight">
\[y_{t+1} = y_t + w_t, w_t \sim {\mathcal N}(0, \sigma_w^2)\]</div>
<div class="math notranslate nohighlight">
\[x_t = y_t + v_t, v_t \sim {\mathcal N}(0, \sigma_v^2)\]</div>
<p>Assume we have observations <span class="math notranslate nohighlight">\(x_{0:T}\)</span>. The complete log-likelihood is similar to the one for the HMM, only now we exploit the properties of the Gaussian distributions:</p>
<div class="math notranslate nohighlight">
\[l= \sum_{t=1}^T \log P(x_t|y_t)P(y_t|y_{t-1}) = -\sum_{t=1}^T \left(\frac{1}{2} \log 2\pi \sigma_v^2 + \frac{(x_t-y_t)^2 }{2\sigma_v^2}+ \frac{1}{2} \log 2\pi \sigma_w^2 + \frac{(y_{t+1}-y_t)^2}{2\sigma_w^2} \right)\]</div>
<p>As usual with EM we derive the iterative equations using induction: we assume we have already a set of parameters estimated at iteration <span class="math notranslate nohighlight">\(s\)</span>, <span class="math notranslate nohighlight">\(\theta^{(s)} =\{\sigma_v^{(s)}, \sigma_w^{(s)}\}\)</span> and derive the updates for iteration <span class="math notranslate nohighlight">\(s+1\)</span>. To initialize the algorithm we need a parameter seed that can be based on general heuristics or prior information. We start with the E-step, where we compute the expectation of the log-likelihood using the best inference available on the latent variables at the current iteration, <span class="math notranslate nohighlight">\(P(y_t|x_{0:T}, \theta^{(s)})\)</span>:</p>
<div class="math notranslate nohighlight">
\[Q(\theta|\theta^{(s)}) = -\sum_{t=1}^T \left(\frac{1}{2} \log 2\pi \sigma_v^2 + \frac{\mathbb{E}_s[(x_t-y_t)^2] }{2\sigma_v^2}+ \frac{1}{2} \log 2\pi \sigma_w^2 + \frac{\mathbb{E}_s[(y_{t+1}-y_t)^2]}{2\sigma_w^2} \right)\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbb{E}_s[...] \equiv \mathbb{E}[... |x_{0:T}, \theta^{(s)}]\)</span>. Let us compute the first of the expectations:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}_s[(x_t-y_t)^2] = \mathbb{E}_s[x_t^2 + y_t^2 - 2x_t y_t] = x_t^2 + \mathbb{E}_s[y_t^2] - 2 x_t \mathbb{E}_s[y_t] = x_t^2 + (\hat{y}_{t|T}^{(s)})^2 + (\sigma_{t|T}^{(s)})^2 - 2 x_t \hat{y}_{t|T}^{(s)} \]</div>
<p>where we have used the smoothing equations introduced previously, only their iterative computation is done using parameters <span class="math notranslate nohighlight">\(\theta^{(s)}\)</span>. The second one can be computed as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbb{E}_s[(y_{t+1}-y_t)^2] = \mathbb{E}_s[y_{t+1}^2 + y_t^2 - 2y_{t+1} y_t] = \mathbb{E}_s[y_{t+1}^2] + \mathbb{E}_s[y_t^2] - 2 \mathbb{E}_s[y_{t+1} y_t] \\= (\hat{y}_{t+1|T}^{(s)})^2 + (\sigma_{t+1|T}^{(s)})^2  + (\hat{y}_{t|T}^{(s)})^2 + (\sigma_{t|T}^{(s)})^2 - 2 \sigma^{(s)}_{t+1,t|T}  -  2 \hat{y}_{t+1|T}^{(s)} \hat{y}_{t|T}^{(s)} \\ = (\hat{y}_{t+1|T}^{(s)} - \hat{y}_{t|T}^{(s)})^2 +  (\sigma_{t+1|T}^{(s)})^2 + (\sigma_{t|T}^{(s)})^2 - 2 \sigma^{(s)}_{t+1,t|T} \end{split}\]</div>
<p>where we have introduced the smoothed estimation of the one-lag covariance <span class="math notranslate nohighlight">\(\sigma_{t+1,t|T}^{(s)} \equiv \mathbb{E}_s[y_{t+1} y_t] - \hat{y}_{t+1|T}^{(s)} \hat{y}_{t|T}^{(s)}\)</span>. A derivation of the recursive equation to compute this covariance can be checked in <span id="id9">[<a class="reference internal" href="references.html#id19" title="Kevin P. Murphy. Machine learning : a probabilistic perspective. MIT Press, Cambridge, Mass. [u.a.], 2013. ISBN 9780262018029 0262018020.">3</a>]</span>:</p>
<div class="math notranslate nohighlight">
\[\sigma_{t+1,t|T}^{(s)} =  J_t (\sigma_{t+1|T}^{(s)})^2 \]</div>
<p>where we have introduced the smoother gain <span class="math notranslate nohighlight">\(J_t\)</span>:</p>
<div class="math notranslate nohighlight">
\[J_t \equiv \frac{ (\sigma_{t|t}^{(s)})^2}{ (\sigma_{t+1|t}^{(s)})^2}\]</div>
<p>Let us move now into the M-step, where we obtain new estimators for the parameters of the model by maximizing the expected complete likelihood. We compute the extremes and leave the interested reader the evaluation of the second derivatives to verify that they are indeed maxima:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial \sigma_v^2} Q(\theta|\theta^{(s)}) = \sum_{t=1}^T \left(\frac{1}{2\sigma_v^2} -\frac{\mathbb{E}_s[(x_t-y_t)^2] }{2\sigma_v^4}\right) =0 \rightarrow (\sigma_v^{(s+1)})^2 =  \frac{1}{T}\sum_{t=1}^T \mathbb{E}_s[(x_t-y_t)^2] \]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial \sigma_w^2} Q(\theta|\theta^{(s)}) = \sum_{t=1}^{T-1} \left(\frac{1}{2\sigma_w^2} -\frac{\mathbb{E}_s[(y_{t+1}-y_t)^2] }{2\sigma_w^4}\right) =0 \rightarrow (\sigma_w^{(s+1)})^2 = \frac{1}{T-1} \sum_{t=1}^{T-1} \mathbb{E}_s[(y_{t+1}-y_t)^2] \]</div>
<p>The new estimators depend on the expectations calculated in the E-step, which themselves are calculated doing a forward and smoothing passes over the data.</p>
<p>This completes the derivation of the EM algorithm for the local level model, since now starting from a seed for the parameters, we can iteratively refine our estimators until we reach a convergence in terms of parameters or log-likelihood.</p>
<p>To see the algorithm in action we simulate a local level model over 200 time-steps, with <span class="math notranslate nohighlight">\(\sigma_v^2 = 1.0\)</span> and <span class="math notranslate nohighlight">\(\sigma_w^2 = 0.05\)</span>. Then we use the simulated data to find the parameters using Expectation Maximization. The algorithm reach quickly convergence, as seen in the figure below. As a sanity check of the implementation, we see that the log-likelihood over EM iterations never decreases.</p>
<figure class="align-default" id="fig-local-level-em-convergence">
<a class="reference internal image-reference" href="../_images/local_level_em_convergence.png"><img alt="../_images/local_level_em_convergence.png" src="../_images/local_level_em_convergence.png" style="width: 8in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.10 </span><span class="caption-text">Log-likelihood of the local level model evaluated with the estimation of the parameters using EM for each iteration. We see that log-likelihood never decreases, which is a sanity check for the correctneess of the implementation.```</span><a class="headerlink" href="#fig-local-level-em-convergence" title="Permalink to this image">#</a></p>
<div class="legend">
<p>The converged parameters for this simulation are <span class="math notranslate nohighlight">\(\hat{\sigma}_v^2 = 0.872\)</span> and <span class="math notranslate nohighlight">\(\hat{\sigma}_w^2 = 0.054\)</span>, which are reasonable approximations although not exact, since EM does not guarantee to reach the maximum of the likelihood. Finally, we run the smoother using the converged parameters and compare it with the observations and the true latent state variable. The smoother provides a good approximation for the true latent state, which remains within a two sigma band for the full simulation.</p>
<figure class="align-default" id="fig-gbdm-returns-timelocal-level-em-smoothingseries">
<a class="reference internal image-reference" href="../_images/local_level_em_smoothing.png"><img alt="../_images/local_level_em_smoothing.png" src="../_images/local_level_em_smoothing.png" style="width: 8in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.11 </span><span class="caption-text">Simulated latent state variable and observations for the local level model with <span class="math notranslate nohighlight">\(\sigma_v^2 = 1.0\)</span> and <span class="math notranslate nohighlight">\(\sigma_w^2 = 0.05\)</span>. The green line shows the inferred latent state using the smoothing algorithm, with a two sigma confidence interval shaded.```</span><a class="headerlink" href="#fig-gbdm-returns-timelocal-level-em-smoothingseries" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</div>
</figcaption>
</figure>
</section>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./markdown"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="algorithmic_trading.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">4. </span>Algorithmic Trading</p>
      </div>
    </a>
    <a class="right-next"
       href="intro_causal.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">6. </span>Causal inference</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-probability">5.1. Bayesian probability</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-as-an-extension-of-logic">5.1.1. Probability as an extension of logic</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#assigning-prior-probabilities">5.1.2. Assigning prior probabilities</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-inference-and-hypothesis-testing">5.1.3. Bayesian inference and hypothesis testing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-decision-theory">5.1.4. Bayesian decision theory</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-estimating-the-probability-of-heads-in-a-coin-toss-experiment">5.1.5. Example: estimating the probability of heads in a coin toss experiment</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-machine-learning">5.2. Bayesian Machine Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-learning">5.2.1. Bayesian learning</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-online-learning">5.2.1.1. Bayesian online learning</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-prediction">5.2.2. Bayesian prediction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-coin-toss-experiment">5.2.3. Example: coin toss experiment</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-linear-regression">5.3. Bayesian Linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probabilistic-graphical-models">5.4. Probabilistic Graphical Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#latent-variable-models">5.5. Latent variable models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#partially-observable-latent-variable-models">5.5.1. Partially observable latent variable models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#full-latent-variable-models">5.5.2. Full latent variable models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-latent-variable-models">5.5.3. Examples of Latent Variable Models</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-mixture-models-gmms">5.5.3.1. Gaussian Mixture Models (GMMs)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hidden-markov-model-hmm">5.5.3.2. Hidden Markov Model (HMM)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-kalman-filter">5.5.3.3. The Kalman Filter</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#the-local-level-model">5.5.3.3.1. The local level model</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation-of-latent-variable-models">5.5.4. Estimation of Latent Variable Models</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation-mle">5.5.4.1. Maximum Likelihood Estimation (MLE)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#expectation-maximization-em">5.5.4.2. Expectation Maximization (EM)</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#example-1-gaussian-mixture-model">5.5.4.2.1. Example 1: Gaussian Mixture Model</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#example-2-hidden-markov-model-the-baum-welch-algorithm">5.5.4.2.2. Example 2: Hidden Markov Model (the Baum - Welch algorithm)</a><ul class="nav section-nav flex-column">
<li class="toc-h6 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-observation-probabilities">5.5.4.2.2.1. Gaussian observation probabilities</a></li>
</ul>
</li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#example-3-local-level-model-simple-case-of-a-kalman-filter">5.5.4.2.3. Example 3: Local Level Model (simple case of a Kalman Filter)</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Javier Sabio González
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>