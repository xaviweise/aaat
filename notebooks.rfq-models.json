{"version":3,"kind":"Notebook","sha256":"29314167671b09061a7aa4c57a5499e1d403809a2d4526f8374424c56b9d1c19","slug":"notebooks.rfq-models","location":"/notebooks/rfq_models.ipynb","dependencies":[],"frontmatter":{"title":"Modelling RfQs in Dealer to Client Markets","content_includes_title":false,"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"authors":[{"nameParsed":{"literal":"Javier Sabio González","given":"Javier Sabio","family":"González"},"name":"Javier Sabio González","id":"contributors-myst-generated-uid-0"}],"github":"https://github.com/xaviweise/aaat","numbering":{"title":{"offset":1}},"source_url":"https://github.com/xaviweise/aaat/blob/main/notebooks/rfq_models.ipynb","edit_url":"https://github.com/xaviweise/aaat/edit/main/notebooks/rfq_models.ipynb","exports":[{"format":"ipynb","filename":"rfq_models.ipynb","url":"/build/rfq_models-1553f5fb9f7e2fe77b9be34fc8e3cfc8.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Generative models for the request for quote activity","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"IcHZ7uioXH"}],"identifier":"generative-models-for-the-request-for-quote-activity","label":"Generative models for the request for quote activity","html_id":"generative-models-for-the-request-for-quote-activity","implicit":true,"key":"pjr3E3JQnT"},{"type":"heading","depth":3,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Simulation of RfQs arrival and client attrition","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"iVXLR54DDL"}],"identifier":"simulation-of-rfqs-arrival-and-client-attrition","label":"Simulation of RfQs arrival and client attrition","html_id":"simulation-of-rfqs-arrival-and-client-attrition","implicit":true,"key":"BkLBLAIdIu"}],"key":"fUKQTaFIYS"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import poisson, norm\nfrom collections import deque\n\ndef run_simulation(N_clients=50,\n                   T=200,\n                   reservation_price_mean=100,\n                   reservation_price_std_pct=0.1,\n                   lambda_mean=1,\n                   lambda_std=0.05,\n                   prior_mean=60,\n                   prior_std=10,\n                   res_price_noise_std_pct=0.2,\n                   hit_rate_target=0.4,\n                   window_size=10,\n                   attrition_threshold=0.1):\n\n    \"\"\"\n    Simulate RFQ interactions with Bayesian quantile pricing and client attrition.\n    Clients stop trading if their moving-average hit rate over `window_size` days falls below `attrition_threshold`.\n\n    Returns:\n        rfq_df: DataFrame of RfQ events\n        active_history: list of active client counts per day\n        activity_matrix: binary DataFrame of daily activity (clients x days)\n    \"\"\"\n    np.random.seed(42)\n    # Generate distribution of reservation prices for the segment of clients\n    reservation_prices = np.random.normal(reservation_price_mean,\n                                  reservation_price_mean * reservation_price_std_pct,\n                                  N_clients)\n    # Reservation prices are noisy given potential changing market conditions\n    res_price_noise_std = res_price_noise_std_pct * reservation_price_mean\n    res_price_noise_var = res_price_noise_std**2\n    # Generate distribution of RfQ intensities for the segment of clients\n    lambdas = np.abs(np.random.normal(lambda_mean, lambda_std, N_clients))\n    clients_posterior = {i: {'mean': prior_mean, 'var': prior_std**2,\n                             'n': 0, 'sum_obs': 0.0}\n                         for i in range(N_clients)}\n    z = norm.ppf(1 - hit_rate_target)\n    active = np.ones(N_clients, dtype=bool)\n    # Track per-client active status at end of each day\n    active_flags = np.zeros((T, N_clients), dtype=bool)\n    daily_history = [deque(maxlen=window_size) for _ in range(N_clients)]\n    active_history = []\n    records = []\n    Y = np.zeros((T, N_clients), dtype=int)\n    for t in range(T):\n        daily_hits = np.zeros(N_clients, dtype=int)\n        daily_reqs = np.zeros(N_clients, dtype=int)\n        for i in range(N_clients):\n            if not active[i]: continue\n            n_rfq = poisson.rvs(lambdas[i])\n            if n_rfq > 0: Y[t,i] = 1\n            for _ in range(n_rfq):\n                post = clients_posterior[i]\n                price = max(0.0, post['mean'] + np.sqrt(post['var'] + res_price_noise_var) * z)\n                r = norm.rvs(reservation_prices[i], res_price_noise_std)\n                # Trading happens when price offered is lower than the reservation price\n                hit = price <= r\n                daily_reqs[i] += 1; daily_hits[i] += int(hit)\n                # The dealer updates the estimation of the reservation price of the client\n                post['n'] += 1; post['sum_obs'] += r\n                post_var = 1/(1/prior_std**2 + post['n']/res_price_noise_var)\n                post['var'] = post_var\n                post['mean'] = post_var*(prior_mean/prior_std**2 + post['sum_obs']/res_price_noise_var)\n                records.append({'time': t, 'client_id': i, 'price': price, 'hit': hit})\n        for i in range(N_clients):\n            if not active[i] or daily_reqs[i]==0: continue\n            rate = daily_hits[i]/daily_reqs[i]\n            daily_history[i].append(rate)\n            # A client stops sending RfQs to the dealer if the hit & miss is too low\n            if len(daily_history[i])==window_size and np.mean(daily_history[i])<attrition_threshold:\n                active[i] = False\n        active_history.append(active.sum())\n        active_flags[t, :] = active.copy()\n    rfq_df = pd.DataFrame(records)\n    activity = pd.DataFrame(Y, columns=[f'client_{i}' for i in range(N_clients)])\n    active_df = pd.DataFrame(active_flags, columns=[f'client_{i}' for i in range(N_clients)])\n    return rfq_df, active_history, activity, active_df\n","key":"Bxlbww6Xgk"},{"type":"outputs","id":"2Qn9dEKzJa6vAYCRLZXQb","children":[],"key":"J29EpV991b"}],"key":"B2Ezv6W6l0"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from scipy.special import betaln\nfrom scipy.optimize import minimize\n\n# ----------------------\n# Segment-level Model\n# ----------------------\ndef estimate_segment_params(activity_matrix):\n    def log_marginal_likelihood(D, alpha, beta, gamma, delta):\n        n, x = len(D), D.sum()\n        last = np.where(D==1)[0]\n        r = n - (last[-1]+1) if last.size>0 else n\n        logA = (betaln(alpha+x, beta+n-x)-betaln(alpha,beta)\n                +betaln(gamma, delta+n)-betaln(gamma,delta))\n        logB = [(betaln(alpha+x, beta+n-x-i)-betaln(alpha,beta)\n                 +betaln(gamma+1, delta+n-i)-betaln(gamma,delta))\n                for i in range(1, r+1)]\n        mags = [logA] + logB; m_max = max(mags)\n        return m_max + np.log(sum(np.exp(m - m_max) for m in mags))\n    def neg_ll(params):\n        alpha, beta, gamma, delta = np.exp(params)\n        return -sum(log_marginal_likelihood(activity_matrix.iloc[:end, j].values,\n                                             alpha, beta, gamma, delta)\n                    for j in range(activity_matrix.shape[1])\n                    for end in [activity_matrix.iloc[:,:].values.shape[0]])\n    res = minimize(neg_ll, np.log([1,1,1,1]), method='L-BFGS-B', bounds=[(-5,5)]*4)\n    return np.exp(res.x)\n\ndef attrition_probability(D, alpha, beta, gamma, delta):\n    n, x = len(D), D.sum()\n    last = np.where(D==1)[0]\n    r = n - (last[-1]+1) if last.size>0 else n\n    logA = (betaln(alpha+x, beta+n-x)-betaln(alpha,beta)\n            +betaln(gamma, delta+n)-betaln(gamma,delta))\n    logB = [(betaln(alpha+x, beta+n-x-i)-betaln(alpha,beta)\n             +betaln(gamma+1, delta+n-i)-betaln(gamma,delta))\n            for i in range(1, r+1)]\n    mags = [logA] + logB; m_max = max(mags)\n    logL = m_max + np.log(sum(np.exp(m - m_max) for m in mags))\n    P_active = np.exp(logA - logL)\n    return 1 - P_active\n","key":"WFk75RYHmr"},{"type":"outputs","id":"Vtz9qlb7UCduv37R1GxZT","children":[],"key":"D8cMGHpvMW"}],"key":"kF6cXl6flV"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"\nfrom sklearn.metrics import (\n    confusion_matrix, accuracy_score,\n    precision_score, recall_score,\n    roc_curve, auc\n)\n\n# ----------------------\n# Workflow: Simulate, Train, Test\n# ----------------------\nT_train, T_test = 100, 100\nT_tot = T_train + T_test\nN_clients = 50\nrfq_all, active_all, Y_all, active_df = run_simulation(N_clients = N_clients, T=T_train+T_test)\n\nY_train = Y_all.iloc[:T_train].reset_index(drop=True)\nY_test  = Y_all.iloc[T_train:].reset_index(drop=True)\nrfq_test = rfq_all[rfq_all['time']>=T_train].copy()\nrfq_test['time'] -= T_train\nactive_test = active_all[T_train:]\n\nalpha, beta, gamma, delta = estimate_segment_params(Y_train)\nprint(f\"Params: α={alpha:.2f}, β={beta:.2f}, γ={gamma:.2f}, δ={delta:.2f}\")\n\n# ----------------------\n# Risk Scoring\n# ----------------------\nrisk_records = []\nfor j in range(Y_test.shape[1]):\n    hist = []\n    for t in range(T_test):\n        hist.append(Y_test.iloc[t, j])\n        D = np.array(hist, dtype=int)\n        p_inact = attrition_probability(D, alpha, beta, gamma, delta)\n        risk_records.append({\n            'time': t,\n            'client_id': j,\n            'p_inactive': p_inact,\n            'alert': p_inact > 0.5  # thresholded prediction\n        })\n\nrisk_df = pd.DataFrame(risk_records)\n\n# Label inactivity\nrisk_df['inactive'] = risk_df.apply(\n    lambda r: not active_df.loc[r['time'] + T_train, f'client_{r[\"client_id\"]}'],\n    axis=1\n)\n\n# ----------------------\n# Evaluation Metrics\n# ----------------------\n\n# Ground truth and predictions\ny_true   = risk_df['inactive']\ny_pred   = risk_df['alert']           # at 0.5 threshold\ny_scores = risk_df['p_inactive']      # raw probabilities\n\n# Confusion matrix\ntn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\nprint(\"Confusion Matrix:\")\nprint(f\"  TN={tn}, FP={fp}, FN={fn}, TP={tp}\")\n\n# Accuracy, Precision, Recall at 0.5\naccuracy  = accuracy_score(y_true, y_pred)\nprecision = precision_score(y_true, y_pred)\nrecall    = recall_score(y_true, y_pred)\nprint(f\"Accuracy : {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall   : {recall:.4f}\")\n\n# ROC curve & AUC\nfpr, tpr, thresholds = roc_curve(y_true, y_scores)\nroc_auc = auc(fpr, tpr)\nprint(f\"AUC      : {roc_auc:.4f}\")\n\n# Plot ROC\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], 'k--', label='Random')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc='lower right')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n","key":"zZloigzCHp"},{"type":"outputs","id":"b4R-k_5BILy8kPVmEREQW","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"Params: α=148.41, β=85.31, γ=0.08, δ=148.41\nConfusion Matrix:\n  TN=4365, FP=0, FN=76, TP=559\nAccuracy : 0.9848\nPrecision: 1.0000\nRecall   : 0.8803\nAUC      : 0.9884\n"},"children":[],"key":"TS6LtCdEaS"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"1b8ca7b96271a3b9fe709f5b5753202a","path":"/build/1b8ca7b96271a3b9fe709f5b5753202a.png"},"text/plain":{"content":"<Figure size 800x600 with 1 Axes>","content_type":"text/plain"}}},"children":[],"key":"fx4X1xmEIb"}],"key":"re7JcpNVqD"}],"key":"fdhGDrq4PT"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# ----------------------\n# Plots: Hit Rate & Attrition Over Full Simulation\n# ----------------------\n# Active-client hit rate over time\nhit_rate_active = []\ndays = np.arange(T_tot)\nfor t in days:\n    act_clients = active_df.iloc[t]\n    active_ids = [int(col.split('_')[1]) for col, flag in act_clients.items() if flag]\n    daily = rfq_all[rfq_all['time'] == t]\n    rates = []\n    for cid in active_ids:\n        cr = daily[daily['client_id'] == cid]\n        if not cr.empty:\n            rates.append(cr['hit'].mean())\n    hit_rate_active.append(np.mean(rates) if rates else np.nan)\n\nplt.figure(figsize=(10,5))\nplt.plot(days, hit_rate_active, label='Avg Hit Rate', color='blue')\nplt.title('Average Hit Rate Over Time')\nplt.xlabel('Day'); plt.ylabel('Hit Rate'); plt.grid(True); plt.legend()\n\n# Attrition rate over time\nattr_rate = [100 * (Y_all.shape[1] - x) / Y_all.shape[1] for x in active_all]\nplt.figure(figsize=(10,5))\nplt.plot(days, attr_rate, label='% Inactive', color='red')\nplt.title('Cumulative Attrition Rate Over Time')\nplt.xlabel('Day'); plt.ylabel('% Inactive'); plt.grid(True); plt.legend()\n","key":"Cll2SnxYx1"},{"type":"outputs","id":"G7qM_8TWWxvn0EHdsd6w_","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"74486992dd3423a5f7a5d047d0cf64e7","path":"/build/74486992dd3423a5f7a5d047d0cf64e7.png"},"text/plain":{"content":"<Figure size 1000x500 with 1 Axes>","content_type":"text/plain"}}},"children":[],"key":"PuS0VkdI7S"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"ad2e5e10a8fb0a30937c10e983ebc549","path":"/build/ad2e5e10a8fb0a30937c10e983ebc549.png"},"text/plain":{"content":"<Figure size 1000x500 with 1 Axes>","content_type":"text/plain"}}},"children":[],"key":"ZbzSs4S0YT"}],"key":"yuejLx1f22"}],"key":"K6YBGhbCye"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# ----------------------\n# Model Fit Visualization\n# ----------------------\n# Compare predicted attrition vs actual on test\npred_rate = risk_df.groupby('time')['p_inactive'].mean()\nactual_rate = [100*(N_clients-x)/N_clients for x in active_test]\nplt.figure(figsize=(10,5))\nplt.plot(pred_rate.index, 100*pred_rate.values, label='Predicted % Inactive')\nplt.plot(actual_rate, label='Actual % Inactive')\nplt.title('Predicted vs Actual Attrition (Test Set)')\nplt.xlabel('Day'); plt.ylabel('% Inactive'); plt.legend(); plt.grid(True)","key":"PWth89ZOso"},{"type":"outputs","id":"o_AAukrof17RshOrJUlkB","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"537cba5b862ba5d881b882a88a6ba29c","path":"/build/537cba5b862ba5d881b882a88a6ba29c.png"},"text/plain":{"content":"<Figure size 1000x500 with 1 Axes>","content_type":"text/plain"}}},"children":[],"key":"jlmhlSzObB"}],"key":"It1hk7OzqX"}],"key":"ey2ju22BGq"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# ----------------------\n# Function: Plot Clients by ID (single figure with subplots)\n# ----------------------\ndef plot_clients_subplots(client_ids):\n    \"\"\"\n    For each client in client_ids, create a subplot in one figure:\n      - Left y-axis: active (blue=active, red=inactive) and trading days (green), with numeric ticks (0/1)\n      - Right y-axis: attrition probability over time\n      - Separate legends: left for 'Active', 'Inactive', 'Traded'; right for 'P(Inactive)'\n      - Title of each subplot shows the client ID\n    \"\"\"\n    n = len(client_ids)\n    fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(12, 4*n), sharex=True)\n    if n == 1:\n        axes = [axes]\n        \n    days = np.arange(T_test)\n\n    for ax1, cid in zip(axes, client_ids):\n        # gather P(inactive) values\n        p_vals = [\n            risk_df.loc[\n                (risk_df['client_id'] == cid) & (risk_df['time'] == t),\n                'p_inactive'\n            ].item()\n            for t in days\n        ]\n        # latent active flags and trading days\n        active_flag = active_df[T_train:][f'client_{cid}'].values\n        trade_days  = Y_test.iloc[:, cid]\n\n        # Left axis: Active vs Inactive\n        idx_active   = days[active_flag]\n        idx_inactive = days[~active_flag]\n        h_active   = ax1.scatter(idx_active,   [1]*len(idx_active),   c='blue', marker='s', alpha=0.3, label='Active')\n        h_inactive = ax1.scatter(idx_inactive, [1]*len(idx_inactive), c='red',  marker='s', alpha=0.3, label='Inactive')\n        # Left axis: actual trades (0 or 1)\n        h_trade = ax1.scatter(trade_days.index, trade_days.values,\n                              c='green', marker='o', label='Traded')\n\n        # Set numeric ticks on left y-axis\n        ax1.set_ylim(-0.2, 1.2)\n        ax1.set_yticks([0, 1])\n        ax1.set_ylabel('Activity (0/1)')\n        ax1.set_title(f'Client {cid} Activity and Attrition Prob')\n\n        # Left-axis legend\n        ax1.legend(handles=[h_active, h_inactive, h_trade],\n                   labels=['Active', 'Inactive', 'Traded'],\n                   loc='upper left')\n\n        # Right axis: P(Inactive)\n        ax2 = ax1.twinx()\n        h_prob, = ax2.plot(days, p_vals, c='black', label='P(Inactive)')\n        ax2.set_ylim(0, 1)\n        ax2.set_ylabel('P(Inactive)')\n\n        # Right-axis legend\n        ax2.legend(handles=[h_prob], loc='upper right')\n\n    axes[-1].set_xlabel('Day')\n    plt.tight_layout()\n    plt.show()\n\n# Example: plot select clients in one figure\n#plot_clients_subplots([3, 8, 9, 18])\nplot_clients_subplots([3, 8, 9, 30])\n","key":"fRVgF6DqNg"},{"type":"outputs","id":"5Bg-zwXOItI8AMTTNOO-v","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"98ec172d4bb907a13abe623ad893af92","path":"/build/98ec172d4bb907a13abe623ad893af92.png"},"text/plain":{"content":"<Figure size 1200x1600 with 8 Axes>","content_type":"text/plain"}}},"children":[],"key":"UKnvE9vvEn"}],"key":"UVTOpvknMs"}],"key":"NqOAsasmHG"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Simulation of client abnormal behavior","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PbovdfOrkV"}],"identifier":"simulation-of-client-abnormal-behavior","label":"Simulation of client abnormal behavior","html_id":"simulation-of-client-abnormal-behavior","implicit":true,"key":"YK5MbGTDgW"}],"key":"tOaKqlxDrW"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import poisson, norm\nfrom collections import deque\n\ndef run_simulation(N_clients=50,\n                   T=200,\n                   reservation_price_mean=100,\n                   reservation_price_std_pct=0.1,\n                   lambda_mean=1,\n                   lambda_std=0.05,\n                   prior_mean=60,\n                   prior_std=10,\n                   reservation_price_noise_std=None,\n                   discount_trigger_pct=0.35,  # trigger threshold: 0.5 means 50% lower\n                   boosted_rate_factor = 10,\n                   hit_rate_target=0.4,\n                   window_size=10,\n                   attrition_threshold=0.1,\n                   random_seed=42):\n\n    \"\"\"\n    Simulate RFQ interactions with Bayesian quantile pricing and client attrition.\n    Adds a mechanism: if a client gets a quote at least `discount_trigger_pct` below their reservation price,\n    they double their RFQ rate until the dealer quotes above their reservation price.\n    \"\"\"\n\n    np.random.seed(random_seed)\n\n    # Generate distribution of reservation prices\n    reservation_prices = np.random.normal(reservation_price_mean,\n                                          reservation_price_mean * reservation_price_std_pct,\n                                          N_clients)\n\n    # Noise in reservation prices\n    fair_price = reservation_price_mean\n    res_price_noise_std = reservation_price_noise_std or (fair_price * reservation_price_std_pct * 2)\n    res_price_noise_var = res_price_noise_std ** 2\n\n    # Base RfQ intensities\n    lambdas = np.abs(np.random.normal(lambda_mean, lambda_std, N_clients))\n    base_lambdas = lambdas.copy()  # store for reset later\n\n    # Posterior beliefs\n    clients_posterior = {i: {'mean': prior_mean, 'var': prior_std ** 2,\n                             'n': 0, 'sum_obs': 0.0}\n                         for i in range(N_clients)}\n\n    z = norm.ppf(1 - hit_rate_target)\n    active = np.ones(N_clients, dtype=bool)\n\n    # Flags for tracking \"boosted\" mode\n    boosted = np.zeros(N_clients, dtype=bool)\n\n    # Track daily active/boosted status\n    active_flags = np.zeros((T, N_clients), dtype=bool)\n    boosted_flags = np.zeros((T, N_clients), dtype=bool)\n\n    daily_history = [deque(maxlen=window_size) for _ in range(N_clients)]\n    active_history = []\n    records = []\n    Y = np.zeros((T, N_clients), dtype=int)  # binary: whether client sent at least one RFQ that day\n\n    for t in range(T):\n        daily_hits = np.zeros(N_clients, dtype=int)\n        daily_reqs = np.zeros(N_clients, dtype=int)\n\n        for i in range(N_clients):\n            if not active[i]:\n                continue\n\n            n_rfq = poisson.rvs(lambdas[i])\n            if n_rfq > 0:\n                Y[t, i] = 1\n\n            for _ in range(n_rfq):\n                post = clients_posterior[i]\n                price = max(0.0, post['mean'] + np.sqrt(post['var'] + res_price_noise_var) * z)\n                r = norm.rvs(reservation_prices[i], res_price_noise_std)\n\n                # Trading happens when price <= reservation price\n                hit = price <= r\n                daily_reqs[i] += 1\n                daily_hits[i] += int(hit)\n\n                # Check discount trigger\n                if price <= (1 - discount_trigger_pct) * r:\n                    boosted[i] = True\n                    lambdas[i] = base_lambdas[i] * boosted_rate_factor\n                elif boosted[i] and price > r / (1-discount_trigger_pct):\n                    boosted[i] = False\n                    lambdas[i] = base_lambdas[i]\n\n                # Update posterior belief\n                post['n'] += 1\n                post['sum_obs'] += r\n                post_var = 1 / (1 / (prior_std ** 2) + post['n'] / res_price_noise_var)\n                post['var'] = post_var\n                post['mean'] = post_var * (prior_mean / (prior_std ** 2) +\n                                           post['sum_obs'] / res_price_noise_var)\n\n                records.append({'time': t,\n                                'client_id': i,\n                                'price': price,\n                                'reservation': r,\n                                'hit': hit,\n                                'boosted': boosted[i]})\n\n        # Update attrition status\n        for i in range(N_clients):\n            if not active[i] or daily_reqs[i] == 0:\n                continue\n            rate = daily_hits[i] / daily_reqs[i]\n            daily_history[i].append(rate)\n            if len(daily_history[i]) == window_size and np.mean(daily_history[i]) < attrition_threshold:\n                active[i] = False\n\n        active_history.append(int(active.sum()))\n        active_flags[t, :] = active.copy()\n        boosted_flags[t, :] = boosted.copy()\n\n    rfq_df = pd.DataFrame(records)\n    activity_df = pd.DataFrame(Y, columns=[f'client_{i}' for i in range(N_clients)])\n    active_df = pd.DataFrame(active_flags, columns=[f'client_{i}' for i in range(N_clients)])\n    boosted_df = pd.DataFrame(boosted_flags, columns=[f'client_{i}' for i in range(N_clients)])\n\n    return rfq_df, active_history, activity_df, active_df, boosted_df\n","key":"rITCjKd3Fs"},{"type":"outputs","id":"bjDgc235evnJ1NwdYBTl-","children":[],"key":"H6CySIh7qD"}],"key":"ckBaNPtoWa"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Run simulation\nrfq_df, active_history, activity_df, active_df, boosted_df = run_simulation()\n\n# Plot active and boosted clients over time\nplt.figure(figsize=(10,5))\nplt.plot(active_df.sum(axis=1), label='Active Clients')\nplt.plot(boosted_df.sum(axis=1), label='Boosted Clients', linestyle='--')\nplt.xlabel('Day')\nplt.ylabel('Number of Clients')\nplt.title('Active vs Boosted Clients Over Time')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Heatmap of boosted status\nplt.figure(figsize=(12,6))\nplt.imshow(boosted_df.T, aspect='auto', cmap='Reds')\nplt.colorbar(label='Boosted Status (1=Yes, 0=No)')\nplt.xlabel('Day')\nplt.ylabel('Client ID')\nplt.title('Boosted Clients Over Time')\nplt.show()\n","key":"jXikFqzq8z"},{"type":"outputs","id":"v94em2y1qbx8m_XlADepe","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"dd1a537736865ce4d5a29cc2499defcd","path":"/build/dd1a537736865ce4d5a29cc2499defcd.png"},"text/plain":{"content":"<Figure size 1000x500 with 1 Axes>","content_type":"text/plain"}}},"children":[],"key":"n4XjouGHzE"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"ab5da0ed352ae91f3d60b73881f936d9","path":"/build/ab5da0ed352ae91f3d60b73881f936d9.png"},"text/plain":{"content":"<Figure size 1200x600 with 2 Axes>","content_type":"text/plain"}}},"children":[],"key":"GGpKqdRjED"}],"key":"dPE3KQoyaz"}],"key":"sYg6p2kVwc"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from scipy.special import betaln, gammaln\nfrom scipy.optimize import minimize\n\n# -----------------------------\n#  Beta-Binomial mixture MLE with shared mean\n# -----------------------------\n\ndef log_beta_binom_pmf(x, n, alpha, beta):\n    # log [ C(n,x) * B(alpha+x, beta+n-x) / B(alpha,beta) ]\n    return (\n        gammaln(n + 1) - gammaln(x + 1) - gammaln(n - x + 1)\n        + betaln(alpha + x, beta + n - x)\n        - betaln(alpha, beta)\n    )\n\ndef neg_log_likelihood(params, xs, ns):\n    # params = [logit_mu, log_k_g, log_k_b, logit_qg]\n    logit_mu, log_k_g, log_k_b, logit_qg = params\n    \n    mu = 1 / (1 + np.exp(-logit_mu))  # shared mean\n    k_g = np.exp(log_k_g) + 1e-8\n    k_b = np.exp(log_k_b) + 1e-8\n    qg  = 1 / (1 + np.exp(-logit_qg))\n\n    # convert to alpha, beta\n    alpha_g, beta_g = mu * k_g, (1 - mu) * k_g\n    alpha_b, beta_b = mu * k_b, (1 - mu) * k_b\n\n    # mixture likelihood per client\n    ll = []\n    for x, n in zip(xs, ns):\n        log_p_g = log_beta_binom_pmf(x, n, alpha_g, beta_g)\n        log_p_b = log_beta_binom_pmf(x, n, alpha_b, beta_b)\n        # log-sum-exp for mixture\n        a = np.log(qg) + log_p_g\n        b = np.log(1 - qg) + log_p_b\n        m = max(a, b)\n        ll_i = m + np.log(np.exp(a - m) + np.exp(b - m))\n        ll.append(ll_i)\n    return -np.sum(ll)\n\ndef fit_beta_mixture_mle(activity_df, train_days):\n    \"\"\"\n    activity_df: (T x N) binary DataFrame\n    train_days: number of days used for training (first half)\n    Returns dict with fitted parameters.\n    \"\"\"\n    T, N = activity_df.shape\n    A = activity_df.iloc[:train_days].values  # (train_days x N)\n    xs = A.sum(axis=0)  # successes per client in training\n    ns = np.full_like(xs, fill_value=train_days)\n\n    # rough start for mean\n    p_hat = xs.mean() / train_days\n    logit_mu0 = np.log(p_hat / (1 - p_hat + 1e-8))\n\n    start = np.array([\n        logit_mu0,\n        np.log(10.0),   # k_g initial concentration\n        np.log(2.0),    # k_b initial concentration\n        0.0             # logit(0.5)\n    ], dtype=float)\n\n    res = minimize(neg_log_likelihood, start, args=(xs, ns), method='L-BFGS-B')\n    logit_mu, log_k_g, log_k_b, logit_qg = res.x\n    \n    mu  = 1 / (1 + np.exp(-logit_mu))\n    k_g = np.exp(log_k_g) + 1e-8\n    k_b = np.exp(log_k_b) + 1e-8\n    \n    params = {\n        'mu':     float(mu),\n        'alpha_g': float(mu * k_g), 'beta_g': float((1 - mu) * k_g),\n        'alpha_b': float(mu * k_b), 'beta_b': float((1 - mu) * k_b),\n        'q_g':    float(1 / (1 + np.exp(-logit_qg))),\n        'success': bool(res.success),\n        'message': res.message\n    }\n    return params\n\ndef posterior_good(x, n, params):\n    a_g = params['alpha_g']; b_g = params['beta_g']\n    a_b = params['alpha_b']; b_b = params['beta_b']\n    q_g = params['q_g']\n\n    log_p_g = log_beta_binom_pmf(x, n, a_g, b_g)\n    log_p_b = log_beta_binom_pmf(x, n, a_b, b_b)\n\n    a = np.log(q_g) + log_p_g\n    b = np.log(1 - q_g) + log_p_b\n    m = max(a, b)\n    num = np.exp(a - m)\n    den = num + np.exp(b - m)\n    return float(num / den)\n","key":"DtBwni2mrB"},{"type":"outputs","id":"eaJOobVQuIdcCc6trVd0R","children":[],"key":"y2HtsYyEtR"}],"key":"e29KYXhjA4"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Fit on first half, removing first 25 days until it settles\nT = activity_df.shape[0]\nmid = T // 2\nparams = fit_beta_mixture_mle(activity_df[25:], train_days=mid)\n\n# Detect abnormalities on second half using a sliding window\nn_window = 10  # window size for detection, can be adjusted\nthreshold = 0.5  # classify as abnormal if p(good|D) < threshold\n\nN = activity_df.shape[1]\nabnormal_flags = np.zeros_like(activity_df.values, dtype=bool)\n\n# For each day t in the second half, use the last n_window days (bounded within the second half)\nfor t in range(mid, T):\n    start = max(mid, t - n_window + 1)\n    end = t + 1\n    window = activity_df.iloc[start:end].values  # (w x N)\n    n = window.shape[0]\n    x = window.sum(axis=0)\n    # Posterior for each client\n    p_good = np.array([posterior_good(int(xi), int(n), params) for xi in x])\n    abnormal_flags[t, :] = p_good < threshold\n\nabnormal_df = pd.DataFrame(abnormal_flags, columns=activity_df.columns, index=activity_df.index)\n","key":"NAEdAij52A"},{"type":"outputs","id":"aeJNmjbVRHxAgVT7n0i3O","children":[],"key":"vuGrOiH58q"}],"key":"Jis9KAsp22"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"params","key":"wvIyyicBoG"},{"type":"outputs","id":"hgbviWEe3ieMx2Z5py0WZ","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":91,"metadata":{},"data":{"text/plain":{"content":"{'mu': 0.6459024041525655,\n 'alpha_g': 17819.14219878137,\n 'beta_g': 9768.837168102045,\n 'alpha_b': 0.6772910514791353,\n 'beta_b': 0.3713055277018204,\n 'q_g': 0.892871535754001,\n 'success': True,\n 'message': 'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'}","content_type":"text/plain"}}},"children":[],"key":"cq4Bip6Bxw"}],"key":"nUs8vJ4WMn"}],"key":"AmUPgv3IO9"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import beta\n\ndef plot_fitted_betas(params, bins=200):\n    \"\"\"\n    Plot fitted good vs bad Beta distributions.\n    params: dict returned by fit_beta_mixture_mle\n    \"\"\"\n    a_g, b_g = params['alpha_g'], params['beta_g']\n    a_b, b_b = params['alpha_b'], params['beta_b']\n    q_g = params['q_g']\n\n    # Grid\n    x = np.linspace(0, 1, bins)\n\n    # Densities\n    pdf_g = beta.pdf(x, a_g, b_g)\n    pdf_b = beta.pdf(x, a_b, b_b)\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(x, pdf_g, label=f\"Good (α={a_g:.2f}, β={b_g:.2f})\", lw=2, color=\"blue\")\n    plt.plot(x, pdf_b, label=f\"Bad  (α={a_b:.2f}, β={b_b:.2f})\", lw=2, color=\"red\")\n    plt.fill_between(x, pdf_g, alpha=0.2, color=\"blue\")\n    plt.fill_between(x, pdf_b, alpha=0.2, color=\"red\")\n\n    plt.title(\"Fitted Beta Distributions (Good vs Bad)\")\n    plt.xlabel(\"Success probability\")\n    plt.ylabel(\"Density\")\n    plt.legend()\n    plt.grid(True, ls=\"--\", alpha=0.5)\n    plt.show()\n\n\nplot_fitted_betas(params)","key":"X6F5bZk5Kd"},{"type":"outputs","id":"TvT5xWUco3FiLEGMh8HcO","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"13005e6defc8aaf12219edc3a9b5d2dc","path":"/build/13005e6defc8aaf12219edc3a9b5d2dc.png"},"text/plain":{"content":"<Figure size 1000x600 with 1 Axes>","content_type":"text/plain"}}},"children":[],"key":"ySOOXsPiDx"}],"key":"LJ3mW8fCf9"}],"key":"mU366PVlyY"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Use a previous window (exclude day t) when computing P(good|D)\nn_window = 10\nthreshold = 0.5\n\ndef build_pgood_panel(activity_df, boosted_df, active_df, params, mid, n_window, threshold):\n    T, N = activity_df.shape\n    rows = []\n    # Start at mid + n_window to ensure a full *previous* window exists\n    for t in range(mid + n_window, T):\n        start = t - n_window\n        end = t  # exclude day t\n        window = activity_df.iloc[start:end]  # (n_window x N)\n        x = window.sum(axis=0).values.astype(int)  # successes per client in previous window\n        n = n_window\n        # compute p_good for each client\n        p_goods = []\n        for j in range(N):\n            p = posterior_good(int(x[j]), int(n), params)\n            p_goods.append(p)\n        p_goods = np.array(p_goods)\n        # flags at time t\n        boosted_t = boosted_df.iloc[t].values.astype(bool)\n        active_t  = active_df.iloc[t].values.astype(bool)\n        inactive_t = ~active_t\n        traded_today = activity_df.iloc[t].values.astype(int)\n        for j in range(N):\n            rows.append({\n                'day': t,\n                'client_id': j,\n                'x': int(x[j]),\n                'n': int(n),\n                'p_good': float(p_goods[j]),\n                'abnormal': bool(p_goods[j] < threshold),\n                'boosted': bool(boosted_t[j]),\n                'inactive': bool(inactive_t[j]),\n                'traded_today': int(traded_today[j])\n            })\n    panel = pd.DataFrame(rows)\n    return panel\n\npanel_df = build_pgood_panel(activity_df, boosted_df, active_df, params, mid, n_window, threshold)\n\n# Correctly aligned daily summary for second half *with windowing*\ndays = sorted(panel_df['day'].unique())\ndaily_summary = panel_df.groupby('day').agg(\n    abnormal_count=('abnormal', 'sum'),\n    boosted_count=('boosted', 'sum'),\n    inactive_count=('inactive', 'sum')\n).reset_index()\n\n# Overlap summary on client-day panel\noverlap_summary = {\n    'days_evaluated': int(len(days)),\n    'clients': int(N),\n    'total_client_days': int(panel_df.shape[0]),\n    'abnormal_client_days': int(panel_df['abnormal'].sum()),\n    'boosted_client_days': int(panel_df['boosted'].sum()),\n    'inactive_client_days': int(panel_df['inactive'].sum()),\n    'abnormal_and_boosted': int((panel_df['abnormal'] & panel_df['boosted']).sum()),\n    'abnormal_and_inactive': int((panel_df['abnormal'] & panel_df['inactive']).sum()),\n    'abnormal_not_boosted_or_inactive': int((panel_df['abnormal'] & ~panel_df['boosted'] & ~panel_df['inactive']).sum())\n}\noverlap_df2 = pd.DataFrame([overlap_summary])\n\n# Heatmap of p_good by client (days x clients) for visual inspection\n# Build a matrix with rows=days, cols=clients\ndays_idx = daily_summary['day'].values\npgood_mat = np.full((len(days_idx), N), np.nan)\nfor i, t in enumerate(days_idx):\n    slice_t = panel_df[panel_df['day'] == t].sort_values('client_id')\n    pgood_mat[i, :] = slice_t['p_good'].values\n\nplt.figure(figsize=(12,6))\nplt.imshow(pgood_mat.T, aspect='auto')\nplt.colorbar(label='P(good | previous window)')\nplt.xlabel('Day index in second half')\nplt.ylabel('Client ID')\nplt.title('Per-client P(good|D) over time (previous window)')\nplt.show()\n\n   ","key":"XJYTPSo5kn"},{"type":"outputs","id":"nquZAKU9jfan1XnKLU3FN","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"cb125445bcff70e51a8e84984c13fe25","path":"/build/cb125445bcff70e51a8e84984c13fe25.png"},"text/plain":{"content":"<Figure size 1200x600 with 2 Axes>","content_type":"text/plain"}}},"children":[],"key":"JV2jHRUCBd"}],"key":"uA4Wk5271V"}],"key":"GcH2WHPAbM"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from matplotlib.lines import Line2D\n\ndef plot_clients_activity_prob(clients, df):\n    \"\"\"\n    One figure with one row per client.\n    On boosted days, only the 'Boosted' marker is shown (others hidden).\n    \"\"\"\n    clients = list(clients)\n    n = len(clients)\n    if n == 0:\n        return\n    \n    fig, axes = plt.subplots(n, 1, figsize=(12, 2.8 * n), sharex=True, constrained_layout=True)\n    if n == 1:\n        axes = [axes]\n    \n    # Build a single, global legend\n    legend_elems = [\n        Line2D([0],[0], marker='s', linestyle='None', color='blue',  alpha=0.3, label='Active'),\n        Line2D([0],[0], marker='x', linestyle='None', color='gray',  alpha=0.3, label='Inactive'),\n        Line2D([0],[0], marker='o', linestyle='None', color='red',   alpha=0.5, label='Traded'),\n        Line2D([0],[0], marker='^', linestyle='None', color='green', alpha=0.7, label='Boosted'),\n        Line2D([0],[0], linestyle='-', color='black', label='P(good|D)'),\n    ]\n    \n    x_min, x_max = None, None\n    for ax, cid in zip(axes, clients):\n        df_c = df[df['client_id'] == cid].sort_values('day').copy()\n        if df_c.empty:\n            ax.set_title(f\"Client {cid} (no data)\")\n            ax.set_yticks([])\n            continue\n        \n        # Masks\n        boosted_mask  = df_c['boosted']\n        active_mask   = (~df_c['inactive']) & (~boosted_mask)\n        inactive_mask = df_c['inactive'] & (~boosted_mask)\n        traded_mask   = (df_c['traded_today'] == 1) & (~boosted_mask)\n        \n        # Scatter statuses\n        ax.scatter(df_c['day'][active_mask],   [1]*active_mask.sum(),   c='blue',  marker='s', alpha=0.3)\n        ax.scatter(df_c['day'][inactive_mask], [1]*inactive_mask.sum(), c='gray',  marker='x', alpha=0.3)\n        ax.scatter(df_c['day'][traded_mask],   [1]*traded_mask.sum(),   c='red',   marker='o', alpha=0.5)\n        ax.scatter(df_c['day'][boosted_mask],  [1]*boosted_mask.sum(),  c='green', marker='^', alpha=0.7, zorder=3)\n        \n        ax.set_yticks([])\n        ax.set_ylim(0.5, 1.5)\n        \n        # p_good\n        ax2 = ax.twinx()\n        ax2.plot(df_c['day'], df_c['p_good'], color='black', lw=2)\n        ax2.set_ylim(0, 1)\n        ax2.set_ylabel(\"P(good|D)\")\n        \n        ax.set_title(f\"Client {cid}\")\n        \n        # Track x-lims\n        dmin, dmax = df_c['day'].min(), df_c['day'].max()\n        x_min = dmin if x_min is None else min(x_min, dmin)\n        x_max = dmax if x_max is None else max(x_max, dmax)\n    \n    axes[-1].set_xlabel(\"Day\")\n    if x_min is not None and x_max is not None:\n        axes[-1].set_xlim(x_min, x_max)\n    \n    fig.suptitle(\"Selected clients activity\", y=1.02)\n    \n    # Move legend below figure\n    fig.legend(handles=legend_elems, loc='lower center', ncol=5, frameon=False, bbox_to_anchor=(0.5, -0.03))\n    plt.show()\n\n\nplot_clients_activity_prob([30,31,35, 37], panel_df)\n","key":"tv3ewsEmHn"},{"type":"outputs","id":"gzh7AmEKYlyKibMF_d04I","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"35a0ac1c4ab9af5ba8bbdb8cd80efdb0","path":"/build/35a0ac1c4ab9af5ba8bbdb8cd80efdb0.png"},"text/plain":{"content":"<Figure size 1200x1120 with 8 Axes>","content_type":"text/plain"}}},"children":[],"key":"NPn6gsPHsa"}],"key":"bBKqz5aw7y"}],"key":"hnWXe6uH5e"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from sklearn.metrics import confusion_matrix\n\ndef _cm_df(cm, labels=(\"Pred 0\", \"Pred 1\"), index=(\"True 0\", \"True 1\")):\n    \"\"\"Pretty dataframe for a 2x2 confusion matrix.\"\"\"\n    return pd.DataFrame(cm, index=index, columns=labels)\n\ndef evaluate_confusions(panel_df, normalize=False):\n    \"\"\"\n    Evaluate confusion matrices without recomputing thresholds.\n\n    Uses:\n      - df['abnormal'] as the model's predicted abnormal label (already thresholded).\n      - df['inactive'] and df['boosted'] as operational labels.\n      - Global abnormality = (inactive OR boosted).\n\n    Returns dict with both raw arrays and pretty DataFrames.\n    Set normalize=True to return per-true-class rates instead of counts.\n    \"\"\"\n    df = panel_df.copy()\n\n    # Ensure ints\n    y_abnormal = df[\"abnormal\"].astype(int).values\n    y_inactive = df[\"inactive\"].astype(int).values\n    y_boosted  = df[\"boosted\"].astype(int).values\n\n    # Global abnormality (prediction-side for the overall CM)\n    y_global_abn = (df[\"inactive\"] | df[\"boosted\"]).astype(int).values\n\n    # Normalization mode for sklearn\n    norm_mode = \"true\" if normalize else None\n\n    results = {}\n\n    # 1) Inactive (truth) vs Abnormal (model)\n    cm_inactive = confusion_matrix(y_inactive, y_abnormal, labels=[0,1], normalize=norm_mode)\n    results[\"inactive_vs_abnormal\"] = cm_inactive\n    results[\"inactive_vs_abnormal_df\"] = _cm_df(cm_inactive)\n\n    # 2) Boosted (truth) vs Abnormal (model)\n    cm_boosted = confusion_matrix(y_boosted, y_abnormal, labels=[0,1], normalize=norm_mode)\n    results[\"boosted_vs_abnormal\"] = cm_boosted\n    results[\"boosted_vs_abnormal_df\"] = _cm_df(cm_boosted)\n\n    # 3) OVERALL: Abnormal (truth) vs Global abnormality (prediction = inactive OR boosted)\n    cm_overall = confusion_matrix(y_abnormal, y_global_abn, labels=[0,1], normalize=norm_mode)\n    results[\"overall_abnormal_vs_global\"] = cm_overall\n    results[\"overall_abnormal_vs_global_df\"] = _cm_df(cm_overall)\n\n    return results\n\n# Example usage:\ncm_results = evaluate_confusions(panel_df, normalize=False)\nprint(\"Inactive (truth) vs Abnormal (model):\\n\", cm_results[\"inactive_vs_abnormal_df\"], \"\\n\")\nprint(\"Boosted (truth) vs Abnormal (model):\\n\", cm_results[\"boosted_vs_abnormal_df\"], \"\\n\")\nprint(\"Overall: Abnormal (truth) vs Global (inactive OR boosted) prediction:\\n\", cm_results[\"overall_abnormal_vs_global_df\"])\n","key":"OLWHUVr1vp"},{"type":"outputs","id":"ZC84a3-zSATWTjM8T1jES","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"Inactive (truth) vs Abnormal (model):\n         Pred 0  Pred 1\nTrue 0    3830      74\nTrue 1      56     540 \n\nBoosted (truth) vs Abnormal (model):\n         Pred 0  Pred 1\nTrue 0    3867     609\nTrue 1      19       5 \n\nOverall: Abnormal (truth) vs Global (inactive OR boosted) prediction:\n         Pred 0  Pred 1\nTrue 0    3811      75\nTrue 1      69     545\n"},"children":[],"key":"HIguZCunZN"}],"key":"SQ0szv425M"}],"key":"wQwczLCNtk"}],"key":"plCDlAID8k"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"Market Microstructure","url":"/notebooks/market-microstructure","group":"Notebooks"},"next":{"title":"Fair Price Estimation","url":"/notebooks/fair-price-estimation","group":"Notebooks"}}},"domain":"http://localhost:3002"}