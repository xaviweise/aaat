

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>6. Stochastic Calculus &#8212; Advanced Analytics and Algorithmic Trading</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'markdown/stochastic_calculus';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="7. Stochastic optimal control" href="stochastic_optimal_control.html" />
    <link rel="prev" title="5. Causal inference" href="intro_causal.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../welcome.html">
  
  
  
  
  
  
    <p class="title logo__title">Advanced Analytics and Algorithmic Trading</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../welcome.html">
                    Welcome
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="releases.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="dedication.html">Dedication</a></li>
<li class="toctree-l1"><a class="reference internal" href="preface.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="symbollist.html">Symbols</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Financial Fundamentals</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="intro_financial_markets.html">1. Financial Markets</a></li>
<li class="toctree-l1"><a class="reference internal" href="market_microstructure.html">2. Market microstructure</a></li>
<li class="toctree-l1"><a class="reference internal" href="algorithmic_trading.html">3. Algorithmic Trading</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Modelling Fundamentals</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="intro_bayesian.html">4. Introduction to Bayesian Modelling</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro_causal.html">5. Causal inference</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">6. Stochastic Calculus</a></li>
<li class="toctree-l1"><a class="reference internal" href="stochastic_optimal_control.html">7. Stochastic optimal control</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_driven_methods.html">8. Data-driven methods</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Execution Algorithms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="execution_fundamentals.html">9. Execution fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="lob_models.html">10. Modelling the Limit Order Book</a></li>
<li class="toctree-l1"><a class="reference internal" href="almgren_chriss.html">11. The Almgren - Chriss Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="execution_tactics.html">12. Execution tactics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Market Making Algorithms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="market_making_fundamentals.html">13. Market Making fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="fair_price_estimation.html">14. Fair price estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="liquidity_modelling.html">15. Liquidity modelling</a></li>
<li class="toctree-l1"><a class="reference internal" href="rfq_models.html">16. Modelling the request for quote process</a></li>
<li class="toctree-l1"><a class="reference internal" href="avellaneda_stoikov.html">17. The Avellaneda and Stoikov Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="enriching_avellaneda.html">18. Enriching the Avellaneda and Stoikov Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="hedging.html">19. Hedging strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="market_making_rl.html">20. Reinforcement Learning and Market Making</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Investment Algorithms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="quant_investment_fundamentals.html">21. Quantitative investment fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="mean_reversion_strategies.html">22. Mean reversion strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="trend_following.html">23. Trend following strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="arbitrage.html">24. Arbitrage Strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="factor_investing.html">25. Factor Investing</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimal_portfolios.html">26. Optimal portfolios</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Glossary and References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="glossary.html">27. Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="references.html">References</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Notebooks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../notebooks/stochastic_calculus.html">Stochastic Calculus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/fair_price_estimation.html">Fair Price Estimation</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/xaviweise/aaat" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/xaviweise/aaat/issues/new?title=Issue%20on%20page%20%2Fmarkdown/stochastic_calculus.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/markdown/stochastic_calculus.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Stochastic Calculus</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-modelling-dynamical-systems">6.1. Introduction: modelling dynamical systems</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deterministic-dynamical-systems">6.2. Deterministic dynamical systems</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-newton-s-laws">6.2.1. Example: Newton’s Laws</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-simple-inflation-targeting-model">6.2.2. Example: simple inflation targeting model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-free-fall-with-friction">6.2.3. Example: Free fall with friction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-solutions-of-dynamical-systems">6.2.4. Numerical solutions of dynamical systems</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-wiener-process">6.3. The Wiener Process</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definition">6.3.1. Definition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-gaussian-processes">6.3.2. Connection to Gaussian Processes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#filtrations-and-the-martingale-property">6.3.3. Filtrations and the Martingale Property</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-tower-law">6.3.4. The Tower Law</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-wiener-process">6.3.5. Multivariate Wiener process</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ito-s-lemma">6.3.6. Ito’s Lemma</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#integrals-of-the-wiener-process">6.3.7. Integrals of the Wiener process</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simulation-of-the-wiener-process">6.3.8. Simulation of the Wiener process</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#univariate-processes">6.3.8.1. Univariate processes</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-processes">6.3.8.2. Multivariate processes</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-differential-equations">6.4. Stochastic differential equations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-feynman-kac-theorem">6.5. The Feynman - Kac Theorem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-stochastic-processes">6.6. Basic stochastic processes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#brownian-motion-with-drift">6.6.1. Brownian motion with drift</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">6.6.1.1. Connection to Gaussian processes</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#simulation">6.6.1.2. Simulation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation">6.6.1.3. Estimation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensional-analysis">6.6.1.4. Dimensional analysis</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#applications">6.6.1.5. Applications</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#geometric-brownian-motion">6.6.2. Geometric Brownian motion</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#arithmetic-average-of-the-brownian-motion">6.6.3. Arithmetic Average of the Brownian motion</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#orstein-uhlenbeck-process">6.6.4. Orstein-Uhlenbeck process</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#other-mean-reverting-processes">6.6.4.1. Other mean reverting processes</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#brownian-bridge">6.6.5. Brownian bridge</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">6.6.5.1. Connection to Gaussian processes</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">6.6.5.2. Simulation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">6.6.5.3. Applications</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#jump-processes">6.7. Jump processes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">6.8. Exercises</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="stochastic-calculus">
<span id="id1"></span><h1><span class="section-number">6. </span>Stochastic Calculus<a class="headerlink" href="#stochastic-calculus" title="Permalink to this heading">#</a></h1>
<section id="introduction-modelling-dynamical-systems">
<span id="modelling-dynamical-systems"></span><h2><span class="section-number">6.1. </span>Introduction: modelling dynamical systems<a class="headerlink" href="#introduction-modelling-dynamical-systems" title="Permalink to this heading">#</a></h2>
<p>A dynamical system is a set of variables that follow a time dynamics law</p>
<div class="math notranslate nohighlight">
\[y = f(t,{X_t})\]</div>
<p>where <span class="math notranslate nohighlight">\(\{X_t\}\)</span> is a set of external factors that influence the trajectory of the system.</p>
<p>The trajectory might be difficult to derive, and sometimes it is easier to derive the dynamics in small steps. Considering the case <span class="math notranslate nohighlight">\(y = f(t)\)</span></p>
<div class="math notranslate nohighlight">
\[\frac{dy}{dt} = g(t)\]</div>
<p>where <span class="math notranslate nohighlight">\(g(t) = \frac{df}{dt}\)</span> and then integrate it</p>
<div class="math notranslate nohighlight">
\[y(t) = y(0) + \int_0^t g(t) dt\]</div>
<p>In the more general case where there are external factors with their own time dynamics:</p>
<div class="math notranslate nohighlight">
\[dy = \frac{\partial f}{\partial t}dt + \sum_{n=1}^N \frac{\partial f}{\partial X_{i,t}} d X_{i,t}\]</div>
<p>Modelization can be done empirically or using fundamental laws if available, we will see examples in next section</p>
<p>The dynamics of the system might not be deterministic. We can also model the dynamics of a stochastic system in the same way. In this case we need to model the source of randomness in the dynamical system, for which we use  stochastic differential equations (SDE) of the form:</p>
<div class="math notranslate nohighlight">
\[d X_t = \mu(X_t, t) dt + \sigma (X_t, t) d W_t\]</div>
<p>where <span class="math notranslate nohighlight">\(d W_t\)</span> is the Wiener process (Brownian motion) that will be introduced later.</p>
</section>
<section id="deterministic-dynamical-systems">
<h2><span class="section-number">6.2. </span>Deterministic dynamical systems<a class="headerlink" href="#deterministic-dynamical-systems" title="Permalink to this heading">#</a></h2>
<p>Let us review some relevant deterministic differential equations that
come across in the modelization of dynamical systems.</p>
<section id="example-newton-s-laws">
<h3><span class="section-number">6.2.1. </span>Example: Newton’s Laws<a class="headerlink" href="#example-newton-s-laws" title="Permalink to this heading">#</a></h3>
<p>A first example comes from Newton’s laws applied to the dynamics of particles in a gravitational field. The second law of Newton states:</p>
<div class="math notranslate nohighlight">
\[F = m a\]</div>
<p>We consider an object of mass m drop at initial height
<span class="math notranslate nohighlight">\(h(0)\)</span> with speed zero. It is only subjected to the gravitational force:
<span class="math notranslate nohighlight">\(F = m g\)</span> where g is the gravitational constant close to Earth’s surface: <span class="math notranslate nohighlight">\(g \approx 9.81 m/s^2\)</span>. The dynamics for the speed is therefore a first order differential equation with constant coefficients:</p>
<div class="math notranslate nohighlight">
\[m \frac{d v}{d t} = - m g \rightarrow \frac{d v}{d t} = - g\]</div>
<p>where we have taken into account that the force is exerted in the opposite direction to which the height axis grows. This equation is simple to integrate:</p>
<div class="math notranslate nohighlight">
\[v(t) = v(0) - \int_0^t g dt = v(0) + g t = - gt\]</div>
<p>since in our case, <span class="math notranslate nohighlight">\(v(0) = 0\)</span>. The dynamics of the position reads:</p>
<div class="math notranslate nohighlight">
\[\frac{d h}{dt} = v(t) = - g t\]</div>
<p>which can be again be easily integrated:</p>
<div class="math notranslate nohighlight">
\[h(t) = h(0) - \int_0^t g t' dt' = h(0) - g \frac{t^2}{2}\]</div>
<p>These are particular examples of a general family of differential
equations:</p>
<div class="math notranslate nohighlight">
\[\frac{d^n y}{d t^n} = f(t)\]</div>
<p>which can be simply integrated step by step by defining intermediate variables, e.g.:</p>
<div class="math notranslate nohighlight">
\[z \equiv \frac{d^{n-1} y}{d t^{n-1}} \rightarrow \frac{d z}{dt} = f(t) \rightarrow z(t) = z(0) + \int_{t_0}^{t_1} f(t) dt\]</div>
</section>
<section id="example-simple-inflation-targeting-model">
<h3><span class="section-number">6.2.2. </span>Example: simple inflation targeting model<a class="headerlink" href="#example-simple-inflation-targeting-model" title="Permalink to this heading">#</a></h3>
<p>A simple model of the dynamics of inflation takes into account Central Bank interventions using monetary policy in order to adjust inflation to a given target. When inflation deviates from the target, monetary policy
is used to bring it back to the target. This can be modelled using the following differential equation:</p>
<div class="math notranslate nohighlight">
\[\frac{d \pi_t}{d t} = \theta (\hat{\pi} - \pi_t)\]</div>
<p>where <span class="math notranslate nohighlight">\(\pi\)</span> is the current level of inflation, <span class="math notranslate nohighlight">\(\hat{\pi}\)</span> is the inflation target, and <span class="math notranslate nohighlight">\(\theta &gt; 0\)</span> is a constant that models the speed at which monetary
policy takes effect. This is an example of a mean - reverting equation, a process whose dynamics is controlled by an external force that drives
back the system towards a stable level (the mean).</p>
<p>This equation is a particular example of a general family of differential equations which we call separable, with the form</p>
<div class="math notranslate nohighlight">
\[\frac{d y}{d t} = g(y)f(t)\]</div>
<p>They can be solved by exploiting the
separability:</p>
<div class="math notranslate nohighlight">
\[\int_{y_0}^{y_t} \frac{dy}{g(y)} = \int_{t_0}^t f(t') dt'\]</div>
<p>Coming back to the particular example of the mean reverting equation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\int_{\pi_{t_0}}^{\pi_t}\frac{d \pi}{\hat{\pi} - \pi} = \theta \int_{t_0}^t dt' \rightarrow -\log(\frac{\hat{\pi} - \pi_t}{\hat{\pi} - \pi_{t_0}}) = \theta (t - t_0) \\
\pi_t = \hat{\pi} + (\pi_{t_0}-\hat{\pi}) e^{-\theta(t-t_0)}
\end{aligned}\end{split}\]</div>
<p>The solution shows as, as expected, a trajectory for the
inflation that decays to the target. The time scale for decay is independent of the size of the initial gap <span class="math notranslate nohighlight">\(\pi_{t_0} -\hat{\pi}\)</span>, depending exclusively on <span class="math notranslate nohighlight">\(\theta\)</span>, which is called the mean reversion speed. In analogy to the physical process of radioactive decay, which
can be described by a similar equation, we can define a half-life <span class="math notranslate nohighlight">\(\tau\)</span>, or time in which the gap is closed by half, as: <span class="math notranslate nohighlight">\(\tau = \frac{\log }{\theta}\)</span> Alternatively, <span class="math notranslate nohighlight">\(1/\theta\)</span> is the time it
takes to close the gap to <span class="math notranslate nohighlight">\(1/e \approx 0.369\)</span></p>
</section>
<section id="example-free-fall-with-friction">
<h3><span class="section-number">6.2.3. </span>Example: Free fall with friction<a class="headerlink" href="#example-free-fall-with-friction" title="Permalink to this heading">#</a></h3>
<p>We can again resort to Newton’s laws to model the dynamics of an object in free fall in the atmosphere, where the air introduces an opposite resistance to the action of gravity. A simple model of such friction is
a force proportional to the speed of the object, namely <span class="math notranslate nohighlight">\(F = \eta v\)</span>, where <span class="math notranslate nohighlight">\(\eta\)</span> is a coefficient that depends on the properties of the atmosphere. Using Newton’s law, we have now:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
m \frac{dv} {dt} = mg - \eta v \\
\frac{d h} {dt} = v
\end{aligned}\end{split}\]</div>
<p>Which is a system of two equations. We can combine it to
get a differential equation on the particle’s trajectory:</p>
<div class="math notranslate nohighlight">
\[m \frac{d^2 h}{d t^2} + \eta \frac{dh}{dt} = mg\]</div>
<p>which is, particularly, a second order non-homogeneous ODE with constant coefficients. A general non-homogeneous linear ODE with constant coefficients reads:</p>
<div class="math notranslate nohighlight">
\[a_n \frac{d^n y}{dt^n} + a_{n-1} \frac{d^{n-1} y}{dt^{n-1}} + ... + a_0 y = f(t)\]</div>
<p>To solve this equation first we find a general solution for the homogeneous equation, and then a particular solution for the non-homogeneous one.</p>
<p>For the general solution of the homogeneous we can actually exploit its connection to a system of first order differential equations, an example
of which we saw when introducing the problem of a particle in free-fall with friction. In general, we can define auxiliary variables
<span class="math notranslate nohighlight">\(y_i = \frac{d^i y}{dt^i}\)</span> for <span class="math notranslate nohighlight">\(i &lt; n\)</span>. We get the following system:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
 \frac{d y_{n-1}}{dt} + \frac{a_{n-1}}{a_n} y_{n-1} + ... + \frac{a_0}{a_n} y_0 = 0 \nonumber \\
\frac{d y_{n-2}}{dt} - y_{n-1} = 0 \nonumber \\
... \nonumber \\
\frac{d y_0}{dt} - y_1 = 0
\end{aligned}\end{split}\]</div>
<p>which has the structure:</p>
<div class="math notranslate nohighlight">
\[\frac{d {\bf y}}{dt} + A {\bf y} = 0\]</div>
<p>where <span class="math notranslate nohighlight">\({\bf y} = (y_0, ..., y_{n-1})\)</span> and A is the matrix form with the coefficients of the system of equations above. The general solution to this equation is:</p>
<div class="math notranslate nohighlight">
\[{\bf y} =  e^{-A t} {\bf y_0}\]</div>
<p>This is the formal solution, but to get a workable solution we need to diagonalize the matrix A, i.e. finding <span class="math notranslate nohighlight">\(A = C \Lambda C^{-1}\)</span> where <span class="math notranslate nohighlight">\(\Lambda\)</span> is a diagonal matrix with the eigenvalues of A, and C has its eigenvectors <span class="math notranslate nohighlight">\(\vec{w}\)</span> as columns. One can then project the solution in the space of eigenvectors:</p>
<div class="math notranslate nohighlight">
\[{\bf y_t} =  C C^{-1} e^{-A t} C C^{-1} {\bf y_0} = C e^{-\Lambda t} C^{-1} {\bf y_0}\]</div>
<p>This is equivalent to writing:</p>
<div class="math notranslate nohighlight">
\[{\bf y_t} = \sum_i w_i e^{-\lambda_i t}  {\bf c_i}\]</div>
<p>where <span class="math notranslate nohighlight">\({\bf c_i}\)</span> is the ith eigenvector with eigenvalue <span class="math notranslate nohighlight">\(\lambda_i\)</span> and <span class="math notranslate nohighlight">\(w_i = (C^{-1} {\bf y_0})_i\)</span> is the projection of the initial state into
the eigenvectors basis. In such basis, the dynamics is simple, driven by the exponential of its eigenvalue: <span class="math notranslate nohighlight">\(e^{-\lambda_i t}\)</span></p>
<p>Coming back to the particle in free-fall, we can obtain the general solution of the homogeneous equation, namely:</p>
<div class="math notranslate nohighlight">
\[\frac{d^2h}{dt} + \frac{\eta}{m} \frac{dh}{dt} = 0\]</div>
<p>by applying directly the ansatz <span class="math notranslate nohighlight">\(e^{\lambda t}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\lambda^2 + \frac{\eta}{m} \lambda = 0\]</div>
<p>which is equivalent to finding the eigenvalues of the matrix of the system of equations. This equation has two eigenvalues, <span class="math notranslate nohighlight">\(\lambda = 0\)</span> and <span class="math notranslate nohighlight">\(\lambda = -\frac{\eta}{m}\)</span>, so the general solution reads:</p>
<div class="math notranslate nohighlight">
\[h_t = C_0 + C_1 e^{-\frac{\eta}{m}t}\]</div>
<p>A particular solution is a linear function <span class="math notranslate nohighlight">\(h_t = \frac{m g}{\eta} t\)</span>. Therefore, the solution for
the full equation is:</p>
<div class="math notranslate nohighlight">
\[h_t = \frac{m g}{\eta} t + C_0 + C_1 e^{-\frac{\eta}{m}t}\]</div>
<p>The speed is therefore:</p>
<div class="math notranslate nohighlight">
\[v_t = \frac{m g}{\eta} - C_1 \frac{\eta}{m} e^{-\frac{\eta}{m}t}\]</div>
<p>Finally, by imposing initial conditions <span class="math notranslate nohighlight">\(h_0\)</span> and <span class="math notranslate nohighlight">\(v_0 = 0\)</span>, then <span class="math notranslate nohighlight">\(C_1 = \frac{m^2 g}{\eta^2}\)</span> and <span class="math notranslate nohighlight">\(C_0 = h_0 - C_1\)</span>, so:</p>
<div class="math notranslate nohighlight">
\[h_t = h_0 + \frac{m g}{eta} t +  \frac{m^2 g}{eta^2} (e^{-\frac{\eta}{m}t} - 1)\]</div>
<p>Notice that the speed of the particle is then:</p>
<div class="math notranslate nohighlight">
\[v_t = \frac{m g}{\eta} -\frac{m g}{\eta} h_0 e^{-\frac{\eta}{m}t}\]</div>
<p>which tends to a constant when <span class="math notranslate nohighlight">\(t \rightarrow \infty\)</span>,
<span class="math notranslate nohighlight">\(v_\infty = \frac{m g}{\eta}\)</span>, which is called the terminal velocity. At this speed, the friction force equates the gravitational pull and the
object no longer accelerates, therefore reaching a constant velocity. Another interesting observation is that the speed equation has also the
form of a mean reverting equation, in this case the mean being the terminal velocity, and the time-scale for mean reversion proportional to <span class="math notranslate nohighlight">\(m/\eta\)</span>.</p>
</section>
<section id="numerical-solutions-of-dynamical-systems">
<h3><span class="section-number">6.2.4. </span>Numerical solutions of dynamical systems<a class="headerlink" href="#numerical-solutions-of-dynamical-systems" title="Permalink to this heading">#</a></h3>
<p>When closed form solutions are not available, one can resort to numerical schemes to simulate dynamical systems. The most popular one is the Euler method. First we discretize the time domain using a grid
<span class="math notranslate nohighlight">\(t_i = t_0 + \Delta * i\)</span>, <span class="math notranslate nohighlight">\(i = 0, .., N\)</span>, where
<span class="math notranslate nohighlight">\(\Delta = \frac{t_1 - t_0}{N}\)</span>. The differentials can be discretized over the grid as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\frac{dy}{dt} \rightarrow \frac{y_{t_{i+1}}-y_{t_i}}{\Delta} \\
\frac{d^2y}{dt^2} = \frac{d}{dt} \frac{dy}{dt} \rightarrow \frac{y_{t_{i+1}}-2 y_{t_i} + y_{t_{i-1}}}{\Delta^2} \\
...
\end{aligned}\end{split}\]</div>
<p>Notice that there is some ambiguity on where to evaluate
the derivatives in the grid. For instance the second order one could also be evaluated in a “forward” scheme instead of the “central”
used:</p>
<div class="math notranslate nohighlight">
\[\frac{d^2y}{dt^2} \rightarrow \frac{y_{t_{i+2}}-2 y_{t_{i+1}} + y_{t_i}}{\Delta^2}\]</div>
<p>The same applies to the evaluation of functions in the equation. There are multiple schemes to do it. The most simple one is the forward method, where we evaluate functions at the initial point of the grid
step, <span class="math notranslate nohighlight">\(y(t) \rightarrow y_{t_i}\)</span>. In the backward case we evaluate it at the final point of the step, <span class="math notranslate nohighlight">\(y(t) \rightarrow y_{y_{i+1}}\)</span>. There other
possibilities, for example the Crank - Nicolson method uses an average of forward and backward:
<span class="math notranslate nohighlight">\(y(t) \rightarrow \frac{1}{2} (y_{t_i} + y_{y_{i+1}})\)</span>. In general all these methods with the exception of the forward one yield implicit discrete equations, which require more work to solve, but have usually
better convergence properties (speed, accuracy).</p>
<p>Let us see an example. Previously, we introduced the following model for inflation targeting:</p>
<div class="math notranslate nohighlight">
\[\frac{d \pi_t}{d t} = \theta (\hat{\pi} - \pi_t)\]</div>
<p>A forward Euler scheme yields the following discrete equation:</p>
<div class="math notranslate nohighlight">
\[\pi_{t_{i+1}} = \pi_{t_i} + \Delta \theta (\hat{\pi} - \pi_{t_i})\]</div>
<p>This equation is ready for simulation. Starting with an initial condition <span class="math notranslate nohighlight">\(\pi_{t_{i_0}}\)</span> and a grid size <span class="math notranslate nohighlight">\(\Delta\)</span>, we can iteratively evaluate the trajectory of inflation.</p>
<p>The backward Euler scheme gives the following implicit equation:</p>
<div class="math notranslate nohighlight">
\[\pi_{t_{i+1}} = \pi_{t_i} + \Delta \theta (\hat{\pi} - \pi_{t_{i+1}} )\]</div>
<p>In order to simulate it, we need to first solve for <span class="math notranslate nohighlight">\(\pi_{t_{i+1}}\)</span>. In this case this is simple:</p>
<div class="math notranslate nohighlight">
\[\pi_{t_{i+1}} = \frac{\pi_{t_i} + \Delta \theta \hat{\pi}}{1 - \Delta \theta }\]</div>
<p>We leave as an exercise the solution of the Crank Nicholson scheme for this equation.</p>
<p>Let us now solve these equations analytically and numerically for particular values, using the forward and backward schemes. First we choose a set of parameters and grid size where both schemes yield accurate results, matching the analytical solution:</p>
<figure class="align-default" id="fig-num-inflation-targetting-normal">
<a class="reference internal image-reference" href="../_images/num_inflation_targetting_normal.png"><img alt="../_images/num_inflation_targetting_normal.png" src="../_images/num_inflation_targetting_normal.png" style="width: 8in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 6.1 </span><span class="caption-text">Solution of the inflation targetting differential equation  comparing the analytical solution with the forward and backward schemes. We choose a set of parameters and grid size where both methods approximate well the analytical solution: <span class="math notranslate nohighlight">\(\theta = 0.1, \Delta = 0.1, \hat{\pi} = 2.0, \pi_0 = 5.0\)</span></span><a class="headerlink" href="#fig-num-inflation-targetting-normal" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Let us now increase the grid size and choose another set of parameters to make the numerical approximation more challenging. We can see how the forward scheme deviates substantially from the analytical solution, whereas the backward scheme remains more robust:</p>
<figure class="align-default" id="fig-num-inflation-targetting-challenging">
<a class="reference internal image-reference" href="../_images/num_inflation_targetting_challenging.png"><img alt="../_images/num_inflation_targetting_challenging.png" src="../_images/num_inflation_targetting_challenging.png" style="width: 8in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 6.2 </span><span class="caption-text">Solution of the inflation targetting differential equation  comparing the analytical solution with the forward and backward schemes. We choose a set of parameters and grid size where a numerical approximation is more challenging, to show the advantages of the backward scheme: <span class="math notranslate nohighlight">\(\theta = 10, \Delta = 0.1, \hat{\pi} = 2.0, \pi_0 = 5.0\)</span></span><a class="headerlink" href="#fig-num-inflation-targetting-challenging" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="the-wiener-process">
<h2><span class="section-number">6.3. </span>The Wiener Process<a class="headerlink" href="#the-wiener-process" title="Permalink to this heading">#</a></h2>
<section id="definition">
<h3><span class="section-number">6.3.1. </span>Definition<a class="headerlink" href="#definition" title="Permalink to this heading">#</a></h3>
<p>So far we have discussed models where we neglect any uncertainty in the trajectories of the dynamical systems we are modelling. However, in many situations, randomness is a dominant characteristic of the process and
we need tools to model such stochastic systems.</p>
<p>The Wiener process is the main building block when modelling stochastic dynamical systems which exhibit continuous trajectories. In order to
model discrete jumps we will introduce later another building block, the Poisson process.</p>
<p>Many introductory books motivate the Wiener process as the continuous limit of a random walk, e.g. a discrete dynamical process in which each step is decided by a random binary variable <span class="math notranslate nohighlight">\(Z_t = {1, -1}\)</span> with
<span class="math notranslate nohighlight">\(p = 0.5\)</span>, i.e. analogous to flipping an unbiased coin. If we know that the process at time <span class="math notranslate nohighlight">\(t\)</span> is <span class="math notranslate nohighlight">\(X_t\)</span>, then <span class="math notranslate nohighlight">\(X_{t+1} = X_t + Z_t\)</span>.</p>
<p>Here, we will instead introduce the Wiener process as a fundamental building block for the modelization of stochastic processes, i.e. a tool
that will allow us to capture certain observed or proposed behaviours of those systems in a model from which we can draw inferences. In such way,
we define a Wiener process as a type of stochastic process, i.e. an (in principle infinite) sequence of random variables <span class="math notranslate nohighlight">\(W_t\)</span> indexed by time
<span class="math notranslate nohighlight">\(t\)</span>, with the following particular properties:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(W_0 = 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(W_{t+s} - W_s \sim N(0, s)\)</span> where <span class="math notranslate nohighlight">\(N(0,s)\)</span> is a Gaussian
distribution with zero mean and variance <span class="math notranslate nohighlight">\(s\)</span>, i.e. the increments of
the Wiener process are Gaussian</p></li>
<li><p><span class="math notranslate nohighlight">\(W_t\)</span> has independent increments, meaning that any
<span class="math notranslate nohighlight">\(W_{t_1}-W_{t_2}\)</span>, <span class="math notranslate nohighlight">\(W_{t_3} - W_{t_4}\)</span> where <span class="math notranslate nohighlight">\(t_1 &lt; t_2 &lt; t_3 &lt; t_4\)</span>
are independent</p></li>
<li><p><span class="math notranslate nohighlight">\(W_t\)</span> has continuous trajectories</p></li>
</ul>
</section>
<section id="connection-to-gaussian-processes">
<h3><span class="section-number">6.3.2. </span>Connection to Gaussian Processes<a class="headerlink" href="#connection-to-gaussian-processes" title="Permalink to this heading">#</a></h3>
<p>The specific properties of the Wiener process make it part of the family of Gaussian processes, which are stochastic processes with the property that any finite set of them, <span class="math notranslate nohighlight">\(X_{t_1}, ..., X_{t_n}\)</span> follow a joined
multivariate Gaussian distribution. The Gaussian process can be described then by a mean function <span class="math notranslate nohighlight">\(\mu_t = \mathbb{E}[X_t]\)</span> and a covariance function <span class="math notranslate nohighlight">\(k(t_1, t_2) = \textrm{cov}[X_{t_1}, X_{t_2}]\)</span> that is usually referred as the kernel function of the Gaussian process.</p>
<p>The index for the stochastic process does not need to refer to time, another typical indexing is space, for instance. When using time as an
index, we talk about Gaussian Processes for time-series modelling. This is the case for the Wiener process, which is a Gaussian process with
mean <span class="math notranslate nohighlight">\(\mu_t = 0\)</span> and kernel:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
k(t_1, t_2) = \textrm{cov}[W_{t_1}, W_{t_2}] = \mathbb{E}[W_{t_1} W_{t_2}] = \nonumber \\ \left(\mathbb{E}[(W_{t_1} - W_{t_2}) W_{t_2}] + \mathbb{E}[W_{t_2}^2]\right)1_{t_1 &gt; t_2} +  \left(\mathbb{E}[W_{t_1}(W_{t_2} - W_{t_1})] + \mathbb{E}[W_{t_1}^2]\right)1_{t_1 \leq t_2} = \nonumber \\
t_2 1_{t_1 &gt; t_2} + t_1 1_{t_1 \leq t_2} = min(t_1,t_2)
\end{aligned}\end{split}\]</div>
<p>In which sense it is useful to introduce this relationship? The theory of Gaussian processes has been pushed in the last years within the Machine Learning community, and there are multiple tools available for building, training and making inferences with these
models. Such toolkit can be useful when building models for dynamical systems using stochastic processes.</p>
</section>
<section id="filtrations-and-the-martingale-property">
<h3><span class="section-number">6.3.3. </span>Filtrations and the Martingale Property<a class="headerlink" href="#filtrations-and-the-martingale-property" title="Permalink to this heading">#</a></h3>
<p>The Wiener process satisfies the Martingale property, meaning that if we have a series of observations of the process <span class="math notranslate nohighlight">\(W_{t_1}, W_{t_2}, ... W_{t_n}\)</span> then the expected value of an observation <span class="math notranslate nohighlight">\(t_{n+1} &gt; t_n &gt; ... &gt; t_1\)</span> conditioned to the previous observations only depends on the last observation:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[W_{t_{n+1}}|W_{t_1}, W_{t_2}, ... W_{t_n}] = W_{t_n}\]</div>
<p>i.e., the information from previous observations is irrelevant. We can generalize the martingale property by introducing the notion of filtration <span class="math notranslate nohighlight">\(F_t\)</span> at
time <span class="math notranslate nohighlight">\(t\)</span>, which contains all the information set available until time <span class="math notranslate nohighlight">\(t\)</span>. We can rewrite the Martingale property using the filtration as:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[W_{t_{n+1}}|F_{t_n}] = W_{t_n}\]</div>
<p>We can prove the Martingale property using the defining properties of the Wiener process:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{E}[W_{t_{n+1}}|F_{t_n}] = \mathbb{E}[W_{t_{n+1}} - W_{t_n} + W_{t_n}|F_{t_n}] = \nonumber \\
 \mathbb{E}[W_{t_{n+1}} - W_{t_n}|F_{t_n}]+ \mathbb{E}[W_{t_n}|F_{t_n}] = W_{t_n}
\end{aligned}\end{split}\]</div>
<p>where we have used that the increments of the Wiener
process have zero mean.</p>
</section>
<section id="the-tower-law">
<h3><span class="section-number">6.3.4. </span>The Tower Law<a class="headerlink" href="#the-tower-law" title="Permalink to this heading">#</a></h3>
<p>A useful property that exploits the concept of filtration in many applications is the so-called Law of Iterated Expectations or Tower Law. It simply states that for any random variable Y, if <span class="math notranslate nohighlight">\(t_m &gt; t_n\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[\mathbb{E}[Y|F_{t_m}]|F_{t_n}] = \mathbb{E}[Y|F_{t_n}]\]</div>
<p>This is a useful property to compute expectations using a divide and conquer philosophy: maybe the
full expectation <span class="math notranslate nohighlight">\(\mathbb{E}[Y|F_{t_n}]\)</span> is not obviously tractable, but by conditioning it at intermediate filtrations
<span class="math notranslate nohighlight">\(\mathbb{E}[\mathbb{E}[\mathbb{E}[...\mathbb{E}[Y|F_{t_m}] ... |F_{t_{n+2}}|F_{t_{n+1}}|F_{t_n}]\)</span> we can
work out the solution. We will see examples later on.</p>
</section>
<section id="multivariate-wiener-process">
<span id="multivariate-wiener"></span><h3><span class="section-number">6.3.5. </span>Multivariate Wiener process<a class="headerlink" href="#multivariate-wiener-process" title="Permalink to this heading">#</a></h3>
<p>We can construct a multivariate Wiener process as a vector
<span class="math notranslate nohighlight">\({\bf W}_t= (W_{1t}, W_{2t}, ..., W_{Nt})\)</span> with the following properties:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\({\bf W}_t = {\bf 0}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\({\bf W}_{t+s} - {\bf W_t}_s \sim N(0, \Sigma s)\)</span> where <span class="math notranslate nohighlight">\(\Sigma\)</span> is a NxN correlation matrix. This means that the increments of the components of the multivariate Wiener process might be correlated</p></li>
<li><p><span class="math notranslate nohighlight">\({\bf W}_t\)</span> has independent increments, meaning that any <span class="math notranslate nohighlight">\({\bf W}_{t_1}-{\bf W}_{t_2}\)</span>, <span class="math notranslate nohighlight">\({\bf W}_{t_3} - {\bf W}_{t_4}\)</span> where <span class="math notranslate nohighlight">\(t_1 &lt; t_2 &lt; t_3 &lt; t_4\)</span> are independent</p></li>
<li><p>Each of the components of <span class="math notranslate nohighlight">\({\bf W}_t\)</span> has continuous trajectories</p></li>
</ul>
</section>
<section id="ito-s-lemma">
<span id="itos-lemma"></span><h3><span class="section-number">6.3.6. </span>Ito’s Lemma<a class="headerlink" href="#ito-s-lemma" title="Permalink to this heading">#</a></h3>
<p>We have seen that given the trajectory of a deterministic dynamical system, <span class="math notranslate nohighlight">\(y_t = f(t, {X_t})\)</span>, its behaviour over a infinitesimal time-step is described by the following differential equation:</p>
<div class="math notranslate nohighlight">
\[dy = \frac{\partial f}{\partial t}dt + \sum_{n=1}^N \frac{\partial f}{\partial X_{i,t}} d X_{i,t}\]</div>
<p>where <span class="math notranslate nohighlight">\(\{X_t\}\)</span> is a set of deterministic external factors. But what if the external factors are Wiener processes, e.g. in the most simple case
<span class="math notranslate nohighlight">\(f(t,W_t)\)</span>?. We might be tempted to just reuse the previous equation:</p>
<div class="math notranslate nohighlight">
\[dy = \frac{\partial f}{\partial t}dt + \frac{\partial f}{\partial W_t} dW_t\]</div>
<p>however, one of the peculiarities of the Wiener process is that this equation is incomplete, since there is an additional term at this order of the expansion. There are different ways to derive this result,
typically starting from the discrete Wiener process (random walk), but a simple non-rigorous argument can be made by expanding the differential to second order in the Taylor series:</p>
<div class="math notranslate nohighlight">
\[dy = \frac{\partial f}{\partial t}dt + \frac{\partial f}{\partial W_t} dW_t +  \frac{1}{2}\frac{\partial^2 f}{\partial t^2}dt^2 + \frac{1}{2} \frac{\partial^2 f}{\partial W_t^2} dW_t^2 + \frac{\partial^2 f}{\partial t \partial W_t}dt dW_t\]</div>
<p>and take the expected value of this differential conditional to the
filtration at t:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{E}[dy|F_t] = \frac{\partial f}{\partial t}dt + \frac{\partial f}{\partial W_t} \mathbb{E}[dW_t|F_t] +  \frac{1}{2}\frac{\partial^2 f}{\partial t^2}dt^2 \nonumber \\ + \frac{1}{2} \frac{\partial^2 f}{\partial W_t^2} \mathbb{E}[dW_t^2|F_t] + \frac{\partial^2 f}{\partial t \partial W_t}dt \mathbb{E}[dW_t|F_t] 
\end{aligned}\end{split}\]</div>
<p>The key for the argument is that whereas <span class="math notranslate nohighlight">\(\mathbb{E}[dW_t|F_t] = 0\)</span>, we have <span class="math notranslate nohighlight">\(\mathbb{E}[dW_t^2|F_t] = dt\)</span>, getting:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[dy|F_t] = \frac{\partial f}{\partial t}dt + \frac{1}{2}\frac{\partial^2 f}{\partial t^2}dt^2 + \frac{1}{2} \frac{\partial^2 f}{\partial W_t^2} dt\]</div>
<p>so if we keep only terms at first order in the expansion:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[dy|F_t] = (\frac{\partial f}{\partial t} + \frac{1}{2} \frac{\partial^2 f}{\partial W_t^2})dt + O(dt^2)\]</div>
<p>we have an extra term coming from the variance of the Wiener process, whose expected value is linear in <span class="math notranslate nohighlight">\(dt\)</span>. This motivates us to propose the following expression for the differential of a function of the Wiener
process, the so-called Ito’s lemma:</p>
<div class="math notranslate nohighlight">
\[\boxed{
dy = (\frac{\partial f}{\partial t} + \frac{1}{2} \frac{\partial^2 f}{\partial W_t^2})dt + \frac{\partial f}{\partial W_t} dW_t 
}\]</div>
<p>The same argument can be applied for a multivariate Wiener process. Let us for example analyze the case of a function of two correlated Wiener processes, <span class="math notranslate nohighlight">\(y = f(t, W_{1t}, W_{2t})\)</span>. Expanding to second order
in a Taylor series we get:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
dy = \frac{\partial f}{\partial t}dt + \frac{\partial f}{\partial W_{1t}} dW_{1t} +   \frac{\partial f}{\partial W_{2t}} dW_{2t} \nonumber \\ + \frac{1}{2}\frac{\partial^2 f}{\partial t^2}dt^2 + \frac{1}{2} \frac{\partial^2 f}{\partial W_{1t}^2} dW_{1t}^2 + 
\frac{1}{2} \frac{\partial^2 f}{\partial W_{2t}^2} dW_{2t}^2  \nonumber \\
+ \frac{\partial^2 f}{\partial t \partial W_{1t}}dt dW_{1t} + 
\frac{\partial^2 f}{\partial t \partial W_{2t}}dt dW_{2t} +
\frac{\partial^2 f}{\partial W_{1t} \partial W_{2t}}d W_{1t} dW_{2t}
\end{aligned}\end{split}\]</div>
<p>Now, using the expected value argument and keeping only
terms <span class="math notranslate nohighlight">\(O(dt)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
\mathbb{E}[dy|F_t] = \frac{\partial f}{\partial t}dt + \frac{1}{2} \frac{\partial^2 f}{\partial W_{1t}^2} dt + 
\frac{1}{2} \frac{\partial^2 f}{\partial W_{2t}^2} dt +
\frac{\partial^2 f}{\partial W_{1t} \partial W_{2t}} \rho_{12} dt
\end{aligned}\]</div>
<p>where we have used
<span class="math notranslate nohighlight">\(\mathbb{E}[d W_{1t} dW_{2t} |F_t] = \rho_{12} dt\)</span> as discussed in section <a class="reference internal" href="#multivariate-wiener"><span class="std std-ref">Multivariate Wiener process</span></a>. This motivates the expression for the two-dimensional Ito’s lemma:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
d y = (\frac{\partial f}{\partial t} + \frac{1}{2} \frac{\partial^2 f}{\partial W_{1t}^2} + 
\frac{1}{2} \frac{\partial^2 f}{\partial W_{2t}^2}  +
\frac{\partial^2 f}{\partial W_{1t} \partial W_{2t}} \rho_{12}) d t  \nonumber \\ + \frac{\partial f}{\partial W_{1t}} d W_{1t} +  \frac{\partial f}{\partial W_{2t}} d W_{2t}
\end{aligned}\end{split}\]</div>
</section>
<section id="integrals-of-the-wiener-process">
<span id="integrals-wiener"></span><h3><span class="section-number">6.3.7. </span>Integrals of the Wiener process<a class="headerlink" href="#integrals-of-the-wiener-process" title="Permalink to this heading">#</a></h3>
<p>A firs relevant integral of the Wiener process is:</p>
<div class="math notranslate nohighlight">
\[I_{1t} = \int_0^t f(u) dW_u\]</div>
<p>What is the distribution of <span class="math notranslate nohighlight">\(I_{1t}\)</span>? A simple way to deduce it is to discretize the integral and then take again the continuous limit:</p>
<div class="math notranslate nohighlight">
\[I_{1t} = \lim_{\Delta \rightarrow 0} \sum_{i=0}^{N-1} f(u_i) (W_{u_{i+1}} - W_{u_i})\]</div>
<p>where <span class="math notranslate nohighlight">\(\Delta = t / N\)</span>, <span class="math notranslate nohighlight">\(u_i = \Delta i\)</span>. This is a sum of independent Gaussian random variables, since by definition the increments of the Wiener process are independent. Therefore its distribution is itself
Gaussian <a class="footnote-reference brackets" href="#id9" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>. Its mean is easily computed as:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[I_{1t}|F_0] = \lim_{\Delta \rightarrow 0}  \sum_{i=0}^{N-1} f(u_i) \mathbb{E}[(W_{u_{i+1}} - W_{u_i})|F_0] =\]</div>
<p>For the variance, we could simply use the result that the variance of independent variables is the sum of variances, but it is instructive to compute it directly over the discrete expression:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[I_{1t}^2|F_0] = \lim_{\Delta \rightarrow 0} \sum_{i=0}^{N-1}  \sum_{j=0}^{N-1}   f(u_i)  f(u_j) \mathbb{E}[(W_{u_{i+1}} - W_{u_i})(W_{u_{j+1}} - W_{u_j})|F_0]\]</div>
<p>The computation of <span class="math notranslate nohighlight">\(\mathbb{E}[(W_{u_{i+1}} - W_{u_i})(W_{u_{j+1}} - W_{u_j})|F_0]\)</span> needs to consider two cases. When <span class="math notranslate nohighlight">\(i \neq j\)</span> the expectation is zero, since increments of the Wiener process are independent. This is no longer the
case of <span class="math notranslate nohighlight">\(i = j\)</span>, for which we have <span class="math notranslate nohighlight">\(\mathbb{E}[(W_{u_{i+1}} - W_{u_i})^2|F_0] = u_{i+1} - u_i = \Delta\)</span>. Therefore:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[(W_{u_{i+1}} - W_{u_i})(W_{u_{j+1}} - W_{u_j})|F_0] = \delta_{i,j} \Delta\]</div>
<p>Plugging this into the previous equation:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[I_{1t}^2|F_0] = \lim_{\Delta \rightarrow 0} \Delta \sum_{i=0}^{N-1}  f^2(u_i) = \int_0^t f^2(u) du\]</div>
<p>Therefore, the distribution of <span class="math notranslate nohighlight">\(I_{1t}\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[I_{1t} \sim N(0, \int_0^t du f^2(u))\]</div>
<p>Notice that once we have motivated the solution by using a discretization of the equation, we can directly skip it and use the continuous version, for instance:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{E}[I_{1t}^2|F_0] = \int_0^t \int_0^t f(u) f(u') \mathbb{E}[dW_u dW_{u'}] \nonumber \\ = \int_0^t \int_0^t du du' f(u) f(u') \delta(u-u') du = \int_0^t f^2(u) du
\end{aligned}\end{split}\]</div>
<p>Let us now move into the second relevant integral of the
Wiener process, namely:</p>
<div class="math notranslate nohighlight">
\[I_{2t} = \int_0^t du f(W_u)\]</div>
<p>What is the distribution of this random variable? In this case, applying the discretization argument does not provide a direct simple path to figure out the distribution, since we have a sum of correlated variables (e.g. <span class="math notranslate nohighlight">\(f(W_{u_i})\)</span> and <span class="math notranslate nohighlight">\(f(W_{u_j})\)</span> are correlated over the common paths shared by them, say if <span class="math notranslate nohighlight">\(u_i &lt; u_j\)</span>, the path below <span class="math notranslate nohighlight">\(u_i\)</span>). We can use Ito’s lemma to advance our comprehension:</p>
<div class="math notranslate nohighlight">
\[d(u f(W_u)) = u df(W_u) + f(W_u) du\]</div>
<p>which being linear it does not have the Ito’s convexity terms. Essentially we can then use integration by parts:</p>
<div class="math notranslate nohighlight">
\[\int_0^t du f(W_u) = t f(W_t) - \int_0^t u df(W_u)\]</div>
<p>Using again Ito’s lemma, now on <span class="math notranslate nohighlight">\(f(W_u)\)</span>:</p>
<div class="math notranslate nohighlight">
\[df(W_u) = \frac{\partial f}{\partial W_u} dW_u + \frac{1}{2} \frac{\partial^2f }{\partial W_u^2} du\]</div>
<p>we get:</p>
<div class="math notranslate nohighlight">
\[\int_0^t du f(W_u) = t f(W_t) - \int_0^t u \frac{\partial f}{\partial W_u} dW_u - \frac{1}{2} \int_0^t u\frac{\partial^2 f}{\partial W_u^2} du\]</div>
<p>At this point we cannot proceed further in general unless we specify specific functions. For instance, the most simple non trivial case is the linear function <span class="math notranslate nohighlight">\(f(W_u) = W_u\)</span>. Applying this formulation:</p>
<div class="math notranslate nohighlight">
\[\int_0^t W_u du = t W_t - \int_0^t u dW_u = \int_0^t (t - u) dW_u\]</div>
<p>which now we can identify as a specific case of the previous integral, with <span class="math notranslate nohighlight">\(f(u) = t - u\)</span>. The distribution is therefore:</p>
<div class="math notranslate nohighlight">
\[\int_0^t W_u du \sim N(0, \frac{t^3}{3})\]</div>
<p>where we have used <span class="math notranslate nohighlight">\(\int_0^t (t-u)^2 du = \frac{t^3}{3}\)</span></p>
</section>
<section id="simulation-of-the-wiener-process">
<span id="simulation-wiener"></span><h3><span class="section-number">6.3.8. </span>Simulation of the Wiener process<a class="headerlink" href="#simulation-of-the-wiener-process" title="Permalink to this heading">#</a></h3>
<section id="univariate-processes">
<h4><span class="section-number">6.3.8.1. </span>Univariate processes<a class="headerlink" href="#univariate-processes" title="Permalink to this heading">#</a></h4>
<p>We can simulate the Wiener process using a discretization like the Euler scheme for deterministic processes. Again, defining a grid <span class="math notranslate nohighlight">\(t_i = t_0 + \Delta * i\)</span>, <span class="math notranslate nohighlight">\(i = 0, .., N\)</span>, where <span class="math notranslate nohighlight">\(\Delta = \frac{t_N - t_0}{N}\)</span>, we can exploit directly the Wiener process properties to get:</p>
<div class="math notranslate nohighlight">
\[dW_t \rightarrow W_{t_{i+1}} - W_{t_i} \sim N(0, \Delta)\]</div>
<p>So we can simulate numerically the Wiener process simply as:</p>
<div class="math notranslate nohighlight">
\[W_{t_{i+1}} =  W_{t_i} + \sqrt{\Delta} Z\]</div>
<p>where <span class="math notranslate nohighlight">\(Z\)</span> is a standard normal distribution. The following plot simulates the Wiener process numerically for a few different paths:</p>
<figure class="align-default" id="fig-wiener-univariate">
<a class="reference internal image-reference" href="../_images/wiener_univariate.png"><img alt="../_images/wiener_univariate.png" src="../_images/wiener_univariate.png" style="width: 8in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 6.3 </span><span class="caption-text">Simulation of five different paths of the the Wiener process using <span class="math notranslate nohighlight">\(t_0=0\)</span> <span class="math notranslate nohighlight">\(t_N=1\)</span>, <span class="math notranslate nohighlight">\(N = 100\)</span>, <span class="math notranslate nohighlight">\(W_0 = 0\)</span></span><a class="headerlink" href="#fig-wiener-univariate" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Alternatively, we can simulate complete paths of the Wiener process by exploiting its connection to Gaussian processes. Using a Gaussian process framework we get the following results:</p>
<figure class="align-default" id="fig-wiener-gp">
<a class="reference internal image-reference" href="../_images/wiener_gp.png"><img alt="../_images/wiener_gp.png" src="../_images/wiener_gp.png" style="width: 8in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 6.4 </span><span class="caption-text">Simulation of five different paths of the the Wiener process using the same parameters, but sampling from a Gaussian process with a Wiener <span class="math notranslate nohighlight">\(min(t_1, t_2)\)</span> kernel</span><a class="headerlink" href="#fig-wiener-gp" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="multivariate-processes">
<h4><span class="section-number">6.3.8.2. </span>Multivariate processes<a class="headerlink" href="#multivariate-processes" title="Permalink to this heading">#</a></h4>
<p>Let us now simulate a multivariate Wiener process of dimension N. The strategy in this case is to start simulating N independent Wiener processes and then use them to generate the correlated paths. This can
be done using the Cholesky decomposition of the correlation matrix <span class="math notranslate nohighlight">\(\Sigma\)</span>. Since <span class="math notranslate nohighlight">\(\Sigma\)</span> is symmetric and positive define, the decomposition reads:</p>
<div class="math notranslate nohighlight">
\[\Sigma = L L^T\]</div>
<p>where <span class="math notranslate nohighlight">\(L\)</span> is a lower triangular matrix. If we now we have a vector of N uncorrelated Wiener increments
<span class="math notranslate nohighlight">\(d \hat{{\bf W_t}}\)</span>, we can obtain the correlated process by using <span class="math notranslate nohighlight">\(L\)</span> such as <span class="math notranslate nohighlight">\(d {\bf W_t} = L d \hat{{\bf W_t}}\)</span>. We can see that this vector has the distribution of the multivariate Wiener process increment:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{E}[d {\bf W_t} d {\bf W_t}^T] = \mathbb{E}[L d \hat{{\bf W}}_{t} d \hat{{\bf W}}_{t}^T L^T]  \nonumber \\ = L \mathbb{E}[d \hat{{\bf W}}_{t} d \hat{{\bf W}}_{t}^T] L = L L^T dt = \Sigma dt
\end{aligned}\end{split}\]</div>
<p>where we have used <span class="math notranslate nohighlight">\(\mathbb{E}[d \hat{{\bf W}}_{t} d \hat{{\bf W}}_{t}^T] = I dt\)</span> where <span class="math notranslate nohighlight">\(I\)</span> is the identity matrix.</p>
<p>It is illustrative to show the case for a two-dimensional multivariate Wiener process. The correlation matrix reads:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\begin{bmatrix}
1 &amp; \rho  \\ \rho &amp; 1
\end{bmatrix}
\end{aligned}\end{split}\]</div>
<p>The Cholesky lower triangular matrix is simply:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
L = 
\begin{bmatrix}
1 &amp; 0  \\ \rho &amp; \sqrt{1-\rho^2}
\end{bmatrix}
\end{aligned}\end{split}\]</div>
<p>Notice that, as expected:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
L L^T = 
\begin{bmatrix}
1 &amp; 0  \\ \rho &amp; \sqrt{1-\rho^2}
\end{bmatrix}
\begin{bmatrix}
1 &amp; \rho  \\ 0 &amp; \sqrt{1-\rho^2}
\end{bmatrix} = 
\begin{bmatrix}
1 &amp; \rho\\ \rho &amp; 1
\end{bmatrix}
\end{aligned}\end{split}\]</div>
<p>We can also write the decomposition in equations:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
d W_{1t} &amp;=&amp; d \hat{W}_{1t} \nonumber \\
d W_{2t} &amp;=&amp; \rho d \hat{W}_{1t} + \sqrt{1-\rho^2}  d \hat{W}_{2t}
\end{aligned}\end{split}\]</div>
</section>
</section>
</section>
<section id="stochastic-differential-equations">
<span id="sde"></span><h2><span class="section-number">6.4. </span>Stochastic differential equations<a class="headerlink" href="#stochastic-differential-equations" title="Permalink to this heading">#</a></h2>
<p>As pointed out in the introduction to the chapter, after studying the Wiener process now we have the tools to model the dynamics of a random or stochastic process <span class="math notranslate nohighlight">\(X_t\)</span> using stochastic differential equations (SDEs) of the general form:</p>
<div class="math notranslate nohighlight">
\[ d X_t = \mu(X_t, t) dt + \sigma(X_t, t) dW_t\]</div>
<p>which generalizes the deterministic differential equation adding a noise term modelled using the Wiener process. Solving a SDEs means working out the distribution of <span class="math notranslate nohighlight">\(X_t\)</span> at arbitraty times given some initial conditions, which for arbitrary choices of <span class="math notranslate nohighlight">\(\mu(X_t, t)\)</span> and <span class="math notranslate nohighlight">\(\sigma(X_t,t)\)</span> can only be solved numerically, for instance using Monte-Carlo sampling techniques. In the next section we will see particular SDEs that are analytically tractable.</p>
</section>
<section id="the-feynman-kac-theorem">
<span id="feynman-kac"></span><h2><span class="section-number">6.5. </span>The Feynman - Kac Theorem<a class="headerlink" href="#the-feynman-kac-theorem" title="Permalink to this heading">#</a></h2>
<p>The Feynman-Kac theorem provides a link between partial differential equations and stochastic processes, specifically, between certain types of PDEs and expectations of integrals of the Wiener process. Its usefulness extends to subjects like physics and finance, where it is used to introduce statistical interpretations of solutions to deterministic systems.</p>
<p>Specifically, the Feynman - Kac theorem it relates the solution <span class="math notranslate nohighlight">\(u(x,t)\)</span> of the PDE</p>
<div class="math notranslate nohighlight">
\[\frac{\partial u}{\partial t} + \mu(x,t) \frac{\partial u}{\partial x} + \frac{1}{2} \sigma^2(x,t) \frac{\partial^2 u}{\partial x^2} - r(x,t)u = 0\]</div>
<p>with the boundary condition <span class="math notranslate nohighlight">\(u(x,T) = f(x)\)</span>, to the expected value</p>
<div class="math notranslate nohighlight">
\[u(x,t) = \mathbb{E}\left[ e^{-\int_t^T r(X_s,s) ds} f(X_T) \Big| X_t = x \right]\]</div>
<p>where <span class="math notranslate nohighlight">\(X_t\)</span> satisfies the general SDE introduced in section  {ref}` stochastic differential equation (SDE):</p>
<div class="math notranslate nohighlight">
\[ d X_t = \mu(X_t, t) dt + \sigma(X_t, t) dW_t\]</div>
<p>To prove it we use Ito’s lemma to compute the differential of the ansatz <span class="math notranslate nohighlight">\(Y_s = e^{-\int_t^s r(X_v,v) dv} u(X_s, s)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
d Y_s = -r(X_s,s) e^{-\int_t^s r(X_v,v) ds}  u(X_s, s) ds \\ + e^{-\int_t^u r(X_v,v) dv} \left(\frac{\partial u}{\partial s} d s + \frac{\partial u}{\partial  X_s} dX_s + \frac{1}{2} \frac{\partial ^2 u}{\partial X_s^2} \sigma^2(X_s,s) ds\right) 
\end{aligned}\end{split}\]</div>
<p>which we can rewrite as
$<span class="math notranslate nohighlight">\(\begin{aligned}
d Y_s = e^{-\int_t^s r(X_v,v) dv} \frac{\partial u}{\partial  X_s} \sigma(X_s, s) dW_s + \\
 e^{-\int_t^s r(X_v,v) dv} \left(-r(X_s, s) u + \frac{\partial u}{\partial  s} + \frac{\partial u}{\partial  X_s} \mu(X_s,s)+ \frac{1}{2} \frac{\partial ^2 u}{\partial X_s^2} \sigma^2(X_s,s)  \right) ds = \\ 
  e^{-\int_t^s r(X_v,v) dv} \frac{\partial u}{\partial  X_s} \sigma(X_s, s) dW_s
\end{aligned}\)</span>$</p>
<p>where we have used that <span class="math notranslate nohighlight">\(u\)</span> is the solution to the above PDE to make the term in parenthesis zero. We can now integrate this equation from t to T:</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
\int_t^T d Y_s = Y_T - Y_t = \int_t^T e^{-\int_t^s r(X_v,v) dv} \frac{\partial u}{\partial  X_s} \sigma(X_s, s) dW_s 
\end{aligned}\]</div>
<p>If we take now expectations, the right hand term is zero by using the properties of the integral of the Wiener process show in previous sections. Therefore:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[Y_T|X_t = x] = Y_t\]</div>
<p>i.e. <span class="math notranslate nohighlight">\(Y_t\)</span> is a martingale. But this is actually the Feynman - Kac solution if we replace <span class="math notranslate nohighlight">\(Y_t\)</span> by its definition, since <span class="math notranslate nohighlight">\(u(X_T, T) = f(X_T)\)</span>, which completes the proof.</p>
</section>
<section id="basic-stochastic-processes">
<h2><span class="section-number">6.6. </span>Basic stochastic processes<a class="headerlink" href="#basic-stochastic-processes" title="Permalink to this heading">#</a></h2>
<section id="brownian-motion-with-drift">
<h3><span class="section-number">6.6.1. </span>Brownian motion with drift<a class="headerlink" href="#brownian-motion-with-drift" title="Permalink to this heading">#</a></h3>
<p>The most simple stochastic process using the Wiener process as a building block is the Brownian motion with drift:</p>
<div class="math notranslate nohighlight">
\[d S_t = \mu_t dt + \sigma_t d W_t\]</div>
<p>Here we are essentially shifting and rescaling the Wiener process to describe dynamical systems whose stochastic fluctuations have a deterministic non-zero mean <span class="math notranslate nohighlight">\(mu_t\)</span>
(drift) and arbitrary variance, the latter controlled by the so-called volatility <span class="math notranslate nohighlight">\(\sigma_t\)</span>. In practice, this is the process to use for any real application of stochastic calculus to model random time-series, since the simple Wiener process is simply too restrictive to describe
any real process. The name Brownian motion comes actually from the early application of this kind of process to describe the random fluctuations of pollen suspended in water observed by English botanist Robert Brown
in 1827. The actual modelling of this process would be pioneered by Albert Einstein <span id="id3">[<a class="reference internal" href="references.html#id6" title="A. Einstein. Über die von der molekularkinetischen Theorie der Wärme geforderte Bewegung von in ruhenden Flüssigkeiten suspendierten Teilchen. Annalen der Physik, 322(8):549-560, January 1905. doi:10.1002/andp.19053220806.">5</a>]</span>, in one of the four papers published in 1905, his “annus mirabilis”, each of them transformative of its
respective field.</p>
<p>The distribution of <span class="math notranslate nohighlight">\(dS_t\)</span> is easily derived by using the property that multiplying and summing scalars to a Gaussian random variable (<span class="math notranslate nohighlight">\(dW_t\)</span>) yields another Gaussian variable. Hence since <span class="math notranslate nohighlight">\(dW_t \sim N(0, t)\)</span> then
we have:</p>
<div class="math notranslate nohighlight">
\[d S_t \sim N(\mu_t, \sigma_t^2 dt)\]</div>
<p>This stochastic differential equation can be integrated to get the solution for arbitrary times:</p>
<div class="math notranslate nohighlight">
\[S_t = S_0 + \int_0^t \mu_u du + \int_0^t \sigma_u d W_u\]</div>
<p>which again follows a Gaussian distribution:</p>
<div class="math notranslate nohighlight">
\[S_t \sim N(S_0 + \int_0^t \mu_u du, \int_0^t \sigma_u^2 du)\]</div>
<p>where we have used the properties of the stochastic integral discussed in section <a class="reference internal" href="#sde"><span class="std std-ref">Stochastic differential equations</span></a>:</p>
<section id="id4">
<h4><span class="section-number">6.6.1.1. </span>Connection to Gaussian processes<a class="headerlink" href="#id4" title="Permalink to this heading">#</a></h4>
<p>The Brownian motion is also a Gaussian process, since any finite set <span class="math notranslate nohighlight">\(X_{†_1}, X_{t_2}, X_{t_3}, ..., X_{t_n}\)</span> follows a multivariate Gaussian process. The kernel of the Brownian process is simply to
derive:</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
k(t_1, t_2) = \textrm{cov}[X_{t_1}, X_{t_2}] = \mathbb{E}[\int_0^{t_1} \sigma_u dW_u \int_0^{t_2} \sigma_v dW_v] = \int_0^{min(t_1, t_2}  \sigma_u^2 du
\end{aligned}\]</div>
<p>If <span class="math notranslate nohighlight">\(\sigma_t = \sigma\)</span>, i.e. is a constant, we hav simply:</p>
<div class="math notranslate nohighlight">
\[\begin{aligned} k(t_1, t_2) = \sigma^2 \textrm{min}(t_1, t_2)
\end{aligned}\]</div>
<p>which is the kernel of the Wiener process rescaled by
<span class="math notranslate nohighlight">\(\sigma^2\)</span>, as expected given the construction of the Brownian process.</p>
</section>
<section id="simulation">
<h4><span class="section-number">6.6.1.2. </span>Simulation<a class="headerlink" href="#simulation" title="Permalink to this heading">#</a></h4>
<p>Similarly to the Wiener process, simulation can be carried out using a discretization scheme. To improve numerical stability and convergence it is convenient to integrate first the distribution between points in the
simulation grid <span class="math notranslate nohighlight">\(t_0, ..., t_N\)</span>, <span class="math notranslate nohighlight">\(t_{i+1} = t_i + \Delta\)</span>, namely:</p>
<div class="math notranslate nohighlight">
\[S_{t_{i+1}} |F_{t_n} \sim N(S_{t_i} + \int_{t_i}^{t_{i+1}} \mu_u du, \int_{t_i}^{t_{i+1}} \sigma^2_u du)\]</div>
<p>If we have a generator of standard Gaussian random numbers <span class="math notranslate nohighlight">\(Z\)</span>, we can perform the simulation using:</p>
<div class="math notranslate nohighlight">
\[S_{t_{i+1}} = S_{t_i} + \int_{t_i}^{t_{i+1}} \mu_u du + \left(\int_{t_i}^{t_{i+1}} \sigma^2_u du \right)^{1/2} Z\]</div>
<p>If <span class="math notranslate nohighlight">\(\mu_u\)</span> and <span class="math notranslate nohighlight">\(\sigma_u\)</span> are smooth functions of time and <span class="math notranslate nohighlight">\(\Delta\)</span> is small enough such that the variation in the interval is negligible, we can approximate the integrals:</p>
<div class="math notranslate nohighlight">
\[S_{t_{i+1}} |F_{t_n} \sim N(S_{t_i} + \mu_{t_i} \Delta,  \sigma^2_{t_i} \Delta)\]</div>
<p>or simply:</p>
<div class="math notranslate nohighlight">
\[S_{t_{i+1}} = S_{t_i} + \mu_{t_i} \Delta + \sigma_{t_i} \sqrt{\Delta} Z\]</div>
<p>SIMULATION DISCRETE</p>
<p>Alternatively, we can use the connection to Gaussian processes to perform the simulations using standard packages:</p>
<p>SIMULATION GAUSSIAN</p>
</section>
<section id="estimation">
<h4><span class="section-number">6.6.1.3. </span>Estimation<a class="headerlink" href="#estimation" title="Permalink to this heading">#</a></h4>
<p>If we have a set of observations of a process <span class="math notranslate nohighlight">\(D = \{S_{t_0}, ..., S_{t_N}\}\)</span> that we want to model as a Brownian
motion, we need to find the value of the parameters of the process that best fit the data.</p>
<p>Let us consider for simplicity that <span class="math notranslate nohighlight">\(\mu_t = \mu\)</span> and
<span class="math notranslate nohighlight">\(\sigma_u = \sigma\)</span>. As discussed in the context of Bayesian models, a general estimation process models the parameters as random variables that capture our uncertainty about their exact values, which comes from
the fact that the sample is finite, or also in general because the data generation process will not necessarily be the one we are proposing, being models a simplification of reality. In this setup, the best
inference of the parameters is carried out by computing the posterior distribution <span class="math notranslate nohighlight">\(P(\mu, \sigma^2|D, I_P)\)</span> where <span class="math notranslate nohighlight">\(I_P\)</span> represents the prior information about the parameters.</p>
<p>Since as shown in the previous section we can write the process between observations as
<span class="math notranslate nohighlight">\(S_{t_{i+1}} - S_{t_i} = \mu \Delta + \sigma \sqrt{\Delta} Z\)</span> with <span class="math notranslate nohighlight">\(Z\sim N(0,1)\)</span>, this is equivalent to fitting a Bayesian linear regression model om <span class="math notranslate nohighlight">\(S_{t_{i+1}} - S_{t_i}\)</span> with an intercept <span class="math notranslate nohighlight">\(\mu \Delta\)</span> and a noise term <span class="math notranslate nohighlight">\(\epsilon \sim N(0, \sigma^2 \Delta)\)</span>. As
we shown in chapter <a class="reference internal" href="intro_bayesian.html#intro-bayesian"><span class="std std-ref">Introduction to Bayesian Modelling</span></a>, if we model the prior distributions of the parameters as a Normal Inverse Gamma (NIG), meaning that:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    \mu \Delta | \sigma^2 \Delta \sim N(\mu_0 \Delta, \sigma^2 \Delta V_0) \\
    \sigma^2 \Delta \sim IG(\alpha_0, \beta_0)
\end{aligned}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(V_0\)</span>, <span class="math notranslate nohighlight">\(\mu_0\)</span>, <span class="math notranslate nohighlight">\(\alpha_0\)</span>, <span class="math notranslate nohighlight">\(\beta_0\)</span> are parameters that define the prior, this distribution is a conjugate prior of the likelihood:</p>
<div class="math notranslate nohighlight">
\[P(D|\mu \Delta, \sigma^2 \Delta, P_I) = \Pi_{n=1}^N \frac{1}{\sqrt{2\pi\Delta^2\sigma^2}} e^{-\frac{(S_{t_n} - S_{t_{n-1}} - \Delta \mu)^2}{2 \Delta^2 \sigma^2}}\]</div>
<p>Therefore, the posterior is also a NIG, with updated parameters:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    \mu \Delta | D, \sigma^2  \Delta \sim N(\mu_N \Delta, \sigma^2 \Delta V_N) \\
    \sigma^2 \Delta | D \sim IG(\alpha_N, \beta_N)
\end{aligned}\end{split}\]</div>
<p>To get the updated parameters from the general equations, bear in mind that in this representation with only an
intercept our feature-set can be seen as a N dimensional vector <span class="math notranslate nohighlight">\({\bf X} = (1, ..., 1)\)</span>. Therefore:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    V_N = (V_0^{-1} + N)^{-1} \\
   \mu_N \Delta = \mu_0 \Delta \frac{V_N}{V_0} + \frac{1}{V_0^{-1} + N } \sum_{n=1}^N (S_{t_{n}} - S_{t_{n-1}})\\
   \alpha_N = \alpha_0 + N/2 \\
   \beta_N = \beta_0 + \frac{1}{2}\left(\sum_{n=1}^N(S_{t_n}-S_{t_{n-1}})^2 + \mu_0^2 \Delta^2 V_0^{-1} -\mu_N^2 \Delta^2 V_N^{-1} \right)
\end{aligned}\end{split}\]</div>
<p>The predictive marginal for the <span class="math notranslate nohighlight">\(\mu \Delta\)</span> parameter requires integrating out the <span class="math notranslate nohighlight">\(\sigma^2 \Delta\)</span>, resulting in:</p>
<div class="math notranslate nohighlight">
\[\mu \Delta |D \sim T(\mu_N \Delta, \frac{\beta_N}{\alpha_N} V_N, 2 \alpha_N)\]</div>
<p>where <span class="math notranslate nohighlight">\(T\)</span> is the Student’s distribution.</p>
<p>Following our discussion in chapter <a class="reference internal" href="intro_bayesian.html#intro-bayesian"><span class="std std-ref">Introduction to Bayesian Modelling</span></a>, the best estimator of parameters extracted from the posterior in a mean squared error minimizing sense is to compute the mean of the posterior marginals. For <span class="math notranslate nohighlight">\(\sigma^2 \Delta\)</span>, we
have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{E}[\sigma^2 \Delta |D, I_P] = \frac{\beta_N}{\alpha_N-1}=  \frac{\beta_0}{\alpha_0 -1 + N/2} + \nonumber \\ \frac{1}{2 \alpha_0 + N -1}\left(\sum_{n=1}^N(S_{t_n}-S_{t_{n-1}})^2+\mu_0^2 \Delta^2 V_0^{-1} -\mu_N^2 \Delta^2 V_N^{-1} \right) 
\end{aligned}\end{split}\]</div>
<p>whereas for <span class="math notranslate nohighlight">\(\mu\)</span> we have:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[\mu \Delta|D, I_P] = \mu_N \Delta = \mu_0 \Delta \frac{V_0}{V_N} + \frac{1}{V_0^{-1} + N} \sum_{n=1}^N (S_{t_{n}} - S_{t_{n-1}}))\]</div>
<p>A special case deserved our attention: if we take the limit in the prior distribution <span class="math notranslate nohighlight">\(V_0\rightarrow\infty\)</span> and
<span class="math notranslate nohighlight">\(\alpha_0, \beta_0 \rightarrow 0\)</span>, the prior distribution becomes a non-informative or Jeffrey’s prior, with
<span class="math notranslate nohighlight">\(p(\sigma^2 \Delta) \sim 1/(\sigma^2 \Delta)\)</span>, and
<span class="math notranslate nohighlight">\(p(\mu\Delta|\sigma^2\Delta)\)</span> becoming a flat prior. Jeffrey’s prior is considered the best choice to describe non-informative priors of scale parameters as is the case of <span class="math notranslate nohighlight">\(\sigma^2 \Delta\)</span>. In this situation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{E}[\mu \Delta|D, I_P] \rightarrow \mu_{MLE}\Delta = \frac{1}{N} \sum_{n=1}^N (S_{t_{n}} - S_{t_{n-1}})\\ 
\mathbb{E}[\sigma^2 \Delta|D, I_P] \rightarrow \frac{N}{N -1} \sigma_{MLE}^2 \Delta \nonumber \\ \frac{N}{N -1}\left(\frac{1}{N} \sum_{n=1}^N(S_{t_n}-S_{t_{n-1}})^2 -\left(\frac{1}{N} \sum_{n=1}^N (S_{t_{n}} - S_{t_{n-1}})\right)^2 \right) 
\end{aligned}\end{split}\]</div>
<p>which are the maximum likelihood estimators (MLE) of the
mean and the variance of a Gaussian distribution <a class="footnote-reference brackets" href="#id10" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>, which is expected for a non-informative prior since 1) for a Gaussian distribution the mean and the mode coincide, 2) maximizing the posterior with the
non-informative prior is equivalent to maximizing the likelihood. MLE estimators are the standard approach to learn the parameters of stochastic processes in many practical situations, since the Bayesian
estimations might become intractable for more complex processes as the Brownian motion. However, we must keep in mind their limitations, particularly that they neglect any prior information that might be useful in certain setups, and that they throw away the rest of the
information contained in the posterior distribution.</p>
<p>We will see in later chapters that keeping the full posterior distribution can be relevant when we want to incorporate model estimation uncertainty into the inferences that are derived using stochastic processes. For instance, in the presence of a non-negligible
estimation uncertainty of the parameters of our Brownian process, the correct predictive distribution of the next value of the process given its past history is actually given by:</p>
<div class="math notranslate nohighlight">
\[S_{t_{N+1}}|S_{t_N}, D, I_P \sim T(\mu_N \Delta, \frac{\beta_N}{\alpha_N}(1+V_N), 2\alpha_N)\]</div>
<p>If we consider a non-informative prior, this becomes:</p>
<div class="math notranslate nohighlight">
\[S_{t_{N+1}}|S_{t_N}, D, I_P \sim T(\mu_{MLE} \Delta, \sigma_{MLE}^2(1 + \frac{1}{N}), N)\]</div>
<p>This is to be compared with the inference using the MLE estimator, which would essentially plug the point MLE estimator into the Brownian process, yielding:</p>
<div class="math notranslate nohighlight">
\[S_{t_{N+1}}|S_{t_N}, D, I_P \sim N(\mu_{MLE} \Delta, \sigma_{MLE}^2 \Delta)\]</div>
<p>The mean and the mode of both distributions is the same, namely <span class="math notranslate nohighlight">\(\mu_{MLE} \Delta\)</span>, but the Student distribution has fatter tails than the Gaussian distribution, reflecting the extra uncertainty coming from
the estimation error. Only in the case of having a very large data-set <span class="math notranslate nohighlight">\(N\rightarrow \infty\)</span>, both distributions converge, as in this limit the Student distribution becomes a Gaussian <span class="math notranslate nohighlight">\(N(\mu_{MLE} \Delta, \sigma_{MLE} \Delta)\)</span>. Another way of seeing this is to interpret the variance term of the Student distribution as the sum
of two terms: the first one coming from the intrinsic noise of the Brownian process, and the second one, the <span class="math notranslate nohighlight">\(1/N\)</span> correction, coming from the estimation uncertainty.</p>
</section>
<section id="dimensional-analysis">
<h4><span class="section-number">6.6.1.4. </span>Dimensional analysis<a class="headerlink" href="#dimensional-analysis" title="Permalink to this heading">#</a></h4>
<p>In order to reason about stochastic processes it is important to keep track or the units of their parameters. This can also be useful as a consistency check on the estimators derived, for instance. We will use a
brackets notation for dimensional analysis. For the case of the Brownian motion we write:</p>
<div class="math notranslate nohighlight">
\[[dS_t]= [\mu_t] [dt] + [\sigma_t] [dW_t]\]</div>
<p>In here, we know that <span class="math notranslate nohighlight">\([dt]\)</span> has time units, for instance seconds, hours, or days. We denote them generically as <span class="math notranslate nohighlight">\([t]\)</span> The Wiener process being distributed as <span class="math notranslate nohighlight">\(dW_t \sim N(0, dt)\)</span> has therefore units of <span class="math notranslate nohighlight">\([t]^{1/2}\)</span>, i.e. the square root of time. Therefore, the parameters have the following units:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
[\mu_t ] &amp;=&amp; [S] / [t] \\
[\sigma_t] &amp;=&amp; [S] / [t]^{1/2}
\end{aligned}\end{split}\]</div>
<p>where we have denoted generically <span class="math notranslate nohighlight">\([S]\)</span> as the units of
the process. If for instance <span class="math notranslate nohighlight">\(S\)</span> is modelling a price of a financial instrument in a given currency like dollars, the units are dollars: <span class="math notranslate nohighlight">\([dS_t] = \$\)</span>. If additionally we are using seconds as time units, <span class="math notranslate nohighlight">\([t] = s\)</span>. then we have <span class="math notranslate nohighlight">\([\mu_t] = \$ / s\)</span> and <span class="math notranslate nohighlight">\([\sigma_t] = \$ / s^{1/2}\)</span>. We can see that these units are consistent with the Gaussian distribution for the evolution of <span class="math notranslate nohighlight">\(S_t\)</span>: the argument
of the exponential should be a-dimensional. Applying the previous units to the expression we can see that it is indeed the case:</p>
<div class="math notranslate nohighlight">
\[([S] - [\mu_t] [t])^2 / ([\sigma_t]^2 [t]^2) = 1\]</div>
</section>
<section id="applications">
<h4><span class="section-number">6.6.1.5. </span>Applications<a class="headerlink" href="#applications" title="Permalink to this heading">#</a></h4>
<p>The Brownian motion model has plenty of applications. In the financial domain it can be used to model financial or economical indicators for which there is no clear pattern in the time-series beyond potentially a drift to motivate a more sophisticated modelling, i.e. the time-series
is mostly unpredictable. Notice that the Brownian motion has a domain that extends from <span class="math notranslate nohighlight">\((-\infty, \infty)\)</span>, so indicators that have a more restrictive domain might not be suitable for being modelled using the
Brownian motion process. A typical example are prices of financial instruments, which cannot on general grounds be negative. However, there might be exceptions to this rule, for instance if we are modelling
prices in a time - domain such as the probability of reaching negative values is negligible, the model might be well suited. For instance, when modelling intraday prices.</p>
</section>
</section>
<section id="geometric-brownian-motion">
<h3><span class="section-number">6.6.2. </span>Geometric Brownian motion<a class="headerlink" href="#geometric-brownian-motion" title="Permalink to this heading">#</a></h3>
<p>The geometric Brownian motion is a simple extension of the Brownian motion that introduces a constraint in the domain of the process, allowing only positive values. The stochastic differential equation is
the following one:</p>
<div class="math notranslate nohighlight">
\[d S_t = \mu_t S_t dt + \sigma_t S_t d W_t\]</div>
<p>In order to find the distribution of <span class="math notranslate nohighlight">\(S_t\)</span> for arbitrary times, let us apply Ito’s lemma over the function <span class="math notranslate nohighlight">\(f(S_t) = \log S_t\)</span>. Such ansatz is motivated by looking at the deterministic part of the equation,
<span class="math notranslate nohighlight">\(dS_t = \mu_t S_t dt\)</span>, which can be rewritten as
<span class="math notranslate nohighlight">\(d \log S_t = \mu_t dt\)</span>. Coming back to the stochastic differential equation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
d \log S_t = \frac{1}{S_t} d S_t - \frac{1}{2}\frac{1}{S_t^2} \sigma_t^2 S_t^2 dt = \nonumber \\
= (\mu_t - \frac{1}{2} \sigma_t^2)dt + \sigma_t d W_t
\end{aligned}\end{split}\]</div>
<p>This is the stochastic differential equation of the
Brownian motion, which we already now how to integrate from the previous section:</p>
<div class="math notranslate nohighlight">
\[\log S_t = \log S_0 + \int_0^t du (\mu_u - \frac{1}{2} \sigma_u^2) +  \int_0^t \sigma_u d W_u\]</div>
<p>which means that <span class="math notranslate nohighlight">\(\log S_t\)</span> follows a Gaussian distribution</p>
<div class="math notranslate nohighlight">
\[\log S_t \sim N(\log S_0 + \int_0^t du (\mu_u - \frac{1}{2} \sigma_u^2), \int_0^u \sigma_u^2 du)\]</div>
<p>The distribution of <span class="math notranslate nohighlight">\(S_t\)</span> is the well studied Log-normal distribution, which by definition is the distribution of a random variable whose logarithm follows a Gaussian distribution:</p>
<div class="math notranslate nohighlight">
\[S_t \sim LN(\log S_0 + \int_0^t du (\mu_u - \frac{1}{2} \sigma_u^2), \int_0^u \sigma_u^2 du)\]</div>
<p>We can also simply write:</p>
<div class="math notranslate nohighlight">
\[S_t = S_0 \exp(\int_0^t du (\mu_u - \frac{1}{2} \sigma_u^2) +  \int_0^t \sigma_u d W_u)\]</div>
<p>An interesting property of the Log-normal distribution is the simple solution for its moments: $<span class="math notranslate nohighlight">\(\begin{aligned}
X \sim LN(\mu, \sigma^2) \nonumber \\
\mathbb{E}[X^n] = e^{n \mu + \frac{1}{2} n^2 \sigma^2}
\end{aligned}\)</span>$</p>
<p>In particular, the mean of the distribution is
<span class="math notranslate nohighlight">\(\mathbb{E}[X] = e^{\mu + \sigma^2/2}\)</span> which we will use in multiple applications. This means, coming back to the Geometric Brownian Motion, that:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[S_t] = S_0 e^{\int_0^t du (\mu_u - \frac{1}{2} \sigma_u^2) +  \frac{1}{2} \int_0^t \sigma_u^2 du} = S_0 e^{\int_0^t \mu_u du}\]</div>
</section>
<section id="arithmetic-average-of-the-brownian-motion">
<h3><span class="section-number">6.6.3. </span>Arithmetic Average of the Brownian motion<a class="headerlink" href="#arithmetic-average-of-the-brownian-motion" title="Permalink to this heading">#</a></h3>
<p>As the name suggests, this is a derived process from the standard Brownian motion in which the latter is averaged continuously up to a certain time <span class="math notranslate nohighlight">\(T\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
M_t = \frac{1}{T-t}\int_t^T S_u du 
\end{aligned}\]</div>
<p>To derive its distribution we integrate by parts the
Brownian process:</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
M_t = \frac{T S_T - t S_t}{T-t} - \frac{1}{T-t}\int_t^T u d S_u = \nonumber 
= S_t + \int_t^T \frac{T-u}{T-t} dS_u
\end{aligned}\]</div>
<p>The result is the sum of two different parts, one is the
evolution of the Brownian motion up to time <span class="math notranslate nohighlight">\(t\)</span>, where averaging starts. The second averages the variations of the Brownian motion. Since both terms are independent and Gaussian their distribution is simply:</p>
<div class="math notranslate nohighlight">
\[M_t \sim N(S_0, t +  \frac{1}{3} (T-t))\]</div>
<p>where we have used:
$<span class="math notranslate nohighlight">\(\int_t^T (\frac{T-u}{T-t})^2 du = \frac{1}{3} (T-t)\)</span>$</p>
<p>Notice that the variance of the averaged part is smaller than the variance up to t. This makes sense, since taking averages smooths out the random behaviour of
the process.</p>
</section>
<section id="orstein-uhlenbeck-process">
<h3><span class="section-number">6.6.4. </span>Orstein-Uhlenbeck process<a class="headerlink" href="#orstein-uhlenbeck-process" title="Permalink to this heading">#</a></h3>
<p>The Orstein - Uhlenbeck process is an extension of the Brownian motion process in which the drift term is modified to prevent large deviations from the mean <span class="math notranslate nohighlight">\(\mu\)</span>. This is achieved by making the drift penalize those
deviations:</p>
<div class="math notranslate nohighlight">
\[d S_t = \theta (S_t - \mu) dt + \sigma_t d W_t\]</div>
<p>The strength of the penalty is controlled by a parameter <span class="math notranslate nohighlight">\(\theta \leq 0\)</span>, which is called the mean reversion speed: the larger this parameter, the faster the process <span class="math notranslate nohighlight">\(S_t\)</span> reverts to the mean. Such property of mean
reversion is not unique to the Orstein - Uhlenbeck process, but the latter is probably the most simple way to achieve this effect in a stochastic differential equation, allowing for the computation of
closed-form solutions for the probability distribution of the process. To derive it, we use the following ansatz:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
d(e^{-\theta t} S_t) = -\theta e^{-\theta t} S_t dt + e^{-\theta t} dS_t = \nonumber \\
e^{-\theta t}(-\mu dt + \sigma_t dW_t)
\end{aligned}\end{split}\]</div>
<p>Integrating this equation:</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
e^{-\theta t} S_t - S_0 = - (e^{-\theta t} - 1) \mu + \int_0^t e^{-\theta u} \sigma_u dW_u
\end{aligned}\]</div>
<p>Finally:</p>
<div class="math notranslate nohighlight">
\[S_t = S_0 e^{-\theta t} + \mu (1 - e^{-\theta t}) + \int_0^t e^{\theta (t-u)} \sigma_u dW_u\]</div>
<section id="other-mean-reverting-processes">
<h4><span class="section-number">6.6.4.1. </span>Other mean reverting processes<a class="headerlink" href="#other-mean-reverting-processes" title="Permalink to this heading">#</a></h4>
<p>CIR $<span class="math notranslate nohighlight">\(d S_t = \theta (S_t - \mu_t) dt + \sigma_t S_t^{1/2} d W_t\)</span>$</p>
</section>
</section>
<section id="brownian-bridge">
<h3><span class="section-number">6.6.5. </span>Brownian bridge<a class="headerlink" href="#brownian-bridge" title="Permalink to this heading">#</a></h3>
<p>Another simple extension of the Wiener process is the Brownian bridge, that we denote as <span class="math notranslate nohighlight">\(B_t\)</span>. It defines a stochastic process within a time interval <span class="math notranslate nohighlight">\([0,T]\)</span> that behaves as a Wiener process with the exception of
having an extra constraint, namely that <span class="math notranslate nohighlight">\(B_T = 0\)</span> (in addition to <span class="math notranslate nohighlight">\(B_0 = 0\)</span> as in the Wiener process). Such process can be constructed from a single Wiener process <span class="math notranslate nohighlight">\(W_t\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[B_t =  W_t - \frac{t}{T} W_T\]</div>
<p>In order to derive the distribution of <span class="math notranslate nohighlight">\(B_t\)</span>, we decompose it in two independent Gaussian variables:</p>
<div class="math notranslate nohighlight">
\[B_t =  W_t - \frac{t}{T} (W_T - W_t + W_t) = (1-\frac{t}{T})W_t - \frac{t}{T}(W_T - W_t)\]</div>
<p>where the two terms of the expression are independent by using the property that non overlapping differences of Wiener processes are independent. This is therefore the distribution of two independent Gaussian variables, which we know is also Gaussian:</p>
<div class="math notranslate nohighlight">
\[B_t \sim N(0, t(1 - \frac{t}{T}))\]</div>
<p>where we have used:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{E}[(1-\frac{t}{T})W_t|F_0]  - \mathbb{E}[\frac{t}{T}(W_T - W_t)|F_0] = 0 \\
\mathbb{E}[((1-\frac{t}{T})W_t)^2|F_0]  + \mathbb{E}[(\frac{t}{T}(W_T - W_t))^2|F_0] = \nonumber \\ (1-\frac{t}{T})^2 t + (\frac{t}{T})^2 (T-t) = t(1 - \frac{t}{T})
\end{aligned}\end{split}\]</div>
<p>The result fits well our expectation of such process:
for <span class="math notranslate nohighlight">\(t = 0\)</span> and <span class="math notranslate nohighlight">\(t = T\)</span>, the process has no variance, since those points correspond to the constraints where the process is set to zero. Given those constraints, and the symmetry of the Brownian bridge with respect
to the transformation <span class="math notranslate nohighlight">\(t \rightarrow T-t\)</span>, we expect the maximum variance to be at the half-point <span class="math notranslate nohighlight">\(T/2\)</span>, which is indeed the maximum of the function <span class="math notranslate nohighlight">\(t(1-\frac{t}{T})\)</span></p>
<section id="id6">
<h4><span class="section-number">6.6.5.1. </span>Connection to Gaussian processes<a class="headerlink" href="#id6" title="Permalink to this heading">#</a></h4>
<p>The Brownian bridge is also a Gaussian process, with the following kernel:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
k(t_1,t_2) = \textrm{cov}[B_{t_1} B_{t_2}] = \mathbb{E}[(W_{t_1} - \frac{t_1}{T} W_T])(W_{t_2} - \frac{t_2}{T} W_T)] = \nonumber \\ \textrm{min}(t_1, t_2) -  \frac{t_2}{T} t_1 -  \frac{t_2}{T} t_1 + \frac{t_1 t_2}{T} = \textrm{min}(t_1, t_2) - \frac{t_1 t_2}{T}
\end{aligned}\end{split}\]</div>
</section>
<section id="id7">
<h4><span class="section-number">6.6.5.2. </span>Simulation<a class="headerlink" href="#id7" title="Permalink to this heading">#</a></h4>
<p>TO DO</p>
</section>
<section id="id8">
<h4><span class="section-number">6.6.5.3. </span>Applications<a class="headerlink" href="#id8" title="Permalink to this heading">#</a></h4>
<p>Brownian bridges or more realistic but similarly constructed processes (e.g. using the Brownian motion as the primitive instead of the Wiener process, or specifying fixed but non-zero boundaries), can be used to
model situations where there is no uncertainty about the final value of a stochastic process. This is for instance the case of financial instruments like bonds, in particular zero-coupon bonds, which are bonds
that simply return their principal at maturity without paying a coupon. They are also known to be more suitable for simulation than the underlying Wiener process, which can be written as a function of the
Brownian bridge simply as:</p>
<div class="math notranslate nohighlight">
\[W_t = B_t + \frac{t}{T} W_T\]</div>
<p>By simulating the Brownian bridge and a single standard Gaussian variable <span class="math notranslate nohighlight">\(Z\sim N(0,1)\)</span>, so that <span class="math notranslate nohighlight">\(W_T = \sqrt{T}Z\)</span>, we can generate samples of the Wiener process.</p>
</section>
</section>
</section>
<section id="jump-processes">
<h2><span class="section-number">6.7. </span>Jump processes<a class="headerlink" href="#jump-processes" title="Permalink to this heading">#</a></h2>
<p>WIP</p>
</section>
<section id="exercises">
<h2><span class="section-number">6.8. </span>Exercises<a class="headerlink" href="#exercises" title="Permalink to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Derive the Crank - Nikolson scheme discretization of the inflation targeting model. Simulate it numerically and compare with the exact solution</p></li>
<li><p>Use Ito’s lemma to derive the differential of:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f(W_t) = W_t^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(f(W_t) = t W_t\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(f(W_t) = \exp(W_t)\)</span></p></li>
</ul>
</li>
</ol>
<hr class="footnotes docutils" />
<aside class="footnote brackets" id="id9" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">1</a><span class="fn-bracket">]</span></span>
<p>The demonstration is relatively straight-forward by computing the characteristic function of a sum of independent random Gaussian variables</p>
</aside>
<aside class="footnote brackets" id="id10" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">2</a><span class="fn-bracket">]</span></span>
<p>Actually the bias corrected one, which can be achieved by the factor <span class="math notranslate nohighlight">\(N/(N-1)\)</span> as it is well known that the MLE estimator of the variance of a Gaussian distribution is biased, i.e. <span class="math notranslate nohighlight">\(\mathbb{E}[\sigma_{MLE}^2] = \frac{N-1}{N} \sigma^2\)</span>. The Bayesian derivation in this regard is more consistent than the MLE estimation. Notice that the MLE estimator actually corresponds to <span class="math notranslate nohighlight">\(\beta_N/\alpha_N\)</span> in the case of a non-informative prior</p>
</aside>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./markdown"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="intro_causal.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">5. </span>Causal inference</p>
      </div>
    </a>
    <a class="right-next"
       href="stochastic_optimal_control.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">7. </span>Stochastic optimal control</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-modelling-dynamical-systems">6.1. Introduction: modelling dynamical systems</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deterministic-dynamical-systems">6.2. Deterministic dynamical systems</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-newton-s-laws">6.2.1. Example: Newton’s Laws</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-simple-inflation-targeting-model">6.2.2. Example: simple inflation targeting model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-free-fall-with-friction">6.2.3. Example: Free fall with friction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-solutions-of-dynamical-systems">6.2.4. Numerical solutions of dynamical systems</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-wiener-process">6.3. The Wiener Process</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definition">6.3.1. Definition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-gaussian-processes">6.3.2. Connection to Gaussian Processes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#filtrations-and-the-martingale-property">6.3.3. Filtrations and the Martingale Property</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-tower-law">6.3.4. The Tower Law</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-wiener-process">6.3.5. Multivariate Wiener process</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ito-s-lemma">6.3.6. Ito’s Lemma</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#integrals-of-the-wiener-process">6.3.7. Integrals of the Wiener process</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simulation-of-the-wiener-process">6.3.8. Simulation of the Wiener process</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#univariate-processes">6.3.8.1. Univariate processes</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-processes">6.3.8.2. Multivariate processes</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-differential-equations">6.4. Stochastic differential equations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-feynman-kac-theorem">6.5. The Feynman - Kac Theorem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-stochastic-processes">6.6. Basic stochastic processes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#brownian-motion-with-drift">6.6.1. Brownian motion with drift</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">6.6.1.1. Connection to Gaussian processes</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#simulation">6.6.1.2. Simulation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation">6.6.1.3. Estimation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensional-analysis">6.6.1.4. Dimensional analysis</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#applications">6.6.1.5. Applications</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#geometric-brownian-motion">6.6.2. Geometric Brownian motion</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#arithmetic-average-of-the-brownian-motion">6.6.3. Arithmetic Average of the Brownian motion</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#orstein-uhlenbeck-process">6.6.4. Orstein-Uhlenbeck process</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#other-mean-reverting-processes">6.6.4.1. Other mean reverting processes</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#brownian-bridge">6.6.5. Brownian bridge</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">6.6.5.1. Connection to Gaussian processes</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">6.6.5.2. Simulation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">6.6.5.3. Applications</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#jump-processes">6.7. Jump processes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">6.8. Exercises</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Javier Sabio González
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>