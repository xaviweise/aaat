

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>10. Generative Artificial Intelligence &#8212; Advanced Analytics and Algorithmic Trading</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'markdown/generative_ai';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="11. Execution fundamentals" href="execution_fundamentals.html" />
    <link rel="prev" title="9. Data-driven methods" href="data_driven_methods.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../welcome.html">
  
  
  
  
  
  
    <p class="title logo__title">Advanced Analytics and Algorithmic Trading</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../welcome.html">
                    Welcome
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="releases.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="dedication.html">Dedication</a></li>
<li class="toctree-l1"><a class="reference internal" href="preface.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="symbollist.html">Symbols</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Financial Markets Fundamentals</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="intro_financial_markets.html">1. Financial Markets</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro_financial_instruments.html">2. Mechanics of Financial Instruments</a></li>
<li class="toctree-l1"><a class="reference internal" href="market_microstructure.html">3. Market microstructure</a></li>
<li class="toctree-l1"><a class="reference internal" href="algorithmic_trading.html">4. Algorithmic Trading</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Financial Modelling Fundamentals</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="intro_bayesian.html">5. Introduction to Bayesian Modelling</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro_causal.html">6. Causal inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="stochastic_calculus.html">7. Stochastic Calculus</a></li>
<li class="toctree-l1"><a class="reference internal" href="stochastic_optimal_control.html">8. Stochastic optimal control</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_driven_methods.html">9. Data-driven methods</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">10. Generative Artificial Intelligence</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Execution Algorithms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="execution_fundamentals.html">11. Execution fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="lob_models.html">12. Modelling the Limit Order Book</a></li>
<li class="toctree-l1"><a class="reference internal" href="almgren_chriss.html">13. The Almgren - Chriss Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="execution_tactics.html">14. Execution tactics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Market Making Algorithms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="market_making_fundamentals.html">15. Market Making fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="fair_price_estimation.html">16. Fair price estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="liquidity_modelling.html">17. Liquidity modelling</a></li>
<li class="toctree-l1"><a class="reference internal" href="rfq_models.html">18. Modelling the request for quote process</a></li>
<li class="toctree-l1"><a class="reference internal" href="avellaneda_stoikov.html">19. The Avellaneda and Stoikov Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="enriching_avellaneda.html">20. Enriching the Avellaneda and Stoikov Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="hedging.html">21. Hedging strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="market_making_rl.html">22. Reinforcement Learning and Market Making</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Investment Algorithms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="quant_investment_fundamentals.html">23. Quantitative investment fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="mean_reversion_strategies.html">24. Mean reversion strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="trend_following.html">25. Trend following strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="arbitrage.html">26. Arbitrage Strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="factor_investing.html">27. Factor Investing</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimal_portfolios.html">28. Optimal portfolios</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Glossary and References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="glossary.html">27. Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="references.html">References</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Notebooks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../notebooks/stochastic_calculus.html">Stochastic Calculus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/market_microstructure.html">Market Microstructure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/fair_price_estimation.html">Fair Price Estimation</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/xaviweise/aaat" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/xaviweise/aaat/issues/new?title=Issue%20on%20page%20%2Fmarkdown/generative_ai.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/markdown/generative_ai.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Generative Artificial Intelligence</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-gen-ai">10.1. What is Gen AI?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#artificial-intelligence">10.1.1. Artificial Intelligence</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-models">10.1.2. Generative models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gen-ai-models">10.1.3. Gen AI models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#large-language-models">10.2. Large Language Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#traditional-language-modelling">10.2.1. Traditional language modelling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-transformer-architecture">10.2.2. The Transformer Architecture</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decoder-only-models">10.2.3. Decoder-only models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning-models-with-rlhl">10.2.4. Fine-tuning models with RLHL</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reasoning-models">10.2.5. Reasoning models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#working-with-large-language-models">10.3. Working with Large Language Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prompt-engineering">10.3.1. Prompt Engineering</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chain-of-thought">10.3.2. Chain of Thought</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#retrieval-augmented-generation-rag">10.3.3. Retrieval - Augmented Generation (RAG)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#agentic-systems">10.3.4. Agentic systems</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="generative-artificial-intelligence">
<h1><span class="section-number">10. </span>Generative Artificial Intelligence<a class="headerlink" href="#generative-artificial-intelligence" title="Permalink to this heading">#</a></h1>
<p>Since the release in 2022 of Chat GPT, the field of Artificial Intelligence has been radically transformed. Chat GPT was based on the third version of the Generalized Pre-trained Transformer (GPT) model, a so-called large language model, with surprising capabilities for reasoning and conversation in a creative way. Since then, new versions of the model have been released, as well as competition in the space, with other providers catching up on the capabilities. Moreover, multi-modal so-called foundational models that extend their understanding beyond language to images and video, blending them. For instance, models like Stable Diffusion are able to convert text into highly realistic (although not necessarily consistent) images. This type of models, which mostly share a common neural-network architecture, the Transformer, that has boosted its capabilities together with a large number of parameters and training data, form the backbone of the emerging field of Generative Artificial Intelligence (Gen AI). In order to understand them and make an efficient use of them, particularly in business applications, it is convenient to understand their fundamentals.</p>
<section id="what-is-gen-ai">
<h2><span class="section-number">10.1. </span>What is Gen AI?<a class="headerlink" href="#what-is-gen-ai" title="Permalink to this heading">#</a></h2>
<p>In simple conceptual terms, Gen AI consists on the application of generative probabilistic models to the domain of Artificial Intelligence. Let us understand each of these concepts to shed light on the term.</p>
<section id="artificial-intelligence">
<h3><span class="section-number">10.1.1. </span>Artificial Intelligence<a class="headerlink" href="#artificial-intelligence" title="Permalink to this heading">#</a></h3>
<p>Considered the father of the field of Artificial Intelligence, the computer scientist John McCarthy coined the term and provided a simple but compelling definition: Artificial Intelligence is <em>“the Science and Engineering of Making Intelligent Machines”</em>. What what does it mean intelligence? In general, here we are referring to human capabilities for learning, reasoning, interpreting, understanding and adapting.</p>
<p>It is key here to focus on the idea of “human intelligence”. Human beings are the benchmark for Artificial Intelligence, as the famous test proposed by Alan Turing in 1949, the imitation game, clearly showcases. Here, a human evaluator tries to differentiate the human and the machine in a text transcript of a natural language conversation between them. The machine passes the test if the evaluator cannot tell them apart with certain confidence. There are naturally many domains where human intelligence is quite limited, for instance when working with raw datasets trying to generate statistical inferences. A Turing test where one of the participants is able to extract patterns from a billion lines dataset will provide a clear hit to the evaluator on who is the machine. And despite this relatively obvious fact, during the last decades the field of Machine Learning has been almost synonymous of Artificial Intelligence.</p>
<p>This does not mean that Machine Learning has not been able to excel at some tasks that can be clearly labelled as human intelligence. The most impressive ones were propelled, though, by the use of artificial neural networks, which from the second decade of this century, thanks to improved algorithms for learning without over-fitting, and the availability of data and computing power, all blended together in the field of Deep Learning, started to pass or even beat human beings in specific tasks like object recognition in images, or playing games board games like Go. However, most of the applications of Machine Learning, at least in the business environment, have been on improving upon traditional statistical models, which are precisely good at learning patterns from large datasets. In these tasks, these systems can easily show super-human capabilities.</p>
<p>Despite the advances of Deep Learning in some human intelligence tasks, these systems where considered too specialized in their capabilities. Yes, they could excel humans in recognizing hand-written characters, but such system trained to do so would be later incapable of generalize the knowledge to even similar tasks like object recognition in images. In that sense, we can talk about <em>narrow</em> intelligence, limited to a specific task, and <em>general</em> intelligence, which can extrapolate knowledge acquired to perform a set of tasks to others for which it has not been specifically trained. The problem is that such capability of extrapolation is seemingly key even when Artificial Intelligence systems are to be applied in specific domains of knowledge. That was one of the learnings after the rise and fall of so-called expert AI systems in 80s of the previous century. These systems were based on a large corpus of rules compiled from experts in a specific field, the idea being that these systems could be used to reason about new problems in those domains. But they ended up becoming too complex to maintain and lacking those extrapolation capabilities that are key for the necessary bit of creativity needed to tackle new problems.</p>
<p>In some sense, the philosophical theory of knowledge was already pointing out the problem since the Ancient Greece: to learn is to generate some abstractions in our minds, that extract regularities from what our senses perceive. Reasoning, then, is the combination of those abstractions to build further inferences or perform deductions. Neurologists now understand that human brains generate such abstractions by building or strengthening connections between neurons in our brains when exposed to regularities in perceptions. It does not come then as a surprise that the most successful machine models were built upon neural networks, that resemble at a high level the workings of the brain. They seemingly build their own abstractions as they are trained on a large number of data points, and have a sufficient scale to learn complex patterns coupled with mechanism for regularization, i.e. avoid to learn so well the patterns of the training data that will later not generalize to unseen datasets.</p>
<p>During the second decade of this century, the combination of neural networks increasingly large trained on increasing large datasets started to provide surprising examples of a sort of emergent behaviour in terms of capabilities: a sort of threshold in data and parameters from which capabilities would become akin or superior to humans. Such observations however did not anticipate that such neural network systems, when trained upon language datasets with the seemingly simple task of predicting the next work given the previous ones, would suddenly become shockingly good at probably the most central skills considered in the domain of human intelligence: general language interpretation and reasoning.</p>
<p>It was not, though, without effort. The field of natural language processing (NLP) had been trying during years to build systems capable of generating realistic conversations by training statistical models to sequences of words. One of the main challenges was that those systems would quickly forget the context of the conversation, making sentences incoherent in their syntax and conversations that would jump randomly across topics. With the boom of Deep Learning, specific neural network architectures were built to try to address this shortcoming, such as <em>recurrent neural networks</em> and <em>long short term memory</em> (LSTM) cells. They improved upon previous systems, but did not provide major breakthroughs. The tipping point happened with the release of the paper “Attention is All You Need”, by researchers from Google, in 2017. They introduced the Transformer architecture, which could have been easily seen as yet another proposal to try to fix the memory issue of sequential neural networks. However, conveniently fine-tuned, when applied to a vast corpus of text to learn a massive amount of neural weights, researchers witnessed an emerging behaviour in reasoning and understanding that still surprises any casual user of them.</p>
<p>With the advent of such <em>Large Language Models (LMMs)</em>, the field of Artificial Intelligence has come back to its roots of building systems that target human capabilities.  Naturally, this has raised the interest of corporations across the world that have a lot of interest in gaining understanding of these models to automate and optimize tasks that few years back were thought exclusively to belong to the human domain.</p>
</section>
<section id="generative-models">
<h3><span class="section-number">10.1.2. </span>Generative models<a class="headerlink" href="#generative-models" title="Permalink to this heading">#</a></h3>
<p>In the chapter on Bayesian theory, we already introduced the concept of generative probabilistic models in contrast to discriminative probabilistic models. At the heart is the way we try to use probabilistic models to perform inferences from datasets. Discriminative models focus on understanding the distribution of a subset of variables conditional to the other, i.e. the domain of so-called Supervised Learning using the Machine Learning terminology. In simple terms, if we have a dataset composed of two variables <span class="math notranslate nohighlight">\(X, Y\)</span>, a discriminative model seeks to understand the conditional distribution:</p>
<div class="math notranslate nohighlight">
\[P(Y|X)\]</div>
<p>A generative model, however, tries to model the full dataset, akin to understanding the full data generation process, hence the name. It models the joint probability distribution:</p>
<div class="math notranslate nohighlight">
\[P(X,Y)\]</div>
<p>A generative model is more general that a discriminative model, since correct modelling of the joint distribution allows us to derive the distribution of the discriminative model, by virtue of use of the product rule of probability:</p>
<div class="math notranslate nohighlight">
\[P(Y|X) = \frac{P(X,Y)}{P(Y)}\]</div>
<p>A point to notice is that were we only interested in computing <span class="math notranslate nohighlight">\(P(Y|X)\)</span>, it might be more efficient to learn this directly distribution using any of the multiple available statistical or Machine Learning supervised models. Modelling the joint distribution is more complex and therefore the quality of the inferences of <span class="math notranslate nohighlight">\(P(Y|X)\)</span> might suffer. However, having the full generative model allows us to solve a wider range of inferences than the discriminative model.</p>
<p>The previous argument can also work in reverse: if we learn all the relevant discriminative distributions separately, we have knowledge of the generative model, since:</p>
<div class="math notranslate nohighlight">
\[P(X, Y) = P(Y|X) P(X)\]</div>
<p>We can generalize this result to a sequence of <span class="math notranslate nohighlight">\(N\)</span> variables:</p>
<div class="math notranslate nohighlight">
\[P(X_1, ..., X_N) = P(X_1| X_2, .., X_N) P(X_2| X_3, ..., X_N) ... P(X_{N-1}|X_N) P(X_N)\]</div>
<p>where we have simply applied the product rule sequentially.</p>
<p>This structure already provides a hint on the connection between generative models and Gen AI models, particularly Large Language Models, mentioned in the previous section. Statistical language models try to compute the distribution of words (or tokens, which are more granular building blocks to decompose language that perform better in practical tasks). For instance, given the sentence “the cat had blue …”, such models try to estimate the probability of any existing next word, conditional to the previous words, for instance:</p>
<div class="math notranslate nohighlight">
\[P(\text{&quot;eyes&quot;}| \text{&quot;the&quot;}, \text{&quot;cat&quot;}, \text{&quot;had&quot;}, \text{&quot;blue&quot;})\]</div>
<p>In this case, a well estimated model needs to be able to compute such probability for any word in the English vocabulary. Of course a useful model would not be specifically trained to compute probabilities for this case. This means that it should be able to compute the probability of any other sequence, in particular <span class="math notranslate nohighlight">\(P(\text{&quot;blue&quot;}|\text{&quot;the&quot;}, \text{&quot;cat&quot;}, \text{&quot;had&quot;})\)</span>, <span class="math notranslate nohighlight">\(P(\text{&quot;had&quot;}|\text{&quot;the&quot;}, \text{&quot;cat&quot;})\)</span>, <span class="math notranslate nohighlight">\(P(\text{&quot;cat&quot;}| \text{&quot;the&quot;})\)</span> and <span class="math notranslate nohighlight">\(P(\text{&quot;the&quot;})\)</span>. But using the previous equation this means that this model must be able to compute:</p>
<div class="math notranslate nohighlight">
\[P(\text{&quot;the&quot;}, \text{&quot;cat&quot;}, \text{&quot;had&quot;}, \text{&quot;blue&quot;}, \text{&quot;eyes&quot;})\]</div>
<p>i.e. is a generative model for language.</p>
<p>Having a generative model for human language opens up the possibility for multiple tasks. In particular, we can use it to generate language given some context of <em>prompt</em>. We can have an expert model that sticks to most likely facts by generating sequences taking the most likely word according to the model. Or we can build creative systems that sample the distribution according to the computed probabilities. A common practice to control the degree of creativity of these models is to transform the probability distribution for the next word or token into a Gibbs distribution. The motivation is the transformation:</p>
<div class="math notranslate nohighlight">
\[P_i = e^{\log P_i} \rightarrow \hat{P}_i = \frac{e^{-s_i /T}}{Z}\]</div>
<p>where we have defined the scores <span class="math notranslate nohighlight">\(s_i = -\log P_i\)</span> per word <span class="math notranslate nohighlight">\(i\)</span> in the vocabulary, <span class="math notranslate nohighlight">\(T\)</span> is an external parameter called the Temperature in analogy to statistical mechanics, where the Gibbs distribution was introduced, and <span class="math notranslate nohighlight">\(Z = \sum_i e^{-s_i /T}\)</span> is a normalization constant, also called the partition function using the terminology of statistical mechanics. An insight into the Gibbs distribution is that it is the distribution that maximizes entropy when the average score is a constraint: let us assume unknown probabilities <span class="math notranslate nohighlight">\(P_i\)</span> for each word <span class="math notranslate nohighlight">\(i\)</span>, but the average score is known: <span class="math notranslate nohighlight">\(\bar{s} = \sum_i p_i s_i\)</span>. The constrained optimization problem can be written using the Lagrangian:</p>
<div class="math notranslate nohighlight">
\[{\mathcal L} = \sum_i p_i \log p_i + \lambda_1 (\sum_i p_i - 1) + \lambda_2 (\sum_i p_i s_i - \bar{s})\]</div>
<p>where we also have the normalization constraint (all probabilities sum to one), and <span class="math notranslate nohighlight">\(\lambda_1, \lambda_2\)</span> are Lagrange multipliers. The extreme of this function (which can be shown to be a maximum) is given by:</p>
<div class="math notranslate nohighlight">
\[1 + \log \hat{p}_i + \lambda_1 + \lambda_2 s_i = 0 \rightarrow \hat{p}_i = e^{-(1 + \lambda_1 + \lambda_2 s_i)}\]</div>
<p>Applying now the normalization constraint we get the Gibbs distribution:</p>
<div class="math notranslate nohighlight">
\[\hat{p}_i = \frac{e^{-\lambda_2 s_i}}{\sum_i e^{-\lambda_2 s_i }} \equiv \frac{e^{-s_i / T}}{Z}\]</div>
<p>where we have identified the temperature <span class="math notranslate nohighlight">\(T\)</span> as the inverse of the Lagrange multiplier <span class="math notranslate nohighlight">\(\lambda_2\)</span>. By varying the temperature, we can control the degree of creativity:</p>
<ul class="simple">
<li><p>For <span class="math notranslate nohighlight">\(T\rightarrow 0\)</span>, all terms tend to zero but the one with the smaller score <span class="math notranslate nohighlight">\(s_i\)</span> is the slowest, therefore we get <span class="math notranslate nohighlight">\(\hat{p_i} \rightarrow \delta_{i, i_{max}}\)</span>, where <span class="math notranslate nohighlight">\(i_{max}\)</span> is the word with the lowest score, which is the largest probability since <span class="math notranslate nohighlight">\(s_i = -\log p_i\)</span>. In this case the model is less creative, sticking to the most likely next words</p></li>
<li><p>For <span class="math notranslate nohighlight">\(T = 1\)</span>, we recover the original distribution of probabilities <span class="math notranslate nohighlight">\(\hat{p}_i = p_i\)</span></p></li>
<li><p>For <span class="math notranslate nohighlight">\(T \rightarrow \infty\)</span> the exponential terms tend to <span class="math notranslate nohighlight">\(1\)</span>, so the distribution is uniform over all words once normalization is accounted. In this case, the model is random over the distribution of all possible words.</p></li>
</ul>
</section>
<section id="gen-ai-models">
<h3><span class="section-number">10.1.3. </span>Gen AI models<a class="headerlink" href="#gen-ai-models" title="Permalink to this heading">#</a></h3>
</section>
</section>
<section id="large-language-models">
<h2><span class="section-number">10.2. </span>Large Language Models<a class="headerlink" href="#large-language-models" title="Permalink to this heading">#</a></h2>
<section id="traditional-language-modelling">
<h3><span class="section-number">10.2.1. </span>Traditional language modelling<a class="headerlink" href="#traditional-language-modelling" title="Permalink to this heading">#</a></h3>
<p>N-grams, HMMs, first neural network architectures</p>
</section>
<section id="the-transformer-architecture">
<h3><span class="section-number">10.2.2. </span>The Transformer Architecture<a class="headerlink" href="#the-transformer-architecture" title="Permalink to this heading">#</a></h3>
</section>
<section id="decoder-only-models">
<h3><span class="section-number">10.2.3. </span>Decoder-only models<a class="headerlink" href="#decoder-only-models" title="Permalink to this heading">#</a></h3>
</section>
<section id="fine-tuning-models-with-rlhl">
<h3><span class="section-number">10.2.4. </span>Fine-tuning models with RLHL<a class="headerlink" href="#fine-tuning-models-with-rlhl" title="Permalink to this heading">#</a></h3>
</section>
<section id="reasoning-models">
<h3><span class="section-number">10.2.5. </span>Reasoning models<a class="headerlink" href="#reasoning-models" title="Permalink to this heading">#</a></h3>
</section>
</section>
<section id="working-with-large-language-models">
<h2><span class="section-number">10.3. </span>Working with Large Language Models<a class="headerlink" href="#working-with-large-language-models" title="Permalink to this heading">#</a></h2>
<section id="prompt-engineering">
<h3><span class="section-number">10.3.1. </span>Prompt Engineering<a class="headerlink" href="#prompt-engineering" title="Permalink to this heading">#</a></h3>
</section>
<section id="chain-of-thought">
<h3><span class="section-number">10.3.2. </span>Chain of Thought<a class="headerlink" href="#chain-of-thought" title="Permalink to this heading">#</a></h3>
</section>
<section id="retrieval-augmented-generation-rag">
<h3><span class="section-number">10.3.3. </span>Retrieval - Augmented Generation (RAG)<a class="headerlink" href="#retrieval-augmented-generation-rag" title="Permalink to this heading">#</a></h3>
</section>
<section id="agentic-systems">
<h3><span class="section-number">10.3.4. </span>Agentic systems<a class="headerlink" href="#agentic-systems" title="Permalink to this heading">#</a></h3>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./markdown"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="data_driven_methods.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">9. </span>Data-driven methods</p>
      </div>
    </a>
    <a class="right-next"
       href="execution_fundamentals.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">11. </span>Execution fundamentals</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-gen-ai">10.1. What is Gen AI?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#artificial-intelligence">10.1.1. Artificial Intelligence</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-models">10.1.2. Generative models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gen-ai-models">10.1.3. Gen AI models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#large-language-models">10.2. Large Language Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#traditional-language-modelling">10.2.1. Traditional language modelling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-transformer-architecture">10.2.2. The Transformer Architecture</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decoder-only-models">10.2.3. Decoder-only models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning-models-with-rlhl">10.2.4. Fine-tuning models with RLHL</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reasoning-models">10.2.5. Reasoning models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#working-with-large-language-models">10.3. Working with Large Language Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prompt-engineering">10.3.1. Prompt Engineering</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chain-of-thought">10.3.2. Chain of Thought</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#retrieval-augmented-generation-rag">10.3.3. Retrieval - Augmented Generation (RAG)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#agentic-systems">10.3.4. Agentic systems</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Javier Sabio González
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>